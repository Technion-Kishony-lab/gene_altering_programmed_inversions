{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5b7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# replace PATH_TO_DOWNLOADED_CODE_DIR with the path of our code that you downloaded. This is the directory\n",
    "# of the README file, among others.\n",
    "os.chdir(PATH_TO_DOWNLOADED_CODE_DIR) \n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "# matplotlib.use('TKAgg')\n",
    "# matplotlib.use('Agg')\n",
    "# %matplotlib tk\n",
    "\n",
    "# %matplotlib qt\n",
    "# %pylab qt\n",
    "# %matplotlib gtk\n",
    "# %matplotlib wx\n",
    "\n",
    "\n",
    "# IIUC, according to https://stackoverflow.com/questions/51922480/javascript-error-ipython-is-not-defined-in-jupyterlab/56416229#56416229, the way\n",
    "# to do this is to use \"%matplotlib widget\"\n",
    "# import matplotlib\n",
    "# %matplotlib notebook \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "# dpi = None\n",
    "dpi = 100\n",
    "dpi = 300 # the figures look a lot better (but load a little slower)\n",
    "if dpi:\n",
    "    matplotlib.rcParams['figure.dpi'] = dpi\n",
    "    matplotlib.rc(\"savefig\", dpi=dpi)\n",
    "    \n",
    "if 0:\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0dd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections\n",
    "import subprocess\n",
    "import importlib\n",
    "import math\n",
    "import operator\n",
    "import pathlib\n",
    "import Bio\n",
    "import io\n",
    "import itertools\n",
    "import matplotlib.colors\n",
    "import matplotlib.patches\n",
    "import matplotlib.ticker\n",
    "import matplotlib.image\n",
    "import matplotlib.transforms\n",
    "import string\n",
    "from Bio import SeqRecord\n",
    "\n",
    "from generic import generic_utils\n",
    "importlib.reload(generic_utils)\n",
    "from generic import bio_utils\n",
    "importlib.reload(bio_utils)\n",
    "from generic import blast_interface_and_utils\n",
    "importlib.reload(blast_interface_and_utils)\n",
    "from generic import vsearch_interface_and_utils\n",
    "importlib.reload(vsearch_interface_and_utils)\n",
    "from generic import mauve_interface_and_utils\n",
    "importlib.reload(mauve_interface_and_utils)\n",
    "from searching_for_pis import massive_screening_configuration\n",
    "importlib.reload(massive_screening_configuration)\n",
    "from searching_for_pis import massive_screening_stage_1\n",
    "importlib.reload(massive_screening_stage_1)\n",
    "from searching_for_pis import index_column_names\n",
    "importlib.reload(index_column_names)\n",
    "# from generic import samtools_and_sam_files_interface\n",
    "# importlib.reload(samtools_and_sam_files_interface)\n",
    "\n",
    "\n",
    "import scipy.stats\n",
    "import scipy.integrate\n",
    "import scipy.interpolate\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 200624: It seems that this is the default (what I get from np.geterr() after importing numpy): {'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.seterr.html says: Underflow: result so close to zero that some precision was lost.\n",
    "# so I guess it is Ok that I just ignore underflow problems?\n",
    "# np.seterr(all='raise')\n",
    "np.seterr(divide='raise', over='raise', invalid='raise')\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "\n",
    "search_for_pis_args = massive_screening_configuration.SEARCH_FOR_PROGRAMMED_INVERSIONS_ARGS_DICT\n",
    "\n",
    "debug___num_of_taxa_to_go_over = search_for_pis_args['debug___num_of_taxa_to_go_over']\n",
    "debug___taxon_uids = search_for_pis_args['debug___taxon_uids']\n",
    "\n",
    "stage_out_file_name_suffix = massive_screening_stage_1.get_stage_out_file_name_suffix(\n",
    "    debug___num_of_taxa_to_go_over=debug___num_of_taxa_to_go_over,\n",
    "    debug___taxon_uids=debug___taxon_uids,\n",
    ")\n",
    "\n",
    "paper_figs_and_tables_dir_path = 'paper_figs_and_tables'\n",
    "pathlib.Path(paper_figs_and_tables_dir_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_out_dir_path = search_for_pis_args['stage1']['output_dir_path']\n",
    "stage1_results_info_pickle_file_path = os.path.join(stage1_out_dir_path, search_for_pis_args['stage1']['results_pickle_file_name'])\n",
    "stage1_results_info_pickle_file_path = generic_utils.add_suffix_to_file_name_while_keeping_extension(\n",
    "    stage1_results_info_pickle_file_path, stage_out_file_name_suffix)\n",
    "\n",
    "with open(stage1_results_info_pickle_file_path, 'rb') as f:\n",
    "    stage1_results_info = pickle.load(f)\n",
    "\n",
    "taxa_for_which_download_of_primary_assembly_nuccore_entries_failed_uids = stage1_results_info[\n",
    "    'taxa_for_which_download_of_primary_assembly_nuccore_entries_failed_uids']\n",
    "print(f'len(taxa_for_which_download_of_primary_assembly_nuccore_entries_failed_uids): '\n",
    "      f'{len(taxa_for_which_download_of_primary_assembly_nuccore_entries_failed_uids)}')\n",
    "    \n",
    "\n",
    "num_of_all_cds = stage1_results_info['num_of_all_cds']\n",
    "num_of_taxa = stage1_results_info['num_of_taxa']\n",
    "\n",
    "taxa_df_csv_file_path = stage1_results_info['taxa_df_csv_file_path']\n",
    "taxa_df = pd.read_csv(taxa_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "print(f'len(taxa_df): {len(taxa_df)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_out_dir_path = search_for_pis_args['stage2']['output_dir_path']\n",
    "stage2_results_info_pickle_file_path = os.path.join(stage2_out_dir_path, search_for_pis_args['stage2']['results_pickle_file_name'])\n",
    "stage2_results_info_pickle_file_path = generic_utils.add_suffix_to_file_name_while_keeping_extension(\n",
    "    stage2_results_info_pickle_file_path, stage_out_file_name_suffix)\n",
    "\n",
    "with open(stage2_results_info_pickle_file_path, 'rb') as f:\n",
    "    stage2_results_info = pickle.load(f)\n",
    "\n",
    "num_of_all_ir_pairs = stage2_results_info['num_of_all_ir_pairs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage3_out_dir_path = search_for_pis_args['stage3']['output_dir_path']\n",
    "stage3_results_info_pickle_file_path = os.path.join(stage3_out_dir_path, search_for_pis_args['stage3']['results_pickle_file_name'])\n",
    "stage3_results_info_pickle_file_path = generic_utils.add_suffix_to_file_name_while_keeping_extension(\n",
    "    stage3_results_info_pickle_file_path, stage_out_file_name_suffix)\n",
    "\n",
    "with open(stage3_results_info_pickle_file_path, 'rb') as f:\n",
    "    stage3_results_info = pickle.load(f)\n",
    "\n",
    "ir_pairs_linked_df_csv_file_path = stage3_results_info['ir_pairs_linked_df_csv_file_path']\n",
    "ir_pairs_linked_df = pd.read_csv(ir_pairs_linked_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# pairs_df = pairs_df.sample(frac=1, random_state=0)\n",
    "discard_ir_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num_info = stage3_results_info[\n",
    "    'discard_ir_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num_info']\n",
    "\n",
    "\n",
    "pairs_linked_with_evidence_for_repeat_overlapping_mobile_element_df_csv_file_path = stage3_results_info[\n",
    "    'pairs_linked_with_evidence_for_repeat_overlapping_mobile_element_df_csv_file_path']\n",
    "pairs_linked_with_evidence_for_repeat_overlapping_mobile_element_df = pd.read_csv(\n",
    "    pairs_linked_with_evidence_for_repeat_overlapping_mobile_element_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "print(f'len(pairs_linked_with_evidence_for_repeat_overlapping_mobile_element_df): '\n",
    "      f'{len(pairs_linked_with_evidence_for_repeat_overlapping_mobile_element_df)}')\n",
    "\n",
    "pairs_after_discarding_ir_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num_df_csv_file_path = stage3_results_info[\n",
    "    'pairs_after_discarding_ir_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num_df_csv_file_path']\n",
    "pairs_after_discarding_ir_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num_df = pd.read_csv(\n",
    "    pairs_after_discarding_ir_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d30e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage4_out_dir_path = search_for_pis_args['stage4']['output_dir_path']\n",
    "stage4_results_info_pickle_file_path = os.path.join(stage4_out_dir_path, search_for_pis_args['stage4']['results_pickle_file_name'])\n",
    "stage4_results_info_pickle_file_path = generic_utils.add_suffix_to_file_name_while_keeping_extension(\n",
    "    stage4_results_info_pickle_file_path, stage_out_file_name_suffix)\n",
    "\n",
    "with open(stage4_results_info_pickle_file_path, 'rb') as f:\n",
    "    stage4_results_info = pickle.load(f)\n",
    "    \n",
    "taxa_potential_evidence_for_pi_info_pickle_file_path = stage4_results_info['taxa_potential_evidence_for_pi_info_pickle_file_path']\n",
    "with open(taxa_potential_evidence_for_pi_info_pickle_file_path, 'rb') as f:\n",
    "    taxa_potential_evidence_for_pi_info = pickle.load(f)\n",
    "taxon_uid_to_taxon_potential_evidence_for_pi_info = taxa_potential_evidence_for_pi_info['taxon_uid_to_taxon_potential_evidence_for_pi_info']\n",
    "\n",
    "num_of_taxa_for_which_search_for_wgs_nuccore_entries_failed = taxa_potential_evidence_for_pi_info[\n",
    "    'num_of_taxa_for_which_search_for_wgs_nuccore_entries_failed']\n",
    "print(f'num_of_taxa_for_which_search_for_wgs_nuccore_entries_failed: {num_of_taxa_for_which_search_for_wgs_nuccore_entries_failed}')\n",
    "\n",
    "num_of_taxa_for_which_search_for_local_blast_nuccore_entries_failed = taxa_potential_evidence_for_pi_info[\n",
    "    'num_of_taxa_for_which_search_for_local_blast_nuccore_entries_failed']\n",
    "print(f'num_of_taxa_for_which_search_for_local_blast_nuccore_entries_failed: {num_of_taxa_for_which_search_for_local_blast_nuccore_entries_failed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage5_out_dir_path = search_for_pis_args['stage5']['output_dir_path']\n",
    "stage5_results_info_pickle_file_path = os.path.join(stage5_out_dir_path, search_for_pis_args['stage5']['results_pickle_file_name'])\n",
    "stage5_results_info_pickle_file_path = generic_utils.add_suffix_to_file_name_while_keeping_extension(\n",
    "    stage5_results_info_pickle_file_path, stage_out_file_name_suffix)\n",
    "\n",
    "with open(stage5_results_info_pickle_file_path, 'rb') as f:\n",
    "    stage5_results_info = pickle.load(f)\n",
    "\n",
    "taxon_uid_to_nuccore_accession_to_other_nuccore_entries_evidence_for_pis_in_nuccore_entry_info_pickle_file_path = stage5_results_info[\n",
    "    'taxon_uid_to_nuccore_accession_to_other_nuccore_entries_evidence_for_pis_in_nuccore_entry_info_pickle_file_path']\n",
    "with open(taxon_uid_to_nuccore_accession_to_other_nuccore_entries_evidence_for_pis_in_nuccore_entry_info_pickle_file_path, 'rb') as f:\n",
    "    taxon_uid_to_nuccore_accession_to_other_nuccore_entries_evidence_for_pis_in_nuccore_entry_info = pickle.load(f)\n",
    "\n",
    "extended_merged_cds_pair_region_df_csv_file_path = stage5_results_info['extended_merged_cds_pair_region_df_csv_file_path']\n",
    "extended_merged_cds_pair_region_df = pd.read_csv(extended_merged_cds_pair_region_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "region_in_other_nuccore_df_csv_file_path = stage5_results_info['region_in_other_nuccore_df_csv_file_path']\n",
    "region_in_other_nuccore_df = pd.read_csv(region_in_other_nuccore_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "potential_breakpoint_df_csv_file_path = stage5_results_info['potential_breakpoint_df_csv_file_path']\n",
    "potential_breakpoint_df = pd.read_csv(potential_breakpoint_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "num_of_pairs_with_high_confidence_bps = stage5_results_info['num_of_pairs_with_high_confidence_bps']\n",
    "\n",
    "minimal_ir_pairs_linked_to_breakpoint_pairs_df_csv_file_path = stage5_results_info['minimal_ir_pairs_linked_to_breakpoint_pairs_df_csv_file_path']\n",
    "minimal_ir_pairs_linked_to_breakpoint_pairs_df = pd.read_csv(minimal_ir_pairs_linked_to_breakpoint_pairs_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "filtered_minimal_ir_pairs_linked_to_breakpoint_pairs_df_csv_file_path = stage5_results_info['filtered_minimal_ir_pairs_linked_to_breakpoint_pairs_df_csv_file_path']\n",
    "filtered_minimal_ir_pairs_linked_to_breakpoint_pairs_df = pd.read_csv(filtered_minimal_ir_pairs_linked_to_breakpoint_pairs_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "pairs_with_highest_confidence_bps_df_csv_file_path = stage5_results_info['pairs_with_highest_confidence_bps_df_csv_file_path']\n",
    "pairs_with_highest_confidence_bps_df = pd.read_csv(pairs_with_highest_confidence_bps_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "lens_of_spanning_regions_ratios_df_csv_file_path = stage5_results_info['lens_of_spanning_regions_ratios_df_csv_file_path']\n",
    "lens_of_spanning_regions_ratios_df = pd.read_csv(lens_of_spanning_regions_ratios_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "filtered_pairs_with_highest_confidence_bps_df_csv_file_path = stage5_results_info['filtered_pairs_with_highest_confidence_bps_df_csv_file_path']\n",
    "filtered_pairs_with_highest_confidence_bps_df = pd.read_csv(filtered_pairs_with_highest_confidence_bps_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "taxon_uid_to_downloaded_taxon_wgs_nuccore_entries_info_pickle_file_path_pickle_file_path = stage5_results_info['taxon_uid_to_downloaded_taxon_wgs_nuccore_entries_info_pickle_file_path_pickle_file_path']\n",
    "with open(taxon_uid_to_downloaded_taxon_wgs_nuccore_entries_info_pickle_file_path_pickle_file_path, 'rb') as f:\n",
    "    taxon_uid_to_downloaded_taxon_wgs_nuccore_entries_info_pickle_file_path = pickle.load(f)\n",
    "\n",
    "products_with_high_confidence_bps_value_counts = pd.concat([\n",
    "    pairs_with_highest_confidence_bps_df[pairs_with_highest_confidence_bps_df['high_confidence_bp_for_both_repeats']][f'repeat{repeat_num}_cds_product']\n",
    "    for repeat_num in (1, 2)\n",
    "], ignore_index=True).value_counts()\n",
    "print(f'\\nproducts_with_high_confidence_bps_value_counts:\\n{products_with_high_confidence_bps_value_counts}\\n')\n",
    "\n",
    "curr_df = pairs_with_highest_confidence_bps_df.merge(extended_merged_cds_pair_region_df)\n",
    "stage5_num_of_cds_pairs_with_any_homologous_locus = len(\n",
    "    curr_df[curr_df['num_of_regions_in_other_nuccores_satisfying_mauve_total_match_proportion_and_min_sub_alignment_min_match_proportion_thresholds'] >= 1][\n",
    "        index_column_names.CDS_PAIR_INDEX_COLUMN_NAMES].drop_duplicates())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b159c4-f413-42a5-9b50-d56f02e011ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichment_analysis_out_dir_path = search_for_pis_args['enrichment_analysis']['output_dir_path']\n",
    "enrichment_analysis_results_info_pickle_file_path = os.path.join(enrichment_analysis_out_dir_path, search_for_pis_args['enrichment_analysis']['results_pickle_file_name'])\n",
    "enrichment_analysis_results_info_pickle_file_path = generic_utils.add_suffix_to_file_name_while_keeping_extension(\n",
    "    enrichment_analysis_results_info_pickle_file_path, stage_out_file_name_suffix)\n",
    "\n",
    "with open(enrichment_analysis_results_info_pickle_file_path, 'rb') as f:\n",
    "    enrichment_analysis_results_info = pickle.load(f)\n",
    "\n",
    "\n",
    "extended1_cds_pairs_for_enrichment_analysis_and_logistic_regression_df_csv_file_path = enrichment_analysis_results_info[\n",
    "    'extended1_cds_pairs_for_enrichment_analysis_and_logistic_regression_df_csv_file_path']\n",
    "extended1_cds_pairs_for_enrichment_analysis_and_logistic_regression_df = pd.read_csv(\n",
    "    extended1_cds_pairs_for_enrichment_analysis_and_logistic_regression_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "column_name_to_critical_val_and_passed_threshold_column_name_pickle_file_path = enrichment_analysis_results_info[\n",
    "    'column_name_to_critical_val_and_passed_threshold_column_name_pickle_file_path']\n",
    "with open(column_name_to_critical_val_and_passed_threshold_column_name_pickle_file_path, 'rb') as f:\n",
    "    column_name_to_critical_val_and_passed_threshold_column_name = pickle.load(f)\n",
    "\n",
    "if enrichment_analysis_results_info['logistic_regression_converged']:\n",
    "    if not (\n",
    "            search_for_pis_args['enrichment_analysis']['DEBUG___RANDOM_ANY_HIGH_CONFIDENCE_IR_PAIR_LINKED_TO_CDS_PAIR'] or \n",
    "            search_for_pis_args['enrichment_analysis']['DEBUG___SHUFFLE_OPERON_ASYMMETRY']\n",
    "    ):\n",
    "        repeat_cds_product_fisher_or_g_result_df_csv_file_path = enrichment_analysis_results_info['repeat_cds_product_fisher_or_g_result_df_csv_file_path']\n",
    "        repeat_cds_product_fisher_or_g_result_df = pd.read_csv(repeat_cds_product_fisher_or_g_result_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "        adjacent_repeat_cds_product_fisher_or_g_result_df_csv_file_path = enrichment_analysis_results_info['adjacent_repeat_cds_product_fisher_or_g_result_df_csv_file_path']\n",
    "        adjacent_repeat_cds_product_fisher_or_g_result_df = pd.read_csv(adjacent_repeat_cds_product_fisher_or_g_result_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "    logistic_regression_fit_result_df_csv_file_path = enrichment_analysis_results_info['logistic_regression_fit_result_df_csv_file_path']\n",
    "    logistic_regression_fit_result_df = pd.read_csv(logistic_regression_fit_result_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "    \n",
    "    unified_roc_curve_df_csv_file_path = enrichment_analysis_results_info['unified_roc_curve_df_csv_file_path']\n",
    "    unified_roc_curve_df = pd.read_csv(unified_roc_curve_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "\n",
    "    if not (\n",
    "            search_for_pis_args['enrichment_analysis']['DEBUG___RANDOM_ANY_HIGH_CONFIDENCE_IR_PAIR_LINKED_TO_CDS_PAIR'] or \n",
    "            search_for_pis_args['enrichment_analysis']['DEBUG___SHUFFLE_OPERON_ASYMMETRY']\n",
    "    ):\n",
    "        extended3_all_cds_pairs_df_csv_file_path = (\n",
    "            enrichment_analysis_results_info['extended3_all_cds_pairs_df_csv_file_path'])\n",
    "        extended3_all_cds_pairs_df = pd.read_csv(\n",
    "            extended3_all_cds_pairs_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "\n",
    "else:\n",
    "    logistic_regression_fit_result_df = None\n",
    "    unified_roc_curve_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787aaa9-1673-49c6-93bc-b191bec969d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage6_out_dir_path = search_for_pis_args['stage6']['output_dir_path']\n",
    "stage6_results_info_pickle_file_path = os.path.join(stage6_out_dir_path, search_for_pis_args['stage6']['results_pickle_file_name'])\n",
    "stage6_results_info_pickle_file_path = generic_utils.add_suffix_to_file_name_while_keeping_extension(\n",
    "    stage6_results_info_pickle_file_path, stage_out_file_name_suffix)\n",
    "\n",
    "with open(stage6_results_info_pickle_file_path, 'rb') as f:\n",
    "    stage6_results_info = pickle.load(f)\n",
    "\n",
    "all_raw_read_alignment_results_df_csv_file_path = stage6_results_info['all_raw_read_alignment_results_df_csv_file_path']\n",
    "all_raw_read_alignment_results_df = pd.read_csv(all_raw_read_alignment_results_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "putative_pi_locus_descriptions_df_csv_file_path = stage6_results_info['putative_pi_locus_descriptions_df_csv_file_path']\n",
    "putative_pi_locus_descriptions_df = pd.read_csv(putative_pi_locus_descriptions_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "all_all_diff_score_alignments_df_csv_file_path = stage6_results_info['all_all_diff_score_alignments_df_csv_file_path']\n",
    "all_all_diff_score_alignments_df = pd.read_csv(all_all_diff_score_alignments_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "rna_seq_summary_df_csv_file_path = stage6_results_info['rna_seq_summary_df_csv_file_path']\n",
    "rna_seq_summary_df = pd.read_csv(rna_seq_summary_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "good_rna_sra_accessions_and_predicted_invertible_regions_df = rna_seq_summary_df[\n",
    "            (rna_seq_summary_df['num_of_reads_matching_ref_better'] > 0) &\n",
    "            (rna_seq_summary_df['num_of_reads_matching_non_ref_better'] > 0)\n",
    "][['sra_accession', 'in_silico_inverted_region']].drop_duplicates()\n",
    "\n",
    "good_rna_sra_accessions_df = good_rna_sra_accessions_and_predicted_invertible_regions_df['sra_accession'].drop_duplicates()\n",
    "\n",
    "semi_min_rna_seq_summary_df = rna_seq_summary_df.merge(good_rna_sra_accessions_df)\n",
    "semi_min_all_all_diff_score_alignments_df = all_all_diff_score_alignments_df.merge(good_rna_sra_accessions_df)\n",
    "min_all_all_diff_score_alignments_df = all_all_diff_score_alignments_df.merge(good_rna_sra_accessions_and_predicted_invertible_regions_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_homologs_df_csv_file_path = stage6_results_info['all_homologs_df_csv_file_path']\n",
    "if generic_utils.is_file_empty(all_homologs_df_csv_file_path):\n",
    "    all_homologs_df = None\n",
    "else:\n",
    "    all_homologs_df = pd.read_csv(all_homologs_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "    all_homologs_extended_df = all_homologs_df.merge(taxa_df)\n",
    "    assert len(all_homologs_extended_df) == len(all_homologs_df)\n",
    "    \n",
    "\n",
    "    \n",
    "all_all_repeat_cds_covered_bases_proportions_pickle_file_path = stage6_results_info['all_all_repeat_cds_covered_bases_proportions_pickle_file_path']\n",
    "with open(all_all_repeat_cds_covered_bases_proportions_pickle_file_path, 'rb') as f:\n",
    "    all_all_repeat_cds_covered_bases_proportions = pickle.load(f)\n",
    "    \n",
    "all_all_alignment_bases_covered_by_cds_proportions_pickle_file_path = stage6_results_info['all_all_alignment_bases_covered_by_cds_proportions_pickle_file_path']\n",
    "with open(all_all_alignment_bases_covered_by_cds_proportions_pickle_file_path, 'rb') as f:\n",
    "    all_all_alignment_bases_covered_by_cds_proportions = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f580d67-122d-461d-b786-1113aaf2b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_xlsx_for_df(table_file_path, sheet_name, df, ordered_column_name_and_final_name_and_width, column_name_to_num_format_specifier={}, writer=None):\n",
    "    use_existing_writer = writer is not None\n",
    "    \n",
    "    if use_existing_writer:\n",
    "        assert table_file_path is None\n",
    "    else:\n",
    "        writer = pd.ExcelWriter(table_file_path, engine='xlsxwriter')\n",
    "    \n",
    "    df = df.copy()\n",
    "    df = df[[x[0] for x in ordered_column_name_and_final_name_and_width]]\n",
    "    column_name_to_final_column_name = {x[0]: x[1] for x in ordered_column_name_and_final_name_and_width}\n",
    "    df.rename(columns=column_name_to_final_column_name, inplace=True)\n",
    "    \n",
    "    \n",
    "    # https://stackoverflow.com/questions/55183760/text-wrap-format-gets-ignored-using-worksheet-formatting/55185388#55185388\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False, startrow=1, header=False)\n",
    "    \n",
    "    \n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    \n",
    "    shared_cell_format_attribute_dict = {'text_wrap': True, 'border': 1, 'border_color': 'black', 'align': 'left'}\n",
    "    normal_cell_format = workbook.add_format(shared_cell_format_attribute_dict)\n",
    "    header_cell_format = workbook.add_format({\n",
    "        **shared_cell_format_attribute_dict, \n",
    "        'bold': True, \n",
    "        # 'valign': 'top',\n",
    "    })\n",
    "    \n",
    "    final_column_name_to_num_format = {\n",
    "        column_name_to_final_column_name[column_name]: workbook.add_format(\n",
    "            {**shared_cell_format_attribute_dict, 'num_format': num_format_specifier})\n",
    "        for column_name, num_format_specifier in column_name_to_num_format_specifier.items()\n",
    "    }\n",
    "    \n",
    "    \n",
    "    num_of_columns = len(list(df))\n",
    "    for i in range(num_of_columns):\n",
    "        final_column_name = ordered_column_name_and_final_name_and_width[i][1]\n",
    "        column_cell_format = final_column_name_to_num_format.get(final_column_name, normal_cell_format)\n",
    "        worksheet.set_column(i, i, ordered_column_name_and_final_name_and_width[i][2], column_cell_format)\n",
    "\n",
    "    # https://stackoverflow.com/questions/55183760/text-wrap-format-gets-ignored-using-worksheet-formatting/55185388#55185388\n",
    "    for i, column_name in enumerate(list(df)):\n",
    "        worksheet.write(0, i, column_name, header_cell_format)\n",
    "\n",
    "    if not use_existing_writer:\n",
    "        writer.save()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e3e3c-2116-4a18-8224-14404f959501",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"legend.frameon\"] = False\n",
    "matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "matplotlib.rcParams[\"savefig.facecolor\"] = 'white'\n",
    "matplotlib.rcParams[\"path.snap\"] = False\n",
    "\n",
    "# according to https://matplotlib.org/stable/gallery/color/color_cycle_default.html\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "TEN_DEFAULT_MATPLOTLIB_COLORS = prop_cycle.by_key()['color']\n",
    "TEN_DEFAULT_MATPLOTLIB_COLORS_WITHOUT_GREY = TEN_DEFAULT_MATPLOTLIB_COLORS[:7] + TEN_DEFAULT_MATPLOTLIB_COLORS[8:]\n",
    "TEN_DEFAULT_MATPLOTLIB_COLORS_WITHOUT_GREY_AND_PURPLE = TEN_DEFAULT_MATPLOTLIB_COLORS_WITHOUT_GREY[:4] + TEN_DEFAULT_MATPLOTLIB_COLORS_WITHOUT_GREY[5:]\n",
    "\n",
    "COLOR_OF_HACKY_LINE_TO_OVERWRITE_FRACTION_LINE_OF_ORIENTATION_MATCHING = 'red'\n",
    "COLOR_OF_HACKY_LINE_TO_OVERWRITE_FRACTION_LINE_OF_ORIENTATION_MATCHING = 'white'\n",
    "ALIGNMENT_PROJECTION_COLOR = 'orange'\n",
    "ALIGNMENT_PROJECTION_ALPHA = 0.3\n",
    "ALIGNMENT_PROJECTION_EDGE_COLOR = (*matplotlib.colors.to_rgb('black'), 0.5)\n",
    "ALIGNMENT_PROJECTION_LINE_WIDTH = 0.5\n",
    "VOLCANO_PLOT_EDGE_COLOR = '#222222'\n",
    "VOLCANO_PLOT_LINEWIDTH = 0.5\n",
    "GREY_HIST_COLOR = (0.7,0.7,0.7)\n",
    "THRESHOLD_DASHED_LINE_ALPHA = 0.7\n",
    "FAKE_VAL_THRESHOLD_LINE_ALPHA = 0.7\n",
    "FAKE_VAL_THRESHOLD_LINE_COLOR = 'black'\n",
    "FAKE_VAL_THRESHOLD_LINE_STYLE = ':'\n",
    "# https://colorbrewer2.org\n",
    "COLORBREWER_SET3 = ['#8dd3c7','#ffffb3','#bebada','#fb8072','#80b1d3','#fdb462','#b3de69','#fccde5','#d9d9d9','#bc80bd','#ccebc5','#ffed6f']\n",
    "COLORBREWER_SET3_GREY = '#d9d9d9'\n",
    "COLORBREWER_SET3_WITHOUT_GREY = ['#8dd3c7','#ffffb3','#bebada','#fb8072','#80b1d3','#fdb462','#b3de69','#fccde5','#bc80bd','#ccebc5','#ffed6f']\n",
    "COLORBREWER_SET3_WITHOUT_GREY_AND_YELLOW = ['#8dd3c7' ,'#bebada','#fb8072','#80b1d3','#fdb462','#b3de69','#fccde5','#bc80bd','#ccebc5','#ffed6f']\n",
    "COLORBREWER_SET3_WITHOUT_GREY_AND_YELLOW_AND_ORANGE = ['#8dd3c7' ,'#bebada','#fb8072','#80b1d3','#b3de69','#fccde5','#bc80bd','#ccebc5','#ffed6f']\n",
    "COLORBREWER_SET1 = ['#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00','#ffff33','#a65628','#f781bf','#999999']\n",
    "COLORBREWER_SET1_WITHOUT_GREY = ['#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00','#ffff33','#a65628','#f781bf']\n",
    "COLORBREWER_SET1_WITHOUT_GREY_AND_YELLOW = ['#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00' ,'#a65628','#f781bf']\n",
    "COLORBREWER_SET1_WITHOUT_GREY_AND_YELLOW_AND_ORANGE = ['#e41a1c','#377eb8','#4daf4a','#984ea3' ,'#a65628','#f781bf']\n",
    "COLORBREWER_PASTEL = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
    "COLORBREWER_PASTEL_WITHOUT_GREY = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec']\n",
    "COLORBREWER_PASTEL_WITHOUT_GREY_AND_WITH_GREEN_AND_BROWN_SWITCHED = ['#fbb4ae','#b3cde3','#e5d8bd','#decbe4','#fed9a6','#ffffcc','#ccebc5','#fddaec']\n",
    "COLORBREWER_PASTEL_WITH_GREEN_AND_BROWN_SWITCHED = ['#fbb4ae','#b3cde3','#e5d8bd','#decbe4','#fed9a6','#ffffcc','#ccebc5','#fddaec', '#f2f2f2']\n",
    "PRODUCT_CLASS_COLORS = [\n",
    "    '#fbb4ae','#b3cde3','#e5d8bd','#decbe4','#fed9a6','#ffffcc','#ccebc5',\n",
    "    # '#fddaec', # pink, i think\n",
    "    \n",
    "    # 'paleturquoise',\n",
    "    # tuple(np.array(matplotlib.colors.to_rgb('paleturquoise')) * 0.7),\n",
    "    (*matplotlib.colors.to_rgb('paleturquoise'), 0.7),\n",
    "    # tuple(np.array(matplotlib.colors.to_rgb('fuchsia')) * 0.3),\n",
    "    # (*matplotlib.colors.to_rgb('fuchsia'), 0.15),\n",
    "    (*matplotlib.colors.to_rgb('tab:brown'), 0.4),\n",
    "    (*matplotlib.colors.to_rgb('darkcyan'), 0.45),\n",
    "]\n",
    "\n",
    "NON_REF_VARIANT_COLORS = COLORBREWER_SET1_WITHOUT_GREY_AND_YELLOW + [\n",
    "    'gold',\n",
    "    'grey',\n",
    "    'teal',\n",
    "    'olive',\n",
    "    'olivedrab',\n",
    "    'lawngreen',\n",
    "    'cadetblue',\n",
    "    'mediumseagreen',\n",
    "    # 'mediumorchid',\n",
    "    'mediumpurple',\n",
    "    'deepskyblue',\n",
    "    'indianred',\n",
    "    'tan',\n",
    "    'darkkhaki',\n",
    "    'steelblue',\n",
    "    'lightgreen',\n",
    "    'powderblue',\n",
    "    'plum',\n",
    "    'mediumaquamarine',\n",
    "    'navajowhite',\n",
    "]\n",
    "\n",
    "HISTOGRAM_WITH_OVERLAP_ALPHA = 0.5\n",
    "HIGH_CONFIDENCE_BP_COLOR = 'black'\n",
    "NO_HIGH_CONFIDENCE_BP_COLOR = '#aaaaaa'\n",
    "\n",
    "OTHER_PRODUCT_CLASS_COLOR = 'white'\n",
    "\n",
    "# alternatives - 'outer membrane'\n",
    "# \n",
    "# https://www.annualreviews.org/doi/10.1146/annurev.micro.112408.134247 says:\n",
    "# \"TonB-dependent transporters (TBDTs) are bacterial outer membrane proteins that bind and transport ferric chelates, \n",
    "# called siderophores, as well as vitamin B12, nickel complexes, and carbohydrates. The transport process requires energy \n",
    "# in the form of proton motive force and a complex of three inner membrane proteins, TonB-ExbB-ExbD, to transduce this \n",
    "# energy to the outer membrane. The siderophore substrates range in complexity from simple small molecules such as citrate\n",
    "# to large proteins such as serum transferrin and hemoglobin.\"\n",
    "# \n",
    "# https://www.ebi.ac.uk/interpro/entry/InterPro/IPR023996/ says:\n",
    "# This entry describes a clade of TonB-linked outer membrane proteins (OMP). Members of the family are restricted to the \n",
    "# Bacteriodetes lineage (except for Gemmatimonas aurantiaca T-27 from the novel phylum Gemmatimonadetes) and occur in high\n",
    "# copy numbers, with over 100 members from Bacteroides thetaiotaomicron VPI-5482 alone. Published descriptions of members \n",
    "# of the family are available for RagA from Porphyromonas gingivalis [1], SusC from Bacteroides thetaiotaomicron [2], and \n",
    "# OmpW from Bacteroides caccae [3]. Members form pairs with members of the SusD/RagB family (PF07980). Transporter complexes \n",
    "# including these outer membrane proteins are likely to import large degradation products of proteins (e.g. RagA) or \n",
    "# carbohydrates (e.g. SusC) as nutrients, rather than siderophores.\n",
    "TONB_DEPENDENT_TRANSPORTER_CLASS_NAME = 'TonB-linked OMP'\n",
    "\n",
    "LIST_OF_PRODUCT_AND_PRODUCT_CLASS = [\n",
    "    ('BREX-1 system adenine-specific DNA-methyltransferase PglX', 'DNA MTase'),\n",
    "    ('Eco57I restriction-modification methylase domain-containing protein', 'DNA MTase'),\n",
    "    ('N-6 DNA methylase', 'DNA MTase'),\n",
    "    ('SAM-dependent DNA methyltransferase', 'DNA MTase'),\n",
    "    ('class I SAM-dependent DNA methyltransferase', 'DNA MTase'),\n",
    "    ('type I restriction-modification system subunit M', 'DNA MTase'),\n",
    "    ('eco57I restriction-modification methylase family protein', 'DNA MTase'),\n",
    "    # blastp https://www.ncbi.nlm.nih.gov/protein/WP_128833139.1 to \n",
    "    # https://www.ncbi.nlm.nih.gov/protein/WP_155765198.1, and you will get 87% alignment coverage.\n",
    "    ('type II restriction endonuclease', 'DNA MTase'),\n",
    "\n",
    "    ('DEAD/DEAH box helicase', 'helicase'),\n",
    "    ('DEAD/DEAH box helicase family protein', 'helicase'),\n",
    "    ('DISARM system SNF2-like helicase DrmD', 'helicase'),\n",
    "    ('DISARM system helicase DrmA', 'helicase'),\n",
    "    ('helicase', 'helicase'),\n",
    "    ('ATP-dependent DNA helicase RecG', 'helicase'),\n",
    "\n",
    "    ('GPW/gp25 family protein', 'phage'),\n",
    "    ('tail fiber assembly protein', 'phage'),\n",
    "    ('tail fiber domain-containing protein', 'phage'),\n",
    "    ('tail fiber protein', 'phage'),\n",
    "    ('baseplate J/gp47 family protein', 'phage'),\n",
    "    ('baseplate assembly protein', 'phage'),\n",
    "    ('phage GP46 family protein', 'phage'),\n",
    "    ('phage baseplate assembly protein V', 'phage'),\n",
    "    ('phage tail protein', 'phage'),\n",
    "    ('phage tail protein I', 'phage'),\n",
    "\n",
    "    ('TcpQ domain-containing protein', 'pili'),\n",
    "    ('Flp pilus assembly complex ATPase component TadA', 'pili'),\n",
    "    ('PilN family type IVB pilus formation outer membrane protein', 'pili'),\n",
    "    ('pilin', 'pili'),\n",
    "    ('pilus assembly protein PilX', 'pili'),\n",
    "    ('prepilin peptidase', 'pili'),\n",
    "    ('shufflon system plasmid conjugative transfer pilus tip adhesin PilV', 'pili'),\n",
    "    ('type 4b pilus protein PilO2', 'pili'),\n",
    "    ('type IV pilus biogenesis protein PilM', 'pili'),\n",
    "    ('type IV pilus biogenesis protein PilP', 'pili'),\n",
    "\n",
    "    ('restriction endonuclease subunit S', 'HsdS'),\n",
    "\n",
    "    ('HsdR family type I site-specific deoxyribonuclease', 'HsdR'),\n",
    "    ('type I restriction endonuclease subunit R', 'HsdR'),\n",
    "\n",
    "    ('TonB-dependent siderophore receptor', TONB_DEPENDENT_TRANSPORTER_CLASS_NAME),\n",
    "    ('RagB/SusD family nutrient uptake outer membrane protein', TONB_DEPENDENT_TRANSPORTER_CLASS_NAME),\n",
    "    ('SusC/RagA family TonB-linked outer membrane protein', TONB_DEPENDENT_TRANSPORTER_CLASS_NAME),\n",
    "    ('SusD/RagB family nutrient-binding outer membrane lipoprotein', TONB_DEPENDENT_TRANSPORTER_CLASS_NAME),\n",
    "    ('TonB-dependent receptor', TONB_DEPENDENT_TRANSPORTER_CLASS_NAME),\n",
    "\n",
    "    ('integrase core domain-containing protein', 'recombinase'),\n",
    "    ('master DNA invertase Mpi family serine-type recombinase', 'recombinase'),\n",
    "    ('recombinase family protein', 'recombinase'),\n",
    "    ('site-specific integrase', 'recombinase'),\n",
    "    ('tyrosine-type DNA invertase PsrA', 'recombinase'),\n",
    "    ('tyrosine-type recombinase/integrase', 'recombinase'),\n",
    "    ('invertase', 'recombinase'),\n",
    "    ('Tsr0667 family tyrosine-type DNA invertase', 'recombinase'),\n",
    "    \n",
    "    # The ATP-binding cassette family: a structural perspective (https://link.springer.com/article/10.1007/s00018-009-0064-9) says:\n",
    "    # \"The ATP-binding cassette family is one of the largest groupings of membrane proteins, moving allocrites across lipid\n",
    "    # membranes, using energy from ATP. In bacteria, they reside in the inner membrane and are involved in both uptake and export.\"\n",
    "    # but Structure, Function, and Evolution of Bacterial ATP-Binding Cassette Systems\n",
    "    # (https://journals.asm.org/doi/full/10.1128/MMBR.00031-07) says:\n",
    "    # \"ABC systems couple the energy of ATP hydrolysis to an impressively large variety of essential biological phenomena,\n",
    "    # comprising not only transmembrane (TM) transport, for which they are best known, but also several non-transport-related\n",
    "    # processes, such as translation elongation (62) and DNA repair (174).\"\n",
    "    # and most importantly, it seems, ABC-F Proteins Mediate Antibiotic Resistance through Ribosomal Protection \n",
    "    # (https://journals.asm.org/doi/10.1128/mBio.01975-15) says:\n",
    "    # \"ABC-F proteins are found across all three domains of life, and comprise a single polypeptide containing two \n",
    "    # ATP-binding cassette (ABC) domains separated by a linker of ~80 amino acids. In contrast to canonical ABC transporters, \n",
    "    # the ABC portions of ABC-F proteins are not fused to transmembrane domains (TMDs), nor are they genetically associated \n",
    "    # with TMDs in operons (4). While it may be that some ABC-F proteins associate with TMDs to mediate transport across \n",
    "    # membranes, it is nevertheless apparent that members of this family participate in biological processes other than \n",
    "    # transport, including DNA repair, enzyme regulation, and translational control (5). In Gram-positive bacteria, a \n",
    "    # subgroup of the ABC-F proteins mediates resistance to antibiotics that exert their action on the ribosome.\" \n",
    "    #('ABC-F family ATP-binding cassette domain-containing protein', 'ABC transporter'),\n",
    "    #('ATP-binding cassette domain-containing protein', 'ABC transporter'),\n",
    "\n",
    "    ('ABC transporter ATP-binding protein', 'ABC transporter'),\n",
    "    ('ABC transporter permease', 'ABC transporter'),\n",
    "    ('ABC transporter permease subunit', 'ABC transporter'),\n",
    "    ('ABC transporter substrate-binding protein', 'ABC transporter'),\n",
    "    ('amino acid ABC transporter permease', 'ABC transporter'),\n",
    "    ('carbohydrate ABC transporter permease', 'ABC transporter'),\n",
    "    ('sn-glycerol-3-phosphate ABC transporter ATP-binding protein UgpC', 'ABC transporter'),\n",
    "    ('sugar ABC transporter ATP-binding protein', 'ABC transporter'),\n",
    "    ('sugar ABC transporter permease', 'ABC transporter'),\n",
    "\n",
    "    # ('hypothetical protein', 'hypothetical'),\n",
    "]\n",
    "PRODUCT_CLASS_DF = pd.DataFrame(LIST_OF_PRODUCT_AND_PRODUCT_CLASS, columns=['product', 'product_class'])\n",
    "\n",
    "PRODUCT_CLASSES_WITHOUT_OTHER_ORDERED = [\n",
    "    'recombinase',\n",
    "    'HsdS',\n",
    "    'HsdR',\n",
    "    'DNA MTase',\n",
    "    TONB_DEPENDENT_TRANSPORTER_CLASS_NAME,\n",
    "    'phage',\n",
    "    'helicase',\n",
    "    'pili',\n",
    "    'ABC transporter',\n",
    "    # 'hypothetical',\n",
    "]\n",
    "assert set(PRODUCT_CLASSES_WITHOUT_OTHER_ORDERED) == set(PRODUCT_CLASS_DF['product_class'])\n",
    "PRODUCT_CLASSES_ORDERED = PRODUCT_CLASSES_WITHOUT_OTHER_ORDERED + [\n",
    "    'other',\n",
    "]\n",
    "\n",
    "assert len(PRODUCT_CLASSES_WITHOUT_OTHER_ORDERED) <= len(PRODUCT_CLASS_COLORS)\n",
    "PRODUCT_CLASS_TO_COLOR = {\n",
    "    **{\n",
    "        x: c\n",
    "        for x, c in zip(PRODUCT_CLASSES_WITHOUT_OTHER_ORDERED, PRODUCT_CLASS_COLORS) \n",
    "    },\n",
    "    'other': 'white',\n",
    "    # 'hypothetical protein': 'white',\n",
    "    # 'hypothetical': (0.8,0.8,0.8),\n",
    "}\n",
    "\n",
    "PRODUCT_CLASS_AND_COLOR_DF = pd.DataFrame(list(PRODUCT_CLASS_TO_COLOR.items()), columns=['product_class', 'color'])\n",
    "\n",
    "ATTR_NAME_TO_VAL_TO_COLOR = {\n",
    "    'any_high_confidence_ir_pair_linked_to_cds_pair': {\n",
    "        True: HIGH_CONFIDENCE_BP_COLOR,\n",
    "        False: NO_HIGH_CONFIDENCE_BP_COLOR,\n",
    "    },\n",
    "}\n",
    "ATTR_NAME_TO_VAL_TO_LABEL = {\n",
    "    'any_high_confidence_ir_pair_linked_to_cds_pair': {\n",
    "        True: 'variation identified',\n",
    "        False: 'no variation identified',\n",
    "        # True: 'rearrangements identified',\n",
    "        # False: 'no rearrangements identified',\n",
    "    },\n",
    "}\n",
    "\n",
    "LOGISTIC_REGRESSION_COEFFICIENT_BOOL_COLUMN_NAME_TO_LABEL = {\n",
    "    # 'any_adjacent_recombinase': 'adjacent/nrecombinase',\n",
    "    'any_adjacent_recombinase': 'proximal\\nrecombinase',\n",
    "    'high_operon_asymmetry': \n",
    "        'high\\nasymmetry',\n",
    "    'high_operon_strand_efficiency': \n",
    "        'high strand\\nefficiency',\n",
    "    'high_max_repeat_len': \n",
    "        'long\\nrepeats',\n",
    "    'low_operon_spacer_len': \n",
    "        'short CDS\\ndistance',\n",
    "    'high_operon_closest_repeat_position_orientation_matching': 'high orientation\\nmatching',\n",
    "}\n",
    "\n",
    "def get_pvalue_repr(pvalue, verbose=True):\n",
    "    pval_exponent = math.floor(np.log10(pvalue))\n",
    "    if verbose:\n",
    "        print(f'pvalue: {pvalue}')\n",
    "    if pval_exponent >= -4:\n",
    "        pvalue_repr = f'{pvalue:.4f}'\n",
    "    else:\n",
    "        pval_exponent_repr = f'\\\\mathdefault{{10^{{{pval_exponent}}}}}'\n",
    "        pval_without_exponent = pvalue / (10 ** pval_exponent)\n",
    "        if pval_without_exponent == 1:\n",
    "            pvalue_repr = f'${pval_exponent_repr}$'\n",
    "        else:\n",
    "            pval_repr_without_exponent = f'{pval_without_exponent:.1f}'\n",
    "            pvalue_repr = f'${pval_repr_without_exponent}\\cdot{pval_exponent_repr}$'\n",
    "        if verbose:\n",
    "            print(pval_repr_without_exponent)\n",
    "            print(f'pval_repr_without_exponent: {pval_repr_without_exponent}')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'pvalue_repr: {pvalue_repr}')\n",
    "    \n",
    "    return pvalue_repr\n",
    "    \n",
    "def remove_trailing_zeros(num_str_repr):\n",
    "    if '.' not in num_str_repr:\n",
    "        return num_str_repr\n",
    "    for i, c in enumerate(num_str_repr[::-1]):\n",
    "        if c != '0':\n",
    "            break\n",
    "    num_of_trailing_zeros = i\n",
    "    if num_of_trailing_zeros == 0:\n",
    "        return num_str_repr\n",
    "    if c == '.':\n",
    "        return num_str_repr[:(-num_of_trailing_zeros - 1)]\n",
    "    return num_str_repr[:(-num_of_trailing_zeros)]\n",
    "\n",
    "def add_fake_bin_tick_labels_and_dashed_lines_to_histogram(ax, df, column_name, bin_size, last_real_bin_end=np.inf, first_real_bin_start=-np.inf, \n",
    "                                                           fake_val_threshold_line_color=FAKE_VAL_THRESHOLD_LINE_COLOR, \n",
    "                                                           fake_val_threshold_line_style=FAKE_VAL_THRESHOLD_LINE_STYLE, \n",
    "                                                           fake_val_threshold_line_alpha=FAKE_VAL_THRESHOLD_LINE_ALPHA):\n",
    "    orig_ylim = ax.get_ylim()\n",
    "    \n",
    "    anything_in_fake_last_bin = (df[column_name] > last_real_bin_end).sum() > 0\n",
    "    if anything_in_fake_last_bin:\n",
    "        ax.plot([last_real_bin_end, last_real_bin_end], orig_ylim,\n",
    "                linestyle=fake_val_threshold_line_style,color=fake_val_threshold_line_color,\n",
    "                alpha=0.7)\n",
    "\n",
    "    anything_in_fake_first_bin = (df[column_name] < first_real_bin_start).sum() > 0\n",
    "    if anything_in_fake_first_bin:\n",
    "        ax.plot([first_real_bin_start, first_real_bin_start], orig_ylim,\n",
    "                linestyle=fake_val_threshold_line_style,color=fake_val_threshold_line_color,\n",
    "                alpha=0.7)\n",
    "\n",
    "    ax.set_ylim(orig_ylim)\n",
    "\n",
    "    orig_x_tick_positions = ax.get_xticks()\n",
    "\n",
    "    min_position_for_keeping_ticks = (first_real_bin_start + bin_size / 2) if anything_in_fake_first_bin else -np.inf\n",
    "\n",
    "    max_position_for_keeping_ticks = (last_real_bin_end - bin_size / 2) if anything_in_fake_last_bin else np.inf\n",
    "\n",
    "    num_of_smallest_ticks_to_discard = (orig_x_tick_positions < min_position_for_keeping_ticks).sum()\n",
    "    num_of_ticks_to_keep = ((orig_x_tick_positions >= min_position_for_keeping_ticks) & (orig_x_tick_positions <= max_position_for_keeping_ticks)).sum()\n",
    "    new_x_tick_positions_without_fake = list(orig_x_tick_positions[num_of_smallest_ticks_to_discard:][:num_of_ticks_to_keep])\n",
    "\n",
    "    new_x_tick_positions = new_x_tick_positions_without_fake\n",
    "    new_x_tick_labels = [remove_trailing_zeros(str(f'{x:.2f}')) for x in new_x_tick_positions_without_fake]\n",
    "    new_xlim = list(ax.get_xlim())\n",
    "    if anything_in_fake_first_bin:\n",
    "        fake_first_bin_middle = first_real_bin_start - bin_size / 2\n",
    "        new_x_tick_positions = [fake_first_bin_middle] + new_x_tick_positions\n",
    "        new_x_tick_labels = [f'<{first_real_bin_start}'] + new_x_tick_labels\n",
    "    else:\n",
    "        new_xlim[0] = first_real_bin_start\n",
    "        \n",
    "    if anything_in_fake_last_bin:\n",
    "        fake_last_bin_middle = last_real_bin_end + bin_size / 2\n",
    "        new_x_tick_positions.append(fake_last_bin_middle)\n",
    "        new_x_tick_labels.append(f'$\\geqslant${last_real_bin_end}')\n",
    "        # new_x_tick_labels.append(f'$\\geq${last_real_bin_end}')\n",
    "    else:\n",
    "        new_xlim[1] = last_real_bin_end\n",
    "\n",
    "\n",
    "    ax.set_xticks(new_x_tick_positions)\n",
    "    ax.set_xticklabels(new_x_tick_labels)\n",
    "\n",
    "    ax.set_xlim(new_xlim)\n",
    "    \n",
    "\n",
    "def remove_all_ax_spines_an_x_and_y_axes(ax):\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    \n",
    "def add_product_class_column(df, product_column_name, product_class_column_name, other_product_class='other'):\n",
    "    df = df.copy()\n",
    "    assert product_class_column_name not in df\n",
    "    df = df.merge(PRODUCT_CLASS_DF.rename(columns={'product': product_column_name}), how='left')\n",
    "    \n",
    "    if other_product_class is None:\n",
    "        df.loc[df[product_class_column_name].isna(), product_class_column_name] = df.loc[df[product_class_column_name].isna(), product_column_name]\n",
    "    else:\n",
    "        df[product_class_column_name].fillna('other', inplace=True)\n",
    "\n",
    "    assert not df[product_class_column_name].isna().any()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_alignment_region_raw_read_alignment_args_by_cds_context_name_and_locus(\n",
    "        cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args, \n",
    "        cds_context_name, \n",
    "        nuccore_accession, \n",
    "        longer_linked_repeat_cds_region,\n",
    "):\n",
    "    alignment_regions_raw_read_alignment_args = cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args[\n",
    "        cds_context_name][nuccore_accession]\n",
    "    \n",
    "    for alignment_region_raw_read_alignment_args in alignment_regions_raw_read_alignment_args:\n",
    "        if alignment_region_raw_read_alignment_args['longest_linked_repeat_cds_region_and_protein_id'][0] == longer_linked_repeat_cds_region:\n",
    "            return alignment_region_raw_read_alignment_args\n",
    "    assert False\n",
    "\n",
    "            \n",
    "def get_cds_df_with_product_class(cds_df_csv_file_path, alignment_region_raw_read_alignment_args):\n",
    "    cds_df = pd.read_csv(cds_df_csv_file_path, sep='\\t', low_memory=False)\n",
    "    cds_df = add_product_class_column(cds_df, 'product', 'product_class')\n",
    "\n",
    "    if 'cds_start_and_end_to_curated_product_class' in alignment_region_raw_read_alignment_args:\n",
    "        cds_start_and_end_to_curated_product_class = alignment_region_raw_read_alignment_args['cds_start_and_end_to_curated_product_class']\n",
    "\n",
    "        assert {'start_pos', 'end_pos'} <= set(cds_df)\n",
    "        cds_df = cds_df.merge(pd.DataFrame([(start, end, product_class) for ((start, end), product_class) in cds_start_and_end_to_curated_product_class.items()], \n",
    "                     columns=['start_pos', 'end_pos', 'curated_product_class']), how='left')\n",
    "\n",
    "        cds_with_replaced_product_class_df = cds_df[~cds_df['curated_product_class'].isna()]\n",
    "        replaced_and_curated_products_are_as_expected = (\n",
    "            (cds_with_replaced_product_class_df['product_class'] == 'other') |\n",
    "            # (cds_with_replaced_product_class_df['product_class'] == 'hypothetical') |\n",
    "            ((cds_with_replaced_product_class_df['product_class'] == 'helicase') & (cds_with_replaced_product_class_df['curated_product_class'] == 'HsdR'))\n",
    "        ).all()\n",
    "        if not replaced_and_curated_products_are_as_expected:\n",
    "            raise RuntimeError(f'replaced_and_curated_products_are_as_expected == False. '\n",
    "                               f'cds_with_replaced_product_class_df:\\n{cds_with_replaced_product_class_df}')\n",
    "        \n",
    "        cds_df.loc[~cds_df['curated_product_class'].isna(), 'product_class'] = cds_df.loc[~cds_df['curated_product_class'].isna(), 'curated_product_class'] \n",
    "        \n",
    "    \n",
    "    return cds_df\n",
    "\n",
    "def get_variant_ir_pairs_to_num_of_evidence_reads(read_name_to_possible_ir_pairs_used_to_reach_from_ref):\n",
    "    variant_ir_pairs_to_num_of_evidence_reads = collections.defaultdict(int)\n",
    "    for variant_ir_pairs in read_name_to_possible_ir_pairs_used_to_reach_from_ref.values():\n",
    "        variant_ir_pairs_to_num_of_evidence_reads[frozenset(variant_ir_pairs)] += 1\n",
    "    variant_ir_pairs_to_num_of_evidence_reads = dict(variant_ir_pairs_to_num_of_evidence_reads) # I don't want a defaultdict moving around.\n",
    "    return variant_ir_pairs_to_num_of_evidence_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde5b51-cf77-498e-a553-797c532333f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDS_RATIO_DESCRIPTION_START = 'enrichment in predicted programmed inversion'\n",
    "TARGET_PRODUCT_ODDS_RATIO_DESCRIPTION_END = 'targets'\n",
    "NEIGHBOR_PRODUCT_ODDS_RATIO_DESCRIPTION_END = 'target neighbors'\n",
    "\n",
    "TARGET_PRODUCT_ODDS_RATIO_DESCRIPTION = f'{ODDS_RATIO_DESCRIPTION_START} {TARGET_PRODUCT_ODDS_RATIO_DESCRIPTION_END} (odds ratio)'\n",
    "NEIGHBOR_PRODUCT_ODDS_RATIO_DESCRIPTION = f'{ODDS_RATIO_DESCRIPTION_START} {NEIGHBOR_PRODUCT_ODDS_RATIO_DESCRIPTION_END} (odds ratio)'\n",
    "\n",
    "TARGET_PRODUCT_ENRICHMENT_TABLE_ORDERED_COLUMN_NAME_AND_FINAL_NAME_AND_WIDTH = (\n",
    "    ('product', 'GenBank CDS product annotation', 60),\n",
    "    ('odds_ratio', TARGET_PRODUCT_ODDS_RATIO_DESCRIPTION, 16),\n",
    "    ('corrected_pvalue', f'corrected p-value', 10),\n",
    ")\n",
    "NEIGHBOR_PRODUCT_ENRICHMENT_TABLE_ORDERED_COLUMN_NAME_AND_FINAL_NAME_AND_WIDTH = (\n",
    "    ('product', 'GenBank CDS product annotation', 60),\n",
    "    ('odds_ratio', NEIGHBOR_PRODUCT_ODDS_RATIO_DESCRIPTION, 16),\n",
    "    ('corrected_pvalue', f'corrected p-value', 10),\n",
    ")\n",
    "PRODUCT_ENRICHMENT_COLUMN_NAME_TO_NUM_FORMAT_SPECIFIER = {'odds_ratio': '0.000', 'corrected_pvalue': 11}\n",
    "\n",
    "# DEFAULT_REARRANGED_REGION_CMAP = 'jet'\n",
    "# DEFAULT_REARRANGED_REGION_CMAP = 'gist_rainbow'\n",
    "# DEFAULT_REARRANGED_REGION_CMAP = 'Greys'\n",
    "# DEFAULT_REARRANGED_REGION_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['1', '0.2'])\n",
    "# DEFAULT_REARRANGED_REGION_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', [(0, '1'), (0.4, '0.8'), (0.6, '0.45'), (1, '0.25')])\n",
    "# DEFAULT_REARRANGED_REGION_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', [(0, '0.9'), (0.4, '0.7'), (0.6, '0.45'), (1, '0.25')])\n",
    "# DEFAULT_REARRANGED_REGION_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['1', '0.1'])\n",
    "DEFAULT_REARRANGED_REGION_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', [(0, '0.9'), (0.45, '0.7'), (0.55, '0.45'), (1, '0.25')])\n",
    "# REDS_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['darkred', 'lightcoral'])\n",
    "REDS_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['darkred', 'rosybrown'])\n",
    "BLUES_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['darkblue', 'lightsteelblue'])\n",
    "BLUE_PURPLE_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['blue', 'purple'])\n",
    "BLUE_CYAN_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['blue', 'cyan'])\n",
    "RED_GOLD_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['red', 'gold'])\n",
    "RED_ORANGE_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['darkred', 'darkorange'])\n",
    "PURPLE_ORANGE_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['purple', 'darkorange'])\n",
    "RED_MAGENTA_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['darkred', 'magenta'])\n",
    "GREEN_MAGENTA_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['lawngreen', 'magenta'])\n",
    "RED_YELLOW_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['red', 'yellow'])\n",
    "YELLOW_RED_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['yellow', 'red'])\n",
    "YELLOW_DARK_RED_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list('', ['yellow', 'darkred'])\n",
    "\n",
    "def get_matching_chunk(chunk, interval, matching_interval, matching_interval_strand):\n",
    "    chunk_in_matching_interval = (\n",
    "        [x - interval[0] for x in chunk] if (matching_interval_strand == 1) else \n",
    "        [interval[1] - x for x in chunk[::-1]]\n",
    "    )\n",
    "    matching_chunk = [x + matching_interval[0] for x in chunk_in_matching_interval]\n",
    "    return matching_chunk\n",
    "\n",
    "def draw_cds_invertion_on_ax(\n",
    "    ax,\n",
    "    region=None,\n",
    "    cds_df=pd.DataFrame(),\n",
    "    ir_pairs_df=pd.DataFrame(),\n",
    "    interval_to_matching_interval_and_strand=dict(),\n",
    "    region_with_gradient=None,\n",
    "    draw_matching_region_cds=True,\n",
    "    cds_arrowhead_width=None,\n",
    "    region_cmap=DEFAULT_REARRANGED_REGION_CMAP,\n",
    "    region_bottom_and_top=(0.6, 0.8),\n",
    "    matching_region_bottom_and_top=(0.2, 0.4),\n",
    "    cds_polygon_linewidth=1.7,\n",
    "    cds_polygon_edge_color_with_alpha='black',\n",
    "    genome_rectangle_linewidth=0.6, # seems like for a low linewidth, black becomes grey (i guess because boundaries are gradual?)\n",
    "    genome_rectangle_alpha=1,\n",
    "    product_class_to_fill_color_with_alpha=None,\n",
    "    draw_region_rectangle=True,\n",
    "    transform=None,\n",
    "    clip_on=True,\n",
    "    dont_show_product_name=True,\n",
    "    draw_cds_polygon_again=False, # for overlapping CDSs\n",
    "    ir_pairs_pointing_outward=False,\n",
    "):\n",
    "    repeat_arrow_props = dict(\n",
    "        shrinkA=0, shrinkB=0, \n",
    "        # linewidth=0.5, \n",
    "        linewidth=1, \n",
    "        # arrowstyle=\"->\", \n",
    "        # arrowstyle='->,head_width=5',\n",
    "        arrowstyle='->,head_length=0.22',\n",
    "        # mutation_scale=5,\n",
    "    )\n",
    "    \n",
    "    cds_height = region_bottom_and_top[1] - region_bottom_and_top[0]\n",
    "    region_middle_y = np.mean(region_bottom_and_top)\n",
    "    matching_region_middle_y = np.mean(matching_region_bottom_and_top)\n",
    "    height_truncate = (0.05 * cds_height) if draw_region_rectangle else 0\n",
    "    cds_region_bottom_and_top = (region_bottom_and_top[0] + height_truncate, region_bottom_and_top[1] - height_truncate)\n",
    "    cds_matching_region_bottom_and_top = (matching_region_bottom_and_top[0] + height_truncate, matching_region_bottom_and_top[1] - height_truncate)\n",
    "    \n",
    "    pair_infos = ir_pairs_df.to_dict('records')\n",
    "    cds_infos = cds_df.to_dict('records')\n",
    "\n",
    "    def show_product_name(top_left, top_right, bottom_left, cds_product):\n",
    "        if not dont_show_product_name:\n",
    "            ax.text(\n",
    "                # np.mean((top_left[0],top_right[0])), \n",
    "                np.quantile((top_left[0],top_right[0]), 0.46), \n",
    "                # np.mean((top_left[1],bottom_left[1])), \n",
    "                np.quantile((bottom_left[1], top_left[1]), 0.44), \n",
    "                cds_product, va='center', ha='center', transform=transform, fontsize='x-small',\n",
    "            )\n",
    "        \n",
    "    \n",
    "    if transform is None:\n",
    "        transform = ax.transData\n",
    "    \n",
    "    if region is not None:\n",
    "        genome_height = region_bottom_and_top[1] - region_bottom_and_top[0]\n",
    "        \n",
    "        region_len = region[1] - region[0] + 1\n",
    "        if draw_region_rectangle:\n",
    "            ax.add_patch(matplotlib.patches.Rectangle((region[0], region_bottom_and_top[0]), region_len, genome_height,\n",
    "                                                      color='black', \n",
    "                                                      # alpha=genome_rectangle_alpha, \n",
    "                                                      linewidth=genome_rectangle_linewidth, fill=False, transform=transform, clip_on=clip_on))\n",
    "        \n",
    "        if interval_to_matching_interval_and_strand:\n",
    "            assert np.isclose(genome_height, matching_region_bottom_and_top[1] - matching_region_bottom_and_top[0])\n",
    "            if draw_region_rectangle:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((region[0], matching_region_bottom_and_top[0]), region_len, genome_height,\n",
    "                                                          color='black', \n",
    "                                                          # alpha=genome_rectangle_alpha, \n",
    "                                                          linewidth=genome_rectangle_linewidth, fill=False, transform=transform, clip_on=clip_on))\n",
    "            \n",
    "\n",
    "            if region_with_gradient is None:\n",
    "                interval_with_non_identical_matching_interval_to_matching_interval_and_strand = {\n",
    "                    interval: (matching_interval, matching_interval_strand)\n",
    "                    for interval, (matching_interval, matching_interval_strand) in interval_to_matching_interval_and_strand.items()\n",
    "                    if (matching_interval_strand == -1) or (interval != matching_interval)\n",
    "                }\n",
    "                region_with_gradient = (min(x[0] for x in interval_with_non_identical_matching_interval_to_matching_interval_and_strand),\n",
    "                                        max(x[1] for x in interval_with_non_identical_matching_interval_to_matching_interval_and_strand))\n",
    "        \n",
    "\n",
    "\n",
    "            region_gradient = list(range(region_with_gradient[0], region_with_gradient[1] + 2)) # why +2 and not +1?\n",
    "            ax.imshow(\n",
    "                [region_gradient],\n",
    "                cmap=region_cmap,\n",
    "                extent=(region_with_gradient[0], region_with_gradient[1] + 1, *region_bottom_and_top),\n",
    "                aspect='auto',\n",
    "                vmin=region_with_gradient[0],\n",
    "                vmax=(region_with_gradient[1] + 1),\n",
    "                transform=transform,\n",
    "            )\n",
    "\n",
    "            for interval, (matching_interval, matching_interval_strand) in interval_to_matching_interval_and_strand.items():\n",
    "                if (interval[0] >= region_with_gradient[0]) and (interval[1] <= region_with_gradient[1]):\n",
    "                    interval_gradient = list(range(interval[0], interval[1] + 2))\n",
    "                    ax.imshow(\n",
    "                        [interval_gradient if (matching_interval_strand == 1) else interval_gradient[::-1]],\n",
    "                        cmap=region_cmap,\n",
    "                        extent=(matching_interval[0], matching_interval[1] + 1, *matching_region_bottom_and_top),\n",
    "                        aspect='auto',\n",
    "                        vmin=region_with_gradient[0],\n",
    "                        vmax=(region_with_gradient[1] + 1),\n",
    "                        transform=transform,\n",
    "                    )\n",
    "    \n",
    "    for pair_info in pair_infos:\n",
    "        ir_color = pair_info['color']\n",
    "        repeat_len = pair_info['right1'] - pair_info['left1'] + 1\n",
    "        curr_arrow_props = {**repeat_arrow_props, 'color': ir_color}\n",
    "        for repeat_num in (1,2):\n",
    "            repeat_gradient = list(range(repeat_len + 1))\n",
    "            if repeat_num == 1:\n",
    "                repeat_gradient = repeat_gradient[::-1]\n",
    "            left = pair_info[f'left{repeat_num}']\n",
    "            right = pair_info[f'right{repeat_num}']\n",
    "            repeat = (left, right)\n",
    "            \n",
    "            arrow_end, arrow_start = repeat\n",
    "            if ir_pairs_pointing_outward:\n",
    "                arrow_end, arrow_start = arrow_start, arrow_end\n",
    "            if repeat_num == 2:\n",
    "                arrow_end, arrow_start = arrow_start, arrow_end\n",
    "            ax.annotate(\"\", xy=(arrow_start, region_middle_y), \n",
    "                        xytext=(arrow_end, region_middle_y), arrowprops=curr_arrow_props)\n",
    "            for interval, (matching_interval, matching_interval_strand) in interval_to_matching_interval_and_strand.items():\n",
    "                repeat_chunk = generic_utils.get_intersection_of_2_intervals(repeat, interval)\n",
    "                if repeat_chunk:\n",
    "                    assert repeat == repeat_chunk\n",
    "                    matching_repeat_chunk = get_matching_chunk(\n",
    "                        chunk=repeat_chunk, \n",
    "                        interval=interval,\n",
    "                        matching_interval=matching_interval,\n",
    "                        matching_interval_strand=matching_interval_strand,\n",
    "                    )\n",
    "                    \n",
    "                    arrow_end, arrow_start = matching_repeat_chunk\n",
    "                    if (matching_interval_strand == 1) ^ (repeat_num == 1):\n",
    "                        arrow_end, arrow_start = arrow_start, arrow_end\n",
    "                    ax.annotate(\"\", xy=(arrow_start, matching_region_middle_y), \n",
    "                                xytext=(arrow_end, matching_region_middle_y), arrowprops=curr_arrow_props)\n",
    "    \n",
    "    open_unfilled_polygon_common_kwargs = dict(closed=False, fill=False, edgecolor=cds_polygon_edge_color_with_alpha, \n",
    "                                               linewidth=cds_polygon_linewidth, transform=transform, clip_on=clip_on)\n",
    "    for cds_info in cds_infos:\n",
    "        start = cds_info['start_pos']\n",
    "        end = cds_info['end_pos']\n",
    "        cds_is_on_forward_strand = cds_info['strand'] == 1\n",
    "        cds_region = (start, end)\n",
    "        cds_product = cds_info['product'] if ('product' in cds_info) else None\n",
    "        \n",
    "        fill = False\n",
    "        fill_color = None\n",
    "        if product_class_to_fill_color_with_alpha:\n",
    "            product_class = cds_info['product_class']\n",
    "            if product_class in product_class_to_fill_color_with_alpha:\n",
    "                fill = True\n",
    "                fill_color = product_class_to_fill_color_with_alpha[product_class]\n",
    "        \n",
    "        closed_polygon_common_kwargs = dict(closed=True, fill=fill, facecolor=fill_color, edgecolor=cds_polygon_edge_color_with_alpha,\n",
    "                                            linewidth=cds_polygon_linewidth, transform=transform, clip_on=clip_on)\n",
    "        closed_unfilled_polygon_common_kwargs = dict(closed_polygon_common_kwargs)\n",
    "        closed_unfilled_polygon_common_kwargs['fill'] = False\n",
    "        closed_unfilled_polygon_common_kwargs['facecolor'] = None\n",
    "        # 211112: I split to multiple calls to Polygon because currently there seems to be an annoying bug (in matplotlib, i guess) that makes the \n",
    "        # polygon position inaccurate when it includes a diagonal line. ugh. same for each polygon below that contains a diagonal line...\n",
    "        if cds_is_on_forward_strand:\n",
    "            top_left = [start, cds_region_bottom_and_top[1]]\n",
    "            bottom_left = [start, cds_region_bottom_and_top[0]]\n",
    "            top_right = [end - cds_arrowhead_width, cds_region_bottom_and_top[1]]\n",
    "            bottom_right = [end - cds_arrowhead_width, cds_region_bottom_and_top[0]]\n",
    "            polgyon_position_arg = [bottom_right, bottom_left, top_left, top_right, (end, np.mean(cds_region_bottom_and_top))]\n",
    "            \n",
    "        else:     \n",
    "            top_left = [start + cds_arrowhead_width, cds_region_bottom_and_top[1]]\n",
    "            bottom_left = [start + cds_arrowhead_width, cds_region_bottom_and_top[0]]\n",
    "            top_right = [end, cds_region_bottom_and_top[1]]\n",
    "            bottom_right = [end, cds_region_bottom_and_top[0]]\n",
    "            polgyon_position_arg = [top_left, top_right, bottom_right, bottom_left, (start, np.mean(cds_region_bottom_and_top))]\n",
    "        \n",
    "        ax.add_patch(matplotlib.patches.Polygon(polgyon_position_arg, **closed_polygon_common_kwargs))\n",
    "        if draw_cds_polygon_again:\n",
    "            ax.add_patch(matplotlib.patches.Polygon(polgyon_position_arg, **closed_unfilled_polygon_common_kwargs, zorder=100))\n",
    "\n",
    "        if cds_product:\n",
    "            show_product_name(top_left, top_right, bottom_left, cds_product)\n",
    "            \n",
    "\n",
    "        if draw_matching_region_cds:\n",
    "            for interval, (matching_interval, matching_interval_strand) in interval_to_matching_interval_and_strand.items():\n",
    "                cds_chunk = generic_utils.get_intersection_of_2_intervals(cds_region, interval)\n",
    "                if cds_chunk:\n",
    "                    matching_cds_chunk = get_matching_chunk(\n",
    "                        chunk=cds_chunk, \n",
    "                        interval=interval,\n",
    "                        matching_interval=matching_interval,\n",
    "                        matching_interval_strand=matching_interval_strand,\n",
    "                    )\n",
    "                    top_left = (matching_cds_chunk[0], cds_matching_region_bottom_and_top[1])\n",
    "                    top_right = (matching_cds_chunk[1], cds_matching_region_bottom_and_top[1])\n",
    "                    bottom_left = (matching_cds_chunk[0], cds_matching_region_bottom_and_top[0])\n",
    "                    bottom_right = (matching_cds_chunk[1], cds_matching_region_bottom_and_top[0])\n",
    "                    cds_chunk_contains_cds_start = cds_chunk[0] == cds_region[0]\n",
    "                    cds_chunk_contains_cds_end = cds_chunk[1] == cds_region[1]\n",
    "                    if not cds_is_on_forward_strand:\n",
    "                        cds_chunk_contains_cds_start, cds_chunk_contains_cds_end = cds_chunk_contains_cds_end, cds_chunk_contains_cds_start\n",
    "                    matching_interval_is_on_forward_strand = matching_interval_strand == 1\n",
    "                    matching_interval_cds_is_on_forward_strand = not (cds_is_on_forward_strand ^ matching_interval_is_on_forward_strand)\n",
    "                    \n",
    "                    if not cds_chunk_contains_cds_end:\n",
    "                        if not cds_chunk_contains_cds_start:\n",
    "                            ax.add_patch(matplotlib.patches.Polygon([top_left, top_right], **open_unfilled_polygon_common_kwargs))\n",
    "                            ax.add_patch(matplotlib.patches.Polygon([bottom_right, bottom_left], **open_unfilled_polygon_common_kwargs))\n",
    "\n",
    "                        else:\n",
    "                            assert cds_chunk_contains_cds_start\n",
    "                            if matching_interval_cds_is_on_forward_strand:\n",
    "                                ax.add_patch(matplotlib.patches.Polygon([bottom_right, bottom_left, top_left, top_right], **open_unfilled_polygon_common_kwargs))\n",
    "                            else:\n",
    "                                ax.add_patch(matplotlib.patches.Polygon([top_left, top_right, bottom_right, bottom_left], **open_unfilled_polygon_common_kwargs))\n",
    "                            \n",
    "                    else:\n",
    "                        assert cds_chunk_contains_cds_end\n",
    "                        if matching_interval_cds_is_on_forward_strand:\n",
    "                            top_right = (top_right[0] - cds_arrowhead_width, top_right[1])\n",
    "                            bottom_right = (bottom_right[0] - cds_arrowhead_width, bottom_right[1])\n",
    "                        else:\n",
    "                            top_left = (top_left[0] + cds_arrowhead_width, top_left[1])\n",
    "                            bottom_left = (bottom_left[0] + cds_arrowhead_width, bottom_left[1])\n",
    "                            \n",
    "                        if cds_chunk_contains_cds_start:\n",
    "                            if matching_interval_cds_is_on_forward_strand:\n",
    "                                ax.add_patch(matplotlib.patches.Polygon([bottom_right, bottom_left, top_left, top_right], **open_unfilled_polygon_common_kwargs))\n",
    "                                ax.add_patch(matplotlib.patches.Polygon([top_right, [matching_cds_chunk[1], np.mean(cds_matching_region_bottom_and_top)], \n",
    "                                                                         bottom_right], **open_unfilled_polygon_common_kwargs))\n",
    "                            else:\n",
    "                                ax.add_patch(matplotlib.patches.Polygon([top_left, top_right, bottom_right, bottom_left],\n",
    "                                                                        **open_unfilled_polygon_common_kwargs))\n",
    "                                ax.add_patch(matplotlib.patches.Polygon([bottom_left, [matching_cds_chunk[0], np.mean(cds_matching_region_bottom_and_top)], \n",
    "                                                                         top_left], **open_unfilled_polygon_common_kwargs))\n",
    "                            if cds_product:\n",
    "                                show_product_name(top_left, top_right, bottom_left, cds_product)\n",
    "                                \n",
    "                        else:\n",
    "                            assert not cds_chunk_contains_cds_start\n",
    "                            ax.add_patch(matplotlib.patches.Polygon([bottom_right, bottom_left], **open_unfilled_polygon_common_kwargs))\n",
    "                            ax.add_patch(matplotlib.patches.Polygon([top_left, top_right], **open_unfilled_polygon_common_kwargs))\n",
    "                            if not (cds_is_on_forward_strand ^ matching_interval_is_on_forward_strand):\n",
    "                                ax.add_patch(matplotlib.patches.Polygon([top_right, [matching_cds_chunk[1], np.mean(matching_region_bottom_and_top)],\n",
    "                                                                         bottom_right], **open_unfilled_polygon_common_kwargs))\n",
    "                            if cds_is_on_forward_strand ^ matching_interval_is_on_forward_strand:\n",
    "                                ax.add_patch(matplotlib.patches.Polygon([bottom_left, [matching_cds_chunk[0], np.mean(matching_region_bottom_and_top)], \n",
    "                                                                         top_left], **open_unfilled_polygon_common_kwargs))\n",
    "    \n",
    "        \n",
    "    remove_all_ax_spines_an_x_and_y_axes(ax)\n",
    "    \n",
    "def set_x_lim_according_to_region(ax, region, margin_relative_size):\n",
    "    region_len = region[1] - region[0]\n",
    "    margin_size = margin_relative_size * region_len\n",
    "    new_xlim = (region[0] - margin_size, region[1] + margin_size)\n",
    "    ax.set_xlim(new_xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24bcbf-b7b2-4fa7-8f29-6eb1872fc073",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'fig1' in locals():\n",
    "    plt.close(fig1)\n",
    "# raise    \n",
    "fig1 = plt.figure(\n",
    "    figsize=(7,5), \n",
    "    # constrained_layout=True,\n",
    ")\n",
    "fig1_gridspec = fig1.add_gridspec(\n",
    "    6, 16, \n",
    "    height_ratios=[2,0.65,1,1,1,1],\n",
    ")\n",
    "\n",
    "fig1.subplots_adjust(\n",
    "    top=0.95,\n",
    "    bottom=0.05,\n",
    "    left=0.05,\n",
    "    right=0.95,\n",
    "    # hspace=0.2,\n",
    "    # wspace=0.2\n",
    ")\n",
    "# fig1.tight_layout()\n",
    "\n",
    "fig1a_ax = fig1.add_subplot(fig1_gridspec[0,:8])\n",
    "fig1c_ax = fig1.add_subplot(fig1_gridspec[0,10:])\n",
    "\n",
    "fig1b1_ax = fig1.add_subplot(fig1_gridspec[2:4,:3])\n",
    "fig1b2_ax = fig1.add_subplot(fig1_gridspec[2:4,3:7])\n",
    "fig1b3_ax = fig1.add_subplot(fig1_gridspec[2:4,7:11])\n",
    "fig1b4_ax = fig1.add_subplot(fig1_gridspec[2:4,12:])\n",
    "fig1b5_ax = fig1.add_subplot(fig1_gridspec[4:,11:])\n",
    "fig1b6_ax = fig1.add_subplot(fig1_gridspec[4:,6:10])\n",
    "fig1b7_ax = fig1.add_subplot(fig1_gridspec[4:,:5])\n",
    "\n",
    "\n",
    "def draw_schematic_invertible_cdss_on_fig_and_ax(fig, ax, cds_height, ir_pair_color, no_ir_pairs=False, larger_region=False, \n",
    "                                                 nearby_recombinase=False, no_cds_pairs=False, very_long_repeats=False, \n",
    "                                                 cds_pairs_outside_invertible_region=False, draw_alignment_hourglass=False):\n",
    "    region_1a_end = 11500\n",
    "    fig1a_region = (1, region_1a_end)\n",
    "    if larger_region:\n",
    "        fig1a_region = (fig1a_region[0] - 2000, fig1a_region[1] + 2000)\n",
    "    \n",
    "    if no_cds_pairs:\n",
    "        cds_1a_df = pd.DataFrame()\n",
    "    elif cds_pairs_outside_invertible_region:\n",
    "        cds_1a_df = pd.DataFrame([\n",
    "            {\n",
    "                'start_pos': -800,\n",
    "                'end_pos': 1600,\n",
    "                'strand': 1,\n",
    "                'product': '',\n",
    "            },\n",
    "            {\n",
    "                'start_pos': 9400,\n",
    "                'end_pos': 12200,\n",
    "                'strand': 1,\n",
    "                'product': '',\n",
    "            },\n",
    "        ])\n",
    "    else:\n",
    "        cds_1a_df = pd.DataFrame([\n",
    "            {\n",
    "                'start_pos': 1700,\n",
    "                'end_pos': 4500,\n",
    "                'strand': 1,\n",
    "                'product': '',\n",
    "            },\n",
    "            {\n",
    "                'start_pos': 6500,\n",
    "                'end_pos': 10500,\n",
    "                'strand': -1,\n",
    "                'product': '',\n",
    "            },\n",
    "            *([{\n",
    "                'start_pos': -1400,\n",
    "                'end_pos': 600,\n",
    "                'strand': -1,\n",
    "                'product': 'Rec',\n",
    "            }] if nearby_recombinase else [])\n",
    "        ])\n",
    "    \n",
    "\n",
    "    repeat_len = 2000 if very_long_repeats else 800 \n",
    "    cds_arrowhead_width = 300\n",
    "    \n",
    "    right1_1a = 3100\n",
    "    left2_1a = 7900\n",
    "    \n",
    "    left1_1a = right1_1a - repeat_len\n",
    "    right2_1a = left2_1a + repeat_len\n",
    "    pairs_1a_df = pd.DataFrame([\n",
    "        {\n",
    "            'left1': left1_1a,\n",
    "            'right1': right1_1a,\n",
    "            'left2': left2_1a,\n",
    "            'right2': right2_1a,\n",
    "            'color': ir_pair_color,\n",
    "        },\n",
    "    ])\n",
    "    intervals_1a = (\n",
    "        (fig1a_region[0], right1_1a),\n",
    "        (right1_1a + 1, left2_1a - 1),\n",
    "        (left2_1a, fig1a_region[1]),\n",
    "    )\n",
    "    interval_to_matching_interval_and_strand_1a = {\n",
    "        intervals_1a[0]: (intervals_1a[0], 1),\n",
    "        intervals_1a[1]: (intervals_1a[1], -1),\n",
    "        intervals_1a[2]: (intervals_1a[2], 1),\n",
    "    }\n",
    "\n",
    "    region_top = 0.85\n",
    "    matching_region_bottom = 0.15\n",
    "    \n",
    "    region_bottom = region_top - cds_height\n",
    "    region_bottom_and_top = (region_bottom, region_top)\n",
    "    matching_region_top = matching_region_bottom + cds_height\n",
    "    \n",
    "    \n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax,\n",
    "        region=fig1a_region,\n",
    "        cds_df=cds_1a_df,\n",
    "        ir_pairs_df=(pd.DataFrame() if no_ir_pairs else pairs_1a_df),\n",
    "        interval_to_matching_interval_and_strand=interval_to_matching_interval_and_strand_1a,\n",
    "        cds_arrowhead_width=cds_arrowhead_width,\n",
    "        region_bottom_and_top=region_bottom_and_top,\n",
    "        matching_region_bottom_and_top=(matching_region_bottom, matching_region_top),\n",
    "        dont_show_product_name=False,\n",
    "    )\n",
    "    \n",
    "    if draw_alignment_hourglass:\n",
    "        ax.add_patch(matplotlib.patches.Polygon([\n",
    "            (intervals_1a[1][0], region_bottom), \n",
    "            (intervals_1a[1][1], region_bottom), \n",
    "            (intervals_1a[1][0], matching_region_top), \n",
    "            (intervals_1a[1][1], matching_region_top), \n",
    "        ], closed=True, fill=True, facecolor=ALIGNMENT_PROJECTION_COLOR, alpha=ALIGNMENT_PROJECTION_ALPHA, edgecolor=ALIGNMENT_PROJECTION_EDGE_COLOR,\n",
    "        linewidth=ALIGNMENT_PROJECTION_LINE_WIDTH))\n",
    "\n",
    "    set_x_lim_according_to_region(ax, fig1a_region, margin_relative_size=0.1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_pipeline_on_fig_and_axes(fig, ax1, ax2, ax3, ax4, ax5, ax6, ax7, cds_height, dist_between_desc_and_cds,\n",
    "                                  ir_pair1_color, ir_pair2_color, same_variant_in_other_genome_in_ax7=False):\n",
    "    scaffold_break_linewidth = 1.3\n",
    "    \n",
    "    redX_alpha = 0.8\n",
    "    redX_size = 100\n",
    "    redX_linewidths = 3\n",
    "    \n",
    "    cluster_alpha = 0.4\n",
    "    cluster_linewidth = 1.5\n",
    "    \n",
    "    question_mark_rectangle_linewidth = 1.5\n",
    "    \n",
    "\n",
    "    dist_between_desc_and_cds_with_cluster = dist_between_desc_and_cds + 0.02\n",
    "    \n",
    "    vertical_dist_between_region_in_ref_and_other_genome = 0.15\n",
    "    \n",
    "    remove_all_ax_spines_an_x_and_y_axes(ax1)\n",
    "    \n",
    "    if num_of_taxa >= 1e3:\n",
    "        num_of_taxa_str_repr = generic_utils.get_num_rounded_to_thousands_str_repr(num_of_taxa, 1)\n",
    "    else:\n",
    "        num_of_taxa_str_repr = str(int(num_of_taxa))\n",
    "\n",
    "    if num_of_all_cds >= 1e6:\n",
    "        num_of_all_cds_str_repr = generic_utils.get_num_rounded_to_millions_str_repr(num_of_all_cds)\n",
    "    else:\n",
    "        num_of_all_cds_str_repr = generic_utils.get_num_rounded_to_thousands_str_repr(num_of_all_cds)\n",
    "\n",
    "    if num_of_all_ir_pairs >= 1e6:\n",
    "        num_of_all_ir_pairs_str_repr = generic_utils.get_num_rounded_to_millions_str_repr(num_of_all_ir_pairs, 1)\n",
    "    else:\n",
    "        num_of_all_ir_pairs_str_repr = generic_utils.get_num_rounded_to_thousands_str_repr(num_of_all_ir_pairs, 1)\n",
    "\n",
    "    orig_num_of_cds_pairs = len(ir_pairs_linked_df[index_column_names.CDS_PAIR_INDEX_COLUMN_NAMES].drop_duplicates())\n",
    "    orig_num_of_cds_pairs_str_repr = generic_utils.get_num_rounded_to_thousands_str_repr(\n",
    "        orig_num_of_cds_pairs)\n",
    "\n",
    "    print(f'num_of_taxa: '\n",
    "          f'{num_of_taxa}')\n",
    "    print(f'orig_num_of_cds_pairs: '\n",
    "          f'{orig_num_of_cds_pairs}')\n",
    "\n",
    "    num_of_cds_pairs_after_discarding_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num = len(\n",
    "        pairs_after_discarding_ir_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num_df[\n",
    "            index_column_names.CDS_PAIR_INDEX_COLUMN_NAMES].drop_duplicates())\n",
    "    num_of_cds_pairs_after_discarding_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num_str_repr = (\n",
    "        generic_utils.get_num_rounded_to_thousands_str_repr(\n",
    "            num_of_cds_pairs_after_discarding_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num))\n",
    "\n",
    "    print(f'num_of_cds_pairs_after_discarding_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num: '\n",
    "          f'{num_of_cds_pairs_after_discarding_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num}')\n",
    "\n",
    "    stage5_num_of_cds_pairs_with_any_homologous_locus_str_repr = generic_utils.get_num_rounded_to_thousands_str_repr(stage5_num_of_cds_pairs_with_any_homologous_locus)\n",
    "    print(f'stage5_num_of_cds_pairs_with_any_homologous_locus: {stage5_num_of_cds_pairs_with_any_homologous_locus}')\n",
    "\n",
    "    num_of_cds_pairs_with_high_confidence_ir_pairs = len(\n",
    "        filtered_pairs_with_highest_confidence_bps_df[index_column_names.CDS_PAIR_INDEX_COLUMN_NAMES].drop_duplicates())\n",
    "    num_of_cds_pairs_with_high_confidence_ir_pairs_str_repr = str(num_of_cds_pairs_with_high_confidence_ir_pairs)\n",
    "\n",
    "\n",
    "    ax1.text(0.38, 0.5, f'annotated species\\nrepresentative\\ngenomes ({num_of_taxa_str_repr})', \n",
    "                   va='center', ha='center')\n",
    "\n",
    "    arrow_edge_dist_from_y_middle_in_fig1b1 = 0.05\n",
    "    arrow_edge_dist_from_y_middle_in_fig1b2 = 0.14\n",
    "    \n",
    "    for sign in (1, -1):\n",
    "        fig.add_artist(matplotlib.patches.ConnectionPatch(\n",
    "            (0.97, 0.5 + arrow_edge_dist_from_y_middle_in_fig1b1 * sign),\n",
    "            (0.06, 0.5 + arrow_edge_dist_from_y_middle_in_fig1b2 * sign),\n",
    "            coordsA=ax1.transAxes,\n",
    "            coordsB=ax2.transAxes,\n",
    "            color=\"black\",\n",
    "            arrowstyle=\"-|>\",\n",
    "        ))\n",
    "\n",
    "\n",
    "    ax2_region = (200, 1800)\n",
    "    ax2_region_middle = np.mean(ax2_region)\n",
    "    ax2_cds_arrowhead_width = 110\n",
    "\n",
    "    desc_bottom = 0.72\n",
    "    ax2.text(ax2_region_middle, desc_bottom, f'coding sequences\\n(CDSs, {num_of_all_cds_str_repr})', va='bottom', ha='center')\n",
    "    region_top = desc_bottom - dist_between_desc_and_cds\n",
    "    region_bottom = region_top - cds_height\n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax2,\n",
    "        region=ax2_region,\n",
    "        cds_df=pd.DataFrame([{'start_pos': 700,\n",
    "                              'end_pos': 1300,\n",
    "                              'strand': 1,\n",
    "                              'product': '',}]),\n",
    "        cds_arrowhead_width=ax2_cds_arrowhead_width,\n",
    "        region_bottom_and_top=(region_bottom, region_top),\n",
    "        # cds_polygon_linewidth=2.2,\n",
    "    )\n",
    "\n",
    "    desc_bottom = 0.2\n",
    "    ax2.text(ax2_region_middle, desc_bottom, f'Inverted Repeats\\n(IRs, {num_of_all_ir_pairs_str_repr})', va='bottom', ha='center')\n",
    "    region_top = desc_bottom - dist_between_desc_and_cds\n",
    "    region_bottom = region_top - cds_height\n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax2,\n",
    "        region=ax2_region,\n",
    "        ir_pairs_df=pd.DataFrame([\n",
    "            {\n",
    "                'left1': 500,\n",
    "                'right1': 800,\n",
    "                'left2': 1200,\n",
    "                'right2': 1500,\n",
    "                'color': ir_pair1_color,\n",
    "            },\n",
    "        ]),\n",
    "        region_bottom_and_top=(region_bottom, region_top),\n",
    "    )\n",
    "\n",
    "    set_x_lim_according_to_region(ax2, ax2_region, margin_relative_size=0.4)\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    for sign in (1, -1):\n",
    "        fig.add_artist(matplotlib.patches.ConnectionPatch(\n",
    "            (0.95, 0.5 + arrow_edge_dist_from_y_middle_in_fig1b2 * sign),\n",
    "            (0.1, 0.5 + arrow_edge_dist_from_y_middle_in_fig1b1 * sign),\n",
    "            coordsA=ax2.transAxes,\n",
    "            coordsB=ax3.transAxes,\n",
    "            color=\"black\",\n",
    "            arrowstyle=\"-|>\",\n",
    "        ))\n",
    "\n",
    "\n",
    "\n",
    "    ax3_region = (-300, 2300)\n",
    "    ax3_region_middle = np.mean(ax3_region)\n",
    "    ax3_cds_arrowhead_width = 160\n",
    "\n",
    "    desc_bottom = 0.45\n",
    "    ax3.text(ax3_region_middle, desc_bottom, \n",
    "                   f'CDS pairs with\\n IRs ({orig_num_of_cds_pairs_str_repr})', va='bottom', ha='center')\n",
    "\n",
    "    region_top = desc_bottom - dist_between_desc_and_cds\n",
    "    region_bottom = region_top - cds_height\n",
    "\n",
    "    cds1_start = -50\n",
    "    cds2_end = 2050\n",
    "    repeat_len = 280\n",
    "    left1 = cds1_start + 250\n",
    "    right2 = cds2_end - 130\n",
    "    \n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax3,\n",
    "        region=ax3_region,\n",
    "        cds_df=pd.DataFrame([{'start_pos': cds1_start,\n",
    "                              'end_pos': 950,\n",
    "                              'strand': 1,\n",
    "                              'product': '',},\n",
    "                             {'start_pos': 1250,\n",
    "                              'end_pos': cds2_end,\n",
    "                              'strand': -1,\n",
    "                              'product': '',}]),\n",
    "        ir_pairs_df=pd.DataFrame([\n",
    "            {\n",
    "                'left1': left1,\n",
    "                'right1': left1 + repeat_len ,\n",
    "                'left2': right2 - repeat_len,\n",
    "                'right2': right2,\n",
    "                'color': ir_pair1_color,\n",
    "            },\n",
    "        ]),\n",
    "        cds_arrowhead_width=ax3_cds_arrowhead_width,\n",
    "        region_bottom_and_top=(region_bottom, region_top),\n",
    "        # cds_polygon_linewidth=1,\n",
    "    )\n",
    "    set_x_lim_according_to_region(ax3, ax3_region, margin_relative_size=0.25)\n",
    "    ax3.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "    fig.add_artist(matplotlib.patches.ConnectionPatch(\n",
    "        (0.94, 0.5),\n",
    "        (-0.18, 0.5),\n",
    "        coordsA=ax3.transAxes,\n",
    "        coordsB=ax4.transAxes,\n",
    "        color=\"black\",\n",
    "        arrowstyle=\"-|>\",\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "    ax4_cds_arrowhead_width = 170\n",
    "    ax4_repeat1_len = 280\n",
    "    repeat_leftmost_copy_left = -600\n",
    "    repeat_rightmost_copy_left = 3050\n",
    "    ax4_region = (repeat_leftmost_copy_left - 200, repeat_rightmost_copy_left + ax4_repeat1_len + 200)\n",
    "    ax4_region_middle = np.mean(ax4_region)\n",
    "\n",
    "    desc_bottom = 0.33\n",
    "    ax4.text(ax4_region_middle, desc_bottom, \n",
    "             f'discard CDS pairs with\\nrepetitive IRs, yielding\\nprogrammed inversion\\ncandidates (PICs, '\n",
    "             f'{num_of_cds_pairs_after_discarding_pairs_linked_to_cds_containing_repeats_of_ir_pairs_with_high_repeat_estimated_copy_num_str_repr})', \n",
    "             va='bottom', ha='center')\n",
    "    # alternatives to \"yielding\":\n",
    "    # producing\n",
    "    # giving rise to\n",
    "\n",
    "    region_top = desc_bottom - dist_between_desc_and_cds\n",
    "    region_bottom = region_top - cds_height\n",
    "\n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax4,\n",
    "        region=ax4_region,\n",
    "        cds_df=pd.DataFrame([{'start_pos': 220,\n",
    "                              'end_pos': 1050,\n",
    "                              'strand': -1,\n",
    "                              'product': '',},\n",
    "                             {'start_pos': 1300,\n",
    "                              'end_pos': 2080,\n",
    "                              'strand': 1,\n",
    "                              'product': '',}]),\n",
    "        ir_pairs_df=pd.DataFrame([\n",
    "            {\n",
    "                'left1': 600,\n",
    "                'right1': 600 + ax4_repeat1_len,\n",
    "                'left2': 1470,\n",
    "                'right2': 1470 + ax4_repeat1_len,\n",
    "                'color': ir_pair1_color,\n",
    "            },\n",
    "        ] + [\n",
    "            {\n",
    "                'left1': left1,\n",
    "                'right1': left1 + ax4_repeat1_len,\n",
    "                'left2': left2,\n",
    "                'right2': left2 + ax4_repeat1_len,\n",
    "                'color': ir_pair1_color,\n",
    "            } for left1, left2 in (\n",
    "                (-111111, repeat_leftmost_copy_left),\n",
    "                (2600, 111111),\n",
    "                (repeat_rightmost_copy_left, 111111),\n",
    "            )\n",
    "        ]),\n",
    "        cds_arrowhead_width=ax4_cds_arrowhead_width,\n",
    "        region_bottom_and_top=(region_bottom, region_top),\n",
    "        # cds_polygon_linewidth=1,\n",
    "    )\n",
    "\n",
    "    region_top = region_bottom + cds_height\n",
    "    break_height = cds_height * 1.2\n",
    "    break_bottom = region_bottom - (break_height - cds_height) / 2\n",
    "    break_top = region_top + (break_height - cds_height) / 2\n",
    "\n",
    "    break_width = 300\n",
    "    half_break_width = break_width / 2\n",
    "    for break_x in (-50, 2350):\n",
    "        break_bottom_left = (break_x - half_break_width, break_bottom)\n",
    "        break_bottom_right = (break_x, break_bottom)\n",
    "        break_top_left = (break_x, break_top)\n",
    "        break_top_right = (break_x + half_break_width, break_top)\n",
    "\n",
    "        ax4.plot([break_bottom_left[0], break_top_left[0]], \n",
    "                      [break_bottom_left[1], break_top_left[1]], color='black', linewidth=scaffold_break_linewidth)\n",
    "        ax4.plot([break_bottom_right[0], break_top_right[0]], \n",
    "                      [break_bottom_right[1], break_top_right[1]], color='black', linewidth=scaffold_break_linewidth)\n",
    "        ax4.add_patch(matplotlib.patches.Polygon([break_bottom_left, break_bottom_right, break_top_right, break_top_left], \n",
    "                                                  closed=True, fill=True, facecolor='white', edgecolor=None))\n",
    "\n",
    "    ax4.scatter(ax4_region[0] - 400, region_bottom + cds_height / 2, color='red', marker='x', alpha=redX_alpha, s=redX_size, linewidths=redX_linewidths, clip_on=False)\n",
    "\n",
    "    set_x_lim_according_to_region(ax4, ax4_region, margin_relative_size=0.03)\n",
    "    ax4.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "    fig.add_artist(matplotlib.patches.ConnectionPatch(\n",
    "        (0.47, 0.05),\n",
    "        # (0.605, 0.85),\n",
    "        (0.54, 0.8),\n",
    "        coordsA=ax4.transAxes,\n",
    "        coordsB=ax5.transAxes,\n",
    "        color=\"black\",\n",
    "        arrowstyle=\"-|>\",\n",
    "    ))\n",
    "\n",
    "\n",
    "    ax5_region = (200, 4700)\n",
    "    ax5_region_middle = np.mean(ax5_region)\n",
    "    ax5_cds_arrowhead_width = 150\n",
    "    ax5_repeat1_len = 210\n",
    "\n",
    "    desc_bottom = 0.45\n",
    "    ax5.text(ax5_region_middle, desc_bottom, 'cluster\\noverlapping PICs', va='bottom', ha='center')\n",
    "    # forming PIC loci??\n",
    "    # to form PIC loci??\n",
    "\n",
    "    region_top = desc_bottom - dist_between_desc_and_cds_with_cluster\n",
    "    region_bottom = region_top - cds_height\n",
    "\n",
    "    right_cds_pair_start = 3000\n",
    "    right_cds_pair_end = 4500\n",
    "    right_cds_pair_len = 650\n",
    "    right_cds_pair_ir_pair_pre_repeat_region_len = round(0.5 * right_cds_pair_len)\n",
    "    right_cds_pair_ir_pair_repeat_len = round(0.25 * right_cds_pair_len)\n",
    "    right_cds_pair_ir_pair_left1 = right_cds_pair_start + right_cds_pair_ir_pair_pre_repeat_region_len\n",
    "    right_cds_pair_ir_pair_right2 = right_cds_pair_end - right_cds_pair_ir_pair_pre_repeat_region_len\n",
    "    \n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax5,\n",
    "        region=ax5_region,\n",
    "        cds_df=pd.DataFrame([{'start_pos': 400,\n",
    "                              'end_pos': 1050,\n",
    "                              'strand': -1,\n",
    "                              'product': '',},\n",
    "                             {'start_pos': 1300,\n",
    "                              'end_pos': 1900,\n",
    "                              'strand': 1,\n",
    "                              'product': '',},\n",
    "                             {'start_pos': 2100,\n",
    "                              'end_pos': 2600,\n",
    "                              'strand': 1,\n",
    "                              'product': '',},\n",
    "                             {'start_pos': right_cds_pair_start,\n",
    "                              'end_pos': right_cds_pair_start + right_cds_pair_len,\n",
    "                              'strand': -1,\n",
    "                              'product': '',},\n",
    "                             {'start_pos': right_cds_pair_end - right_cds_pair_len,\n",
    "                              'end_pos': right_cds_pair_end,\n",
    "                              'strand': 1,\n",
    "                              'product': '',},\n",
    "                            ]),\n",
    "        ir_pairs_df=pd.DataFrame([\n",
    "            {\n",
    "                'left1': 700,\n",
    "                'right1': 700 + ax5_repeat1_len,\n",
    "                'left2': 1420,\n",
    "                'right2': 1420 + ax5_repeat1_len,\n",
    "                'color': ir_pair2_color,\n",
    "            },\n",
    "            {\n",
    "                'left1': 111111,\n",
    "                'right1': 111111 + ax5_repeat1_len,\n",
    "                'left2': 2110,\n",
    "                'right2': 2110 + ax5_repeat1_len,\n",
    "                'color': ir_pair2_color,\n",
    "            },\n",
    "            {\n",
    "                'left1': right_cds_pair_ir_pair_left1,\n",
    "                'right1': right_cds_pair_ir_pair_left1 + right_cds_pair_ir_pair_repeat_len,\n",
    "                'left2': right_cds_pair_ir_pair_right2 - right_cds_pair_ir_pair_repeat_len,\n",
    "                'right2': right_cds_pair_ir_pair_right2,\n",
    "                'color': ir_pair1_color,\n",
    "            },\n",
    "        ]),\n",
    "        cds_arrowhead_width=ax5_cds_arrowhead_width,\n",
    "        region_bottom_and_top=(region_bottom, region_top),\n",
    "        # cds_polygon_linewidth=1,\n",
    "    )\n",
    "\n",
    "    cluster_height = cds_height * 1.5\n",
    "    cluster_top = region_top + (cluster_height - cds_height) / 2\n",
    "    cluster_bottom = region_bottom - (cluster_height - cds_height) / 2\n",
    "\n",
    "    for cluster_start, cluster_end in (\n",
    "        (400, 2600),\n",
    "        (right_cds_pair_start, right_cds_pair_end),\n",
    "    ):\n",
    "        ax5.add_patch(matplotlib.patches.Polygon([(cluster_start, cluster_bottom), \n",
    "                                                  (cluster_start, cluster_top), \n",
    "                                                  (cluster_end, cluster_top), \n",
    "                                                  (cluster_end, cluster_bottom)], \n",
    "                                                 closed=True, fill=False, alpha=cluster_alpha, linewidth=cluster_linewidth, linestyle=(0, (1,1))))\n",
    "\n",
    "    set_x_lim_according_to_region(ax5, ax5_region, margin_relative_size=0.03)\n",
    "    ax5.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    fig.add_artist(matplotlib.patches.ConnectionPatch(\n",
    "        (-0.06, 0.5),\n",
    "        (1.03, 0.5),\n",
    "        coordsA=ax5.transAxes,\n",
    "        coordsB=ax6.transAxes,\n",
    "        color=\"black\",\n",
    "        arrowstyle=\"-|>\",\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax6_region = (200, 2100)\n",
    "    ax6_region_middle = np.mean(ax6_region)\n",
    "    ax6_cds_arrowhead_width = 100\n",
    "    ax6_repeat1_len = 110\n",
    "\n",
    "    desc_bottom = 0.53\n",
    "    ax6.text(ax6_region_middle, desc_bottom, \n",
    "                   f'PICs with homologous\\nloci in same-species\\ngenomes ({stage5_num_of_cds_pairs_with_any_homologous_locus_str_repr})', va='bottom', ha='center')\n",
    "\n",
    "    region_top = desc_bottom - dist_between_desc_and_cds_with_cluster\n",
    "    region_bottom = region_top - cds_height\n",
    "\n",
    "    cds1_start = 750\n",
    "    cds1_end = 1200\n",
    "    cds2_start = 1350\n",
    "    cds2_end = 1800\n",
    "    left1 = cds1_start + 240\n",
    "    right2 = cds2_end - 240\n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax6,\n",
    "        region=ax6_region,\n",
    "        cds_df=pd.DataFrame([{'start_pos': cds1_start,\n",
    "                              'end_pos': cds1_end,\n",
    "                              'strand': -1,\n",
    "                              'product': '',},\n",
    "                             {'start_pos': cds2_start,\n",
    "                              'end_pos': cds2_end,\n",
    "                              'strand': 1,\n",
    "                              'product': '',},\n",
    "                            ]),\n",
    "        ir_pairs_df=pd.DataFrame([\n",
    "            {\n",
    "                'left1': left1,\n",
    "                'right1': left1 + ax6_repeat1_len,\n",
    "                'left2': right2 - ax6_repeat1_len,\n",
    "                'right2': right2,\n",
    "                'color': ir_pair1_color,\n",
    "            },\n",
    "        ]),\n",
    "        cds_arrowhead_width=ax6_cds_arrowhead_width,\n",
    "        region_bottom_and_top=(region_bottom, region_top),\n",
    "        # cds_polygon_linewidth=1,\n",
    "    )\n",
    "\n",
    "    \n",
    "    other_nuccore_region_top = region_bottom - vertical_dist_between_region_in_ref_and_other_genome\n",
    "    other_nuccore_region_bottom = other_nuccore_region_top - cds_height\n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax6,\n",
    "        region=ax6_region,\n",
    "        region_bottom_and_top=(other_nuccore_region_bottom, other_nuccore_region_top),\n",
    "        # cds_polygon_linewidth=1,\n",
    "    )\n",
    "    \n",
    "    cluster_top = region_top + (cluster_height - cds_height) / 2\n",
    "    cluster_bottom = region_bottom - (cluster_height - cds_height) / 2\n",
    "    ax6.add_patch(matplotlib.patches.Polygon([(cds1_start, cluster_bottom), \n",
    "                                              (cds1_start, cluster_top), \n",
    "                                              (cds2_end, cluster_top), \n",
    "                                              (cds2_end, cluster_bottom)], \n",
    "                                             closed=True, fill=False, alpha=cluster_alpha, linewidth=cluster_linewidth, linestyle=(0, (1,1))))\n",
    "\n",
    "    position_offset = -150\n",
    "    rectangle_height = cds_height * 1.2\n",
    "    rectangle_top = other_nuccore_region_top + (rectangle_height - cds_height) / 2\n",
    "    rectangle_bottom = other_nuccore_region_bottom - (rectangle_height - cds_height) / 2\n",
    "    rectangle_start = cds1_start + position_offset\n",
    "    rectangle_end = cds2_end + position_offset\n",
    "    ax6.add_patch(matplotlib.patches.Polygon([(rectangle_start, rectangle_bottom), \n",
    "                                              (rectangle_start, rectangle_top), \n",
    "                                              (rectangle_end, rectangle_top), \n",
    "                                              (rectangle_end, rectangle_bottom)], \n",
    "                                             closed=True, fill=True, facecolor='white', edgecolor='darkslategrey', alpha=1, \n",
    "                                             linewidth=question_mark_rectangle_linewidth))\n",
    "    ax6.text(np.mean([rectangle_start, rectangle_end]), np.mean([rectangle_bottom, rectangle_top]) - 0.009, \n",
    "                  '?', va='center', ha='center', fontsize=11)\n",
    "\n",
    "    margin_size = 200\n",
    "\n",
    "    ax6.add_patch(matplotlib.patches.Polygon([\n",
    "        (rectangle_start - margin_size, other_nuccore_region_top), \n",
    "        (rectangle_start, other_nuccore_region_top), \n",
    "        (cds1_start, region_bottom), \n",
    "        (cds1_start - margin_size, region_bottom), \n",
    "    ], closed=True, fill=True, facecolor=ALIGNMENT_PROJECTION_COLOR, alpha=ALIGNMENT_PROJECTION_ALPHA, edgecolor=ALIGNMENT_PROJECTION_EDGE_COLOR,\n",
    "        linewidth=ALIGNMENT_PROJECTION_LINE_WIDTH))\n",
    "\n",
    "    ax6.add_patch(matplotlib.patches.Polygon([\n",
    "        (rectangle_end, other_nuccore_region_top), \n",
    "        (rectangle_end + margin_size, other_nuccore_region_top), \n",
    "        (cds2_end + margin_size, region_bottom), \n",
    "        (cds2_end, region_bottom), \n",
    "    ], closed=True, fill=True, facecolor=ALIGNMENT_PROJECTION_COLOR, alpha=ALIGNMENT_PROJECTION_ALPHA, edgecolor=ALIGNMENT_PROJECTION_EDGE_COLOR,\n",
    "        linewidth=ALIGNMENT_PROJECTION_LINE_WIDTH))\n",
    "\n",
    "    set_x_lim_according_to_region(ax6, ax6_region, margin_relative_size=0.15)\n",
    "    ax6.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "\n",
    "    fig.add_artist(matplotlib.patches.ConnectionPatch(\n",
    "        (-0.04, 0.5),\n",
    "        (1.07, 0.5),\n",
    "        coordsA=ax6.transAxes,\n",
    "        coordsB=ax7.transAxes,\n",
    "        color=\"black\",\n",
    "        arrowstyle=\"-|>\",\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax7_region = (270, 2400)\n",
    "    ax7_region_middle = np.mean(ax7_region)\n",
    "    ax7_cds_arrowhead_width = 70\n",
    "    ax7_repeat1_len = 110\n",
    "\n",
    "    desc_bottom = 0.6\n",
    "    ax7.text(ax7_region_middle, desc_bottom, \n",
    "                   # f'CDS pairs with\\nrearrangements ({num_of_cds_pairs_with_high_confidence_ir_pairs_str_repr})', \n",
    "                   f'PICs with\\nvariation ({num_of_cds_pairs_with_high_confidence_ir_pairs_str_repr})', \n",
    "                   # f'variable\\nCDS pairs ({num_of_cds_pairs_with_high_confidence_ir_pairs_str_repr})', \n",
    "             va='bottom', ha='center')\n",
    "\n",
    "    region_top = desc_bottom - dist_between_desc_and_cds_with_cluster\n",
    "    region_bottom = region_top - cds_height\n",
    "\n",
    "    cds1_start = 650\n",
    "    cds1_end = 1150\n",
    "    cds2_start = 1650\n",
    "    cds2_end = 2150\n",
    "    left1 = cds1_end - ax7_repeat1_len - 200\n",
    "    right2 = cds2_start + ax7_repeat1_len + 150\n",
    "\n",
    "    right1 = left1 + ax7_repeat1_len\n",
    "    left2 = right2 - ax7_repeat1_len\n",
    "\n",
    "    spacer_start = left1 + ax7_repeat1_len + 1\n",
    "    spacer_end = right2 - ax7_repeat1_len - 1\n",
    "    spacer_size = spacer_end - spacer_start\n",
    "\n",
    "    intervals = (\n",
    "        (ax7_region[0], spacer_start - 1),\n",
    "        (spacer_start, spacer_end),\n",
    "        (spacer_end + 1, ax7_region[1]),\n",
    "    )\n",
    "\n",
    "    position_offset = -110\n",
    "\n",
    "    left1_in_other_nuccore = left1 + position_offset\n",
    "    right2_in_other_nuccore = right2 + position_offset\n",
    "\n",
    "    other_nuccore_region_top = region_bottom - vertical_dist_between_region_in_ref_and_other_genome\n",
    "    other_nuccore_region_bottom = other_nuccore_region_top - cds_height\n",
    "\n",
    "    other_nuccore_cds1_start = cds1_start + position_offset\n",
    "    other_nuccore_cds2_end = cds2_end + position_offset\n",
    "    other_nuccore_right1 = other_nuccore_cds1_start + (right1 - cds1_start)\n",
    "    other_nuccore_left2 = other_nuccore_cds2_end - (cds2_end - left2)\n",
    "\n",
    "    matching_intervals = [(x[0] + position_offset, x[1] + position_offset) for x in intervals]\n",
    "    \n",
    "    \n",
    "    right_inversion_interval_to_matching_interval_and_strand = {\n",
    "        intervals[0]: (matching_intervals[0], 1),\n",
    "        intervals[1]: (matching_intervals[1], (1 if same_variant_in_other_genome_in_ax7 else -1)),\n",
    "        intervals[2]: (matching_intervals[2], 1),\n",
    "    }\n",
    "\n",
    "    \n",
    "    ir_pairs_df = pd.DataFrame([\n",
    "        {\n",
    "            'left1': left1,\n",
    "            'right1': right1,\n",
    "            'left2': left2,\n",
    "            'right2': right2,\n",
    "            'color': ir_pair1_color,\n",
    "        },\n",
    "    ])\n",
    "\n",
    "    \n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax7,\n",
    "        region=ax7_region,\n",
    "        cds_df=pd.DataFrame([{'start_pos': cds1_start,\n",
    "                              'end_pos': cds1_end,\n",
    "                              'strand': -1,\n",
    "                              'product': '',},\n",
    "                             {'start_pos': cds2_start,\n",
    "                              'end_pos': cds2_end,\n",
    "                              'strand': 1,\n",
    "                              'product': '',},\n",
    "                            ]),\n",
    "        ir_pairs_df=ir_pairs_df,\n",
    "        region_with_gradient=(spacer_start, spacer_end),\n",
    "        interval_to_matching_interval_and_strand=right_inversion_interval_to_matching_interval_and_strand,\n",
    "        cds_arrowhead_width=ax7_cds_arrowhead_width,\n",
    "        region_bottom_and_top=(region_bottom, region_top),\n",
    "        matching_region_bottom_and_top=(other_nuccore_region_bottom, other_nuccore_region_top),\n",
    "        # draw_matching_region_cds=False,\n",
    "        # cds_polygon_linewidth=1,\n",
    "    )\n",
    "    \n",
    "    cluster_top = region_top + (cluster_height - cds_height) / 2\n",
    "    cluster_bottom = region_bottom - (cluster_height - cds_height) / 2\n",
    "    ax7.add_patch(matplotlib.patches.Polygon([(cds1_start, cluster_bottom), \n",
    "                                              (cds1_start, cluster_top), \n",
    "                                              (cds2_end, cluster_top), \n",
    "                                              (cds2_end, cluster_bottom)], \n",
    "                                             closed=True, fill=False, alpha=cluster_alpha, linewidth=cluster_linewidth, linestyle=(0, (1,1))))\n",
    "\n",
    "    margin_size = 200\n",
    "    ax7.add_patch(matplotlib.patches.Polygon([\n",
    "        (other_nuccore_cds1_start - margin_size, other_nuccore_region_top), \n",
    "        (other_nuccore_cds1_start, other_nuccore_region_top), \n",
    "        (cds1_start, region_bottom), \n",
    "        (cds1_start - margin_size, region_bottom), \n",
    "    ], closed=True, fill=True, facecolor=ALIGNMENT_PROJECTION_COLOR, alpha=ALIGNMENT_PROJECTION_ALPHA, edgecolor=ALIGNMENT_PROJECTION_EDGE_COLOR,\n",
    "        linewidth=ALIGNMENT_PROJECTION_LINE_WIDTH))\n",
    "\n",
    "    ax7.add_patch(matplotlib.patches.Polygon([\n",
    "        (other_nuccore_cds2_end, other_nuccore_region_top), \n",
    "        (other_nuccore_cds2_end + margin_size, other_nuccore_region_top), \n",
    "        (cds2_end + margin_size, region_bottom), \n",
    "        (cds2_end, region_bottom), \n",
    "    ], closed=True, fill=True, facecolor=ALIGNMENT_PROJECTION_COLOR, alpha=ALIGNMENT_PROJECTION_ALPHA, edgecolor=ALIGNMENT_PROJECTION_EDGE_COLOR,\n",
    "        linewidth=ALIGNMENT_PROJECTION_LINE_WIDTH))\n",
    "    \n",
    "    ax7.add_patch(matplotlib.patches.Polygon([\n",
    "        (other_nuccore_right1, other_nuccore_region_top), \n",
    "        (other_nuccore_left2, other_nuccore_region_top), \n",
    "        (right1, region_bottom), \n",
    "        (left2, region_bottom), \n",
    "    ], closed=True, fill=True, facecolor=ALIGNMENT_PROJECTION_COLOR, alpha=ALIGNMENT_PROJECTION_ALPHA, edgecolor=ALIGNMENT_PROJECTION_EDGE_COLOR,\n",
    "        linewidth=ALIGNMENT_PROJECTION_LINE_WIDTH))\n",
    "\n",
    "\n",
    "    set_x_lim_according_to_region(ax7, ax7_region, margin_relative_size=0.03)\n",
    "    ax7.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "def draw_snp_distance_between_rearranged_loci_histogram(fig, ax):\n",
    "    distance_column = (1 - filtered_pairs_with_highest_confidence_bps_df.groupby(index_column_names.MERGED_CDS_PAIR_REGION_INDEX_COLUMN_NAMES)[\n",
    "        'mauve_total_match_proportion_of_region_in_other_nuccore_with_highest_confidence_bps'].max()) * 100\n",
    "    max_distance = distance_column.max()\n",
    "    # print(distance_column.describe())\n",
    "\n",
    "    # bins = list(np.linspace(0, max_distance, 13))\n",
    "    # bin_size = 0.002 # for fraction\n",
    "    bin_size = 0.2 # for percentage\n",
    "    bins = np.arange(0, max_distance + bin_size, bin_size)\n",
    "\n",
    "    assert distance_column.min() >= bins[0]\n",
    "    assert distance_column.max() <= bins[-1]\n",
    "    \n",
    "    hist_color = (*matplotlib.colors.to_rgb('grey'), 1)\n",
    "    hist_color = (0.5,0.5,0.5)\n",
    "    hist_color = 'grey'\n",
    "    hist_color = GREY_HIST_COLOR\n",
    "    ax.hist(\n",
    "        distance_column,\n",
    "        bins=bins,\n",
    "        color=hist_color,\n",
    "        # color='grey',\n",
    "        # edgecolor=(*matplotlib.colors.to_rgb('grey'), 0.1),\n",
    "        edgecolor=hist_color,\n",
    "        # histtype='step', \n",
    "        # linewidth=8,\n",
    "        # alpha=0.75, # ugh. just use the annoying grey and that's it.\n",
    "    )\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    ax.set_ylabel(\n",
    "        # '# rearranged loci',\n",
    "        # '# loci with variation',\n",
    "        # '# variable loci',\n",
    "        '# PIC loci',\n",
    "        # fontsize=9,\n",
    "    )\n",
    "    ax.set_xlabel(\n",
    "        # 'SNP distance between\\nrearranged loci (%)', \n",
    "        'SNP distance between\\nlocus variants (%)', \n",
    "        # fontsize=9,\n",
    "    )\n",
    "    ax.set_xlim(0, bins[-1])\n",
    "\n",
    "    ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%g'))\n",
    "\n",
    "# ir_pair1_color = BLUES_CMAP\n",
    "# ir_pair1_color = BLUE_PURPLE_CMAP\n",
    "# ir_pair2_color = 'Reds_r'\n",
    "# ir_pair2_color = REDS_CMAP\n",
    "# ir_pair2_color = RED_GOLD_CMAP\n",
    "# ir_pair2_color = RED_ORANGE_CMAP\n",
    "# ir_pair2_color = RED_MAGENTA_CMAP\n",
    "\n",
    "ir_pair1_color = COLORBREWER_SET1_WITHOUT_GREY_AND_YELLOW[1]\n",
    "ir_pair2_color = COLORBREWER_SET1_WITHOUT_GREY_AND_YELLOW[2]\n",
    "    \n",
    "draw_schematic_invertible_cdss_on_fig_and_ax(\n",
    "    fig1, fig1a_ax, cds_height=0.14, ir_pair_color=ir_pair1_color, \n",
    "    draw_alignment_hourglass=True,\n",
    ")\n",
    "draw_pipeline_on_fig_and_axes(\n",
    "    fig1, fig1b1_ax, fig1b2_ax, fig1b3_ax, fig1b4_ax, fig1b5_ax, fig1b6_ax, fig1b7_ax, cds_height=0.12, dist_between_desc_and_cds=0.04,\n",
    "    ir_pair1_color=ir_pair1_color, ir_pair2_color=ir_pair2_color,\n",
    "    # same_variant_in_other_genome_in_ax7=True,\n",
    ")\n",
    "draw_snp_distance_between_rearranged_loci_histogram(fig1, fig1c_ax)\n",
    "\n",
    "fig1a_ax.text(0.02, 0.98, 'A', va='center', ha='left', transform=matplotlib.transforms.blended_transform_factory(fig1.transFigure, fig1a_ax.transAxes), fontweight='bold')\n",
    "fig1b1_ax.text(0.02, 0.98, 'B', va='center', ha='left', transform=matplotlib.transforms.blended_transform_factory(fig1.transFigure, fig1b1_ax.transAxes), fontweight='bold')\n",
    "fig1c_ax.text(-0.35, 0.98, 'C', va='center', ha='left', transform=fig1c_ax.transAxes, fontweight='bold')\n",
    "\n",
    "print(f'len(extended_merged_cds_pair_region_df): {len(extended_merged_cds_pair_region_df)}')\n",
    "\n",
    "if 1:\n",
    "    fig1.savefig(os.path.join(paper_figs_and_tables_dir_path, 'fig1.png'))\n",
    "    fig1.savefig(os.path.join(paper_figs_and_tables_dir_path, 'fig1.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e18c4-9f72-45e3-9b2a-659442679022",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_to_critical_val_and_passed_threshold_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549262aa-ba6b-45e1-977f-54b5b0397600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'fig2' in locals():\n",
    "    plt.close(fig2)\n",
    "# raise    \n",
    "fig2 = plt.figure(\n",
    "    figsize=(5.9,8.8), \n",
    "    # constrained_layout=True,\n",
    ")\n",
    "# ugh. if the height of the histograms is too low, the minor ticks won't be showed \n",
    "# (i guess because they are too close to each other, which means forcing matplotlib to show them makes no sense)...\n",
    "fig2_gridspec = fig2.add_gridspec(\n",
    "    7, 1, \n",
    "    height_ratios=[0.6,0.4,1,0.65,1,0.6,0.8], \n",
    "    wspace=0, hspace=0,\n",
    ")\n",
    "\n",
    "fig2.subplots_adjust(\n",
    "    top=0.94,\n",
    "    bottom=0.06,\n",
    "    left=0,\n",
    "    right=0.92,\n",
    "    # hspace=0.2,\n",
    "    # wspace=0.2\n",
    ")\n",
    "\n",
    "fig2_upper_sub_gridspec = fig2_gridspec[0].subgridspec(1, 3, width_ratios=[0.02,1,0], wspace=0,hspace=0)\n",
    "fig2_upper_middle_sub_gridspec = fig2_gridspec[2].subgridspec(1, 5, width_ratios=[0.42,1,0.25,1,0.09], wspace=0,hspace=0)\n",
    "fig2_lower_middle_sub_gridspec = fig2_gridspec[4].subgridspec(1, 5, width_ratios=[0.42,1,0.25,1,0.09], wspace=0,hspace=0)\n",
    "fig2_lower_sub_gridspec = fig2_gridspec[-1].subgridspec(1, 5, width_ratios=[0.42,1,0.25,1,0.09], wspace=0,hspace=0)\n",
    "\n",
    "fig2a_ax = fig2.add_subplot(fig2_upper_sub_gridspec[1])\n",
    "fig2b_ax = fig2.add_subplot(fig2_upper_middle_sub_gridspec[1])\n",
    "fig2c_ax = fig2.add_subplot(fig2_upper_middle_sub_gridspec[3])\n",
    "fig2d_ax = fig2.add_subplot(fig2_lower_middle_sub_gridspec[1])\n",
    "fig2e_ax = fig2.add_subplot(fig2_lower_middle_sub_gridspec[3])\n",
    "fig2f_ax = fig2.add_subplot(fig2_lower_sub_gridspec[1:-1])\n",
    "\n",
    "\n",
    "def get_draw_attr_arrow(ax, upper_attr_line_y, upper_attr_text_y, lower_attr_line_y, lower_attr_text_y, attr_arrow_props):\n",
    "    def draw_attr_arrow(line_xs, text, is_upper):\n",
    "        if is_upper:\n",
    "            line_y = upper_attr_line_y\n",
    "            text_y = upper_attr_text_y\n",
    "            va = 'bottom'\n",
    "        else:\n",
    "            line_y = lower_attr_line_y\n",
    "            text_y = lower_attr_text_y\n",
    "            va = 'top'\n",
    "        \n",
    "        ax.annotate(\"\", xy=(line_xs[0], line_y), xytext=(line_xs[1], line_y), arrowprops=attr_arrow_props)\n",
    "        ax.text(np.mean(line_xs), text_y, text, va=va, ha='center')\n",
    "    return draw_attr_arrow\n",
    "    \n",
    "ATTR_ARROW_PROPS = dict(arrowstyle=\"<->\", shrinkA=0, shrinkB=0, linewidth=0.5)\n",
    "\n",
    "def draw_schematic_invertible_cds_attributes_on_fig_and_ax(fig, ax, cds_height, ir_pair1_color, ir_pair2_color):\n",
    "    region_top = 0.8\n",
    "    # dist_between_attr_line_and_cds = 0.1\n",
    "    dist_between_attr_line_and_cds = cds_height / 0.22 * 0.1\n",
    "    # dist_between_attr_line_and_text = 0.05\n",
    "    dist_between_attr_line_and_text = cds_height / 0.22 * 0.05\n",
    "    dist_between_lower_attr_text_and_formulas = 0.43\n",
    "    region_left_margin_len_as_fraction_of_region_len = 0.02\n",
    "    region_right_margin_len_as_fraction_of_region_len = 0.85\n",
    "    dist_between_region_and_formulas_as_fraction_of_region_len = 0.05\n",
    "    \n",
    "    \n",
    "    \n",
    "    cds_arrowhead_width = 100\n",
    "    r1 = 290\n",
    "    r2 = 350\n",
    "    \n",
    "    cds1_len = 1700\n",
    "    cds2_len = 1500\n",
    "    cds_distance = 330\n",
    "    cds1_start = 0\n",
    "    \n",
    "    o1 = 550\n",
    "    o2 = 300\n",
    "    i1 = 330\n",
    "    i2 = 350\n",
    "    \n",
    "    cds1_end = cds1_start + cds1_len\n",
    "    \n",
    "    cds2_start = cds1_end + cds_distance\n",
    "    cds2_end = cds2_start + cds2_len\n",
    "    \n",
    "    region = (cds1_start - 250, cds2_end + 250)\n",
    "    \n",
    "    cds1 = {\n",
    "        'start_pos': cds1_start,\n",
    "        'end_pos': cds1_end,\n",
    "        'strand': 1,\n",
    "        'product': '',\n",
    "    }\n",
    "    cds2 = {\n",
    "        'start_pos': cds2_start,\n",
    "        'end_pos': cds2_end,\n",
    "        'strand': -1,\n",
    "        'product': '',\n",
    "    }\n",
    "    \n",
    "    cds_df = pd.DataFrame([cds1, cds2])\n",
    "\n",
    "    ir_pair1_left1 = cds1['start_pos'] + o1\n",
    "    ir_pair1_right2 = cds2['end_pos'] - o2\n",
    "    ir_pair1 = {\n",
    "        'left1': ir_pair1_left1,\n",
    "        'right1': ir_pair1_left1 + r1,\n",
    "        'left2': ir_pair1_right2 - r1,\n",
    "        'right2': ir_pair1_right2,\n",
    "        'color': ir_pair1_color,\n",
    "    }\n",
    "    ir_pair2_left2 = cds1['end_pos'] - i1\n",
    "    ir_pair2_right1 = cds2['start_pos'] + i2\n",
    "    ir_pair2 = {\n",
    "        'left1': ir_pair2_left2 - r2,\n",
    "        'right1': ir_pair2_left2,\n",
    "        'left2': ir_pair2_right1,\n",
    "        'right2': ir_pair2_right1 + r2,\n",
    "        'color': ir_pair2_color,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    pairs_df = pd.DataFrame([ir_pair1, ir_pair2])\n",
    "\n",
    "    region_bottom = region_top - cds_height\n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax,\n",
    "        region=region,\n",
    "        cds_df=cds_df,\n",
    "        ir_pairs_df=pairs_df,\n",
    "        cds_arrowhead_width=cds_arrowhead_width,\n",
    "        region_bottom_and_top=(region_bottom, region_top),\n",
    "    )\n",
    "    \n",
    "    \n",
    "    upper_attr_line_y = region_top + dist_between_attr_line_and_cds\n",
    "    lower_attr_line_y = region_bottom - dist_between_attr_line_and_cds\n",
    "    upper_attr_text_y = upper_attr_line_y + dist_between_attr_line_and_text\n",
    "    lower_attr_text_y = lower_attr_line_y - dist_between_attr_line_and_text\n",
    "    \n",
    "    \n",
    "    draw_attr_arrow = get_draw_attr_arrow(ax, upper_attr_line_y, upper_attr_text_y, lower_attr_line_y, lower_attr_text_y, ATTR_ARROW_PROPS)\n",
    "        \n",
    "    \n",
    "    line_xs = [ir_pair1['left1'], ir_pair1['right1']]\n",
    "    draw_attr_arrow(line_xs, r'$r_1$', is_upper=True)\n",
    "    \n",
    "    line_xs = [ir_pair2['left1'], ir_pair2['right1']]\n",
    "    draw_attr_arrow(line_xs, r'$r_2$', is_upper=True)\n",
    "    \n",
    "    \n",
    "    line_xs = [cds1_start, ir_pair1['left1']]\n",
    "    draw_attr_arrow(line_xs, r'$u_L$', is_upper=True)\n",
    "    draw_attr_arrow(line_xs, r'$o_L$', is_upper=False)\n",
    "    \n",
    "    line_xs = [ir_pair1['right2'], cds2_end]\n",
    "    draw_attr_arrow(line_xs, r'$u_R$', is_upper=True)\n",
    "    draw_attr_arrow(line_xs, r'$o_R$', is_upper=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    line_xs = [ir_pair2['right1'], cds1_end]\n",
    "    draw_attr_arrow(line_xs, r'$i_L$', is_upper=False)\n",
    "    \n",
    "    line_xs = [cds2_start, ir_pair2['left2']]\n",
    "    draw_attr_arrow(line_xs, r'$i_R$', is_upper=False)\n",
    "    \n",
    "    line_xs = [cds1_end, cds2_start]\n",
    "    draw_attr_arrow(line_xs, '$d$', is_upper=True)\n",
    "\n",
    "    formulas_y = np.mean([region_top, region_bottom])\n",
    "    \n",
    "    region_len = region[1] - region[0]\n",
    "    left_x = region[0] - region_left_margin_len_as_fraction_of_region_len * region_len\n",
    "    right_x = region[1] + region_right_margin_len_as_fraction_of_region_len * region_len\n",
    "    formulas_left_x = region[1] + dist_between_region_and_formulas_as_fraction_of_region_len * region_len\n",
    "    \n",
    "    formula_vertical_dist_from_region_middle = 0.25\n",
    "    \n",
    "    \n",
    "    ax.text(\n",
    "        formulas_left_x, \n",
    "        formulas_y + 2.2 * formula_vertical_dist_from_region_middle, \n",
    "        r'repeat length$=\\max(r_1,r_2)$',\n",
    "        va='center', ha='left',\n",
    "    )\n",
    "    \n",
    "    ax.text(\n",
    "        formulas_left_x, \n",
    "        formulas_y + 0.7 * formula_vertical_dist_from_region_middle, \n",
    "        r'CDS distance$=d$', \n",
    "        va='center', ha='left',\n",
    "    )\n",
    "    \n",
    "    ax.text(\n",
    "        formulas_left_x, \n",
    "        formulas_y - 3 * formula_vertical_dist_from_region_middle, \n",
    "        r'asymmetry$=1-\\dfrac{\\min(u_L,u_R)}{\\max(u_L,u_R)}$', \n",
    "        va='center', ha='left',\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax.text(\n",
    "        formulas_left_x,\n",
    "        formulas_y - formula_vertical_dist_from_region_middle, \n",
    "        r'$\\dfrac{\\mathrm{orientation}}{\\mathrm{matching}}=\\dfrac{o_L+d+o_R}{(o_L+d+o_R)+(i_L+d+i_R)}$',\n",
    "        va='center', ha='left',\n",
    "    )\n",
    "    # an ugly hack to get rid of the fraction line - we just overwrite it with a white line. ugh.\n",
    "    ax.plot([formulas_left_x, formulas_left_x + region_len * 0.27], \n",
    "            [formulas_y - formula_vertical_dist_from_region_middle] * 2 , \n",
    "            color=COLOR_OF_HACKY_LINE_TO_OVERWRITE_FRACTION_LINE_OF_ORIENTATION_MATCHING, \n",
    "            linewidth=1.8, # 1.8 is fine with 300 dpi.\n",
    "            zorder=100)\n",
    "    \n",
    "    # set_x_lim_according_to_region(ax, region, margin_relative_size=0.03)\n",
    "    ax.set_xlim(left_x, right_x)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "def draw_schematic_orientation_matching_examples_on_fig_and_ax(fig, ax, cds_height, ir_pair1_color, ir_pair2_color, cds_and_repeat_info):\n",
    "    region_top = 0.88\n",
    "    # dist_between_attr_line_and_cds = 0.1\n",
    "    # dist_between_attr_line_and_text = 0.05\n",
    "    dist_between_attr_line_and_cds = cds_height / 0.22 * 0.1\n",
    "    dist_between_attr_line_and_text = cds_height / 0.22 * 0.05\n",
    "    dist_between_lower_attr_text_and_formulas = 0.43\n",
    "    region_left_margin_len_as_fraction_of_region_len = 0.02\n",
    "    region_right_margin_len_as_fraction_of_region_len = 0.35\n",
    "    dist_between_region_and_formulas_as_fraction_of_region_len = 0.05\n",
    "    \n",
    "    dist_between_region_bottom_and_opposite_orientation_region_top = 0.5\n",
    "    \n",
    "    \n",
    "    cds_arrowhead_width = cds_and_repeat_info['cds_arrowhead_width']\n",
    "    r1 = cds_and_repeat_info['r1']\n",
    "    r2 = cds_and_repeat_info['r2']\n",
    "    cds1_strand = cds_and_repeat_info['cds1_strand']\n",
    "    cds1_len = cds_and_repeat_info['cds1_len']\n",
    "    cds2_len = cds_and_repeat_info['cds2_len']\n",
    "    cds_distance = cds_and_repeat_info['cds_distance']\n",
    "    cds1_start = cds_and_repeat_info['cds1_start']\n",
    "    o1 = cds_and_repeat_info['o1']\n",
    "    o2 = cds_and_repeat_info['o2']\n",
    "    i1 = cds_and_repeat_info['i1']\n",
    "    i2 = cds_and_repeat_info['i2']\n",
    "    d = cds_distance\n",
    "    \n",
    "    cds1_end = cds1_start + cds1_len\n",
    "    \n",
    "    cds2_start = cds1_end + cds_distance\n",
    "    cds2_end = cds2_start + cds2_len\n",
    "    \n",
    "    region = (cds1_start - 250, cds2_end + 250)\n",
    "    \n",
    "    cds1 = {\n",
    "        'start_pos': cds1_start,\n",
    "        'end_pos': cds1_end,\n",
    "        'strand': cds1_strand,\n",
    "        'product': '',\n",
    "    }\n",
    "    cds2 = {\n",
    "        'start_pos': cds2_start,\n",
    "        'end_pos': cds2_end,\n",
    "        'strand': -cds1_strand,\n",
    "        'product': '',\n",
    "    }\n",
    "    \n",
    "    cds_df = pd.DataFrame([cds1, cds2])\n",
    "\n",
    "    ir_pair1_left1 = cds1['start_pos'] + o1\n",
    "    ir_pair1_right2 = cds2['end_pos'] - o2\n",
    "    ir_pair1 = {\n",
    "        'left1': ir_pair1_left1,\n",
    "        'right1': ir_pair1_left1 + r1,\n",
    "        'left2': ir_pair1_right2 - r1,\n",
    "        'right2': ir_pair1_right2,\n",
    "        'color': ir_pair1_color,\n",
    "    }\n",
    "    ir_pair2_left2 = cds1['end_pos'] - i1\n",
    "    ir_pair2_right1 = cds2['start_pos'] + i2\n",
    "    ir_pair2 = {\n",
    "        'left1': ir_pair2_left2 - r2,\n",
    "        'right1': ir_pair2_left2,\n",
    "        'left2': ir_pair2_right1,\n",
    "        'right2': ir_pair2_right1 + r2,\n",
    "        'color': ir_pair2_color,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    pairs_df = pd.DataFrame([ir_pair1, ir_pair2])\n",
    "\n",
    "    region_bottom = region_top - cds_height\n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax,\n",
    "        region=region,\n",
    "        cds_df=cds_df,\n",
    "        ir_pairs_df=pairs_df,\n",
    "        cds_arrowhead_width=cds_arrowhead_width,\n",
    "        region_bottom_and_top=(region_bottom, region_top),\n",
    "    )\n",
    "    \n",
    "    opposite_orientation_cds_df = cds_df.copy()\n",
    "    opposite_orientation_cds_df['strand'] = -opposite_orientation_cds_df['strand']\n",
    "    opposite_orientation_region_top = region_bottom - dist_between_region_bottom_and_opposite_orientation_region_top\n",
    "    opposite_orientation_region_bottom = opposite_orientation_region_top - cds_height\n",
    "    opposite_orientation_pairs_df = pairs_df.copy()\n",
    "    opposite_orientation_pairs_df['left1'] = cds1_start + cds1_end - pairs_df['right1']\n",
    "    opposite_orientation_pairs_df['right1'] = cds1_start + cds1_end - pairs_df['left1']\n",
    "    opposite_orientation_pairs_df['left2'] = cds2_start + cds2_end - pairs_df['right2']\n",
    "    opposite_orientation_pairs_df['right2'] = cds2_start + cds2_end - pairs_df['left2']\n",
    "        \n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax,\n",
    "        region=region,\n",
    "        cds_df=opposite_orientation_cds_df,\n",
    "        ir_pairs_df=opposite_orientation_pairs_df,\n",
    "        cds_arrowhead_width=cds_arrowhead_width,\n",
    "        region_bottom_and_top=(opposite_orientation_region_top, opposite_orientation_region_bottom),\n",
    "        ir_pairs_pointing_outward=True,\n",
    "    )\n",
    "    \n",
    "    ax.add_patch(matplotlib.patches.Polygon([\n",
    "        (cds1_start, region_bottom), \n",
    "        (cds1_end, region_bottom), \n",
    "        (cds1_start, opposite_orientation_region_top), \n",
    "        (cds1_end, opposite_orientation_region_top), \n",
    "    ], closed=True, fill=True, facecolor=ALIGNMENT_PROJECTION_COLOR, alpha=ALIGNMENT_PROJECTION_ALPHA, edgecolor=ALIGNMENT_PROJECTION_EDGE_COLOR,\n",
    "        linewidth=ALIGNMENT_PROJECTION_LINE_WIDTH))\n",
    "    ax.add_patch(matplotlib.patches.Polygon([\n",
    "        (cds2_start, region_bottom), \n",
    "        (cds2_end, region_bottom), \n",
    "        (cds2_start, opposite_orientation_region_top), \n",
    "        (cds2_end, opposite_orientation_region_top), \n",
    "    ], closed=True, fill=True, facecolor=ALIGNMENT_PROJECTION_COLOR, alpha=ALIGNMENT_PROJECTION_ALPHA, edgecolor=ALIGNMENT_PROJECTION_EDGE_COLOR,\n",
    "        linewidth=ALIGNMENT_PROJECTION_LINE_WIDTH))\n",
    "    \n",
    "    upper_attr_line_y = region_top + dist_between_attr_line_and_cds\n",
    "    lower_attr_line_y = region_bottom - dist_between_attr_line_and_cds\n",
    "    upper_attr_text_y = upper_attr_line_y + dist_between_attr_line_and_text\n",
    "    lower_attr_text_y = lower_attr_line_y - dist_between_attr_line_and_text\n",
    "    \n",
    "    \n",
    "    draw_attr_arrow = get_draw_attr_arrow(ax, upper_attr_line_y, upper_attr_text_y, lower_attr_line_y, lower_attr_text_y, ATTR_ARROW_PROPS)\n",
    "        \n",
    "    line_xs = [cds1_end, cds2_start]\n",
    "    draw_attr_arrow(line_xs, '$d$', is_upper=True)\n",
    "    \n",
    "    line_xs = [cds1_start, ir_pair1['left1']]\n",
    "    draw_attr_arrow(line_xs, r'$o_L$', is_upper=True)\n",
    "    \n",
    "    line_xs = [ir_pair1['right2'], cds2_end]\n",
    "    draw_attr_arrow(line_xs, r'$o_R$', is_upper=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    line_xs = [ir_pair2['right1'], cds1_end]\n",
    "    draw_attr_arrow(line_xs, r'$i_L$', is_upper=True)\n",
    "    \n",
    "    line_xs = [cds2_start, ir_pair2['left2']]\n",
    "    draw_attr_arrow(line_xs, r'$i_R$', is_upper=True)\n",
    "\n",
    "    formulas_y = np.mean([region_top, region_bottom])\n",
    "    \n",
    "    region_len = region[1] - region[0]\n",
    "    left_x = region[0] - region_left_margin_len_as_fraction_of_region_len * region_len\n",
    "    right_x = region[1] + region_right_margin_len_as_fraction_of_region_len * region_len\n",
    "    formulas_left_x = region[1] + dist_between_region_and_formulas_as_fraction_of_region_len * region_len\n",
    "    \n",
    "    formula_vertical_dist_from_region_middle = 0.55\n",
    "    \n",
    "    orientation_matching = (o1+d+o2) / (o1+d+o2+i1+d+i2)\n",
    "    ax.text(\n",
    "        formulas_left_x,\n",
    "        formulas_y, \n",
    "        r'$\\dfrac{\\mathrm{orientation}}{\\mathrm{matching}}=' + f'{orientation_matching:.1f}$',\n",
    "        va='center', ha='left',\n",
    "    )\n",
    "    # an ugly hack to get rid of the fraction line - we just overwrite it with a white line. ugh.\n",
    "    ax.plot([formulas_left_x, formulas_left_x + region_len * 0.26], \n",
    "            [formulas_y] * 2 , \n",
    "            color=COLOR_OF_HACKY_LINE_TO_OVERWRITE_FRACTION_LINE_OF_ORIENTATION_MATCHING, \n",
    "            linewidth=2.4, # 2.4 is fine with 300 dpi. (lower was ok here but bad in another viewer. ugh)\n",
    "            zorder=100)\n",
    "    \n",
    "    ax.text(\n",
    "        region[0] - region_len * 0.02,\n",
    "        formulas_y, \n",
    "        'observed\\nlocus',\n",
    "        va='center', ha='right',\n",
    "    )\n",
    "    ax.text(\n",
    "        region[0] - region_len * 0.02,\n",
    "        np.mean([opposite_orientation_region_top, opposite_orientation_region_bottom]), \n",
    "        'hypothetical\\nopposite\\norientation locus',\n",
    "        va='center', ha='right',\n",
    "    )\n",
    "    # set_x_lim_according_to_region(ax, region, margin_relative_size=0.03)\n",
    "    ax.set_xlim(left_x, right_x)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "def draw_schematic_asymmetry_examples_on_fig_and_ax(fig, ax, cds_height, ir_pair1_color, ir_pair2_color, cds_and_repeat_info):\n",
    "    region_top = 0.8\n",
    "    # dist_between_attr_line_and_cds = 0.1\n",
    "    # dist_between_attr_line_and_text = 0.05\n",
    "    dist_between_attr_line_and_cds = cds_height / 0.22 * 0.1\n",
    "    dist_between_attr_line_and_text = cds_height / 0.22 * 0.05\n",
    "    dist_between_lower_attr_text_and_formulas = 0.43\n",
    "    region_left_margin_len_as_fraction_of_region_len = 0.02\n",
    "    region_right_margin_len_as_fraction_of_region_len = 0.35\n",
    "    dist_between_region_and_formulas_as_fraction_of_region_len = 0.05\n",
    "    \n",
    "    dist_between_region_bottom_and_opposite_orientation_region_top = 0.5\n",
    "    \n",
    "    \n",
    "    cds_arrowhead_width = cds_and_repeat_info['cds_arrowhead_width']\n",
    "    r1 = cds_and_repeat_info['r1']\n",
    "    r2 = cds_and_repeat_info['r2']\n",
    "    cds1_strand = cds_and_repeat_info['cds1_strand']\n",
    "    cds1_len = cds_and_repeat_info['cds1_len']\n",
    "    cds2_len = cds_and_repeat_info['cds2_len']\n",
    "    cds_distance = cds_and_repeat_info['cds_distance']\n",
    "    cds1_start = cds_and_repeat_info['cds1_start']\n",
    "    o1 = cds_and_repeat_info['o1']\n",
    "    o2 = cds_and_repeat_info['o2']\n",
    "    i1 = cds_and_repeat_info['i1']\n",
    "    i2 = cds_and_repeat_info['i2']\n",
    "    \n",
    "    if cds1_strand == 1:\n",
    "        u1, u2 = o1, o2\n",
    "    else:\n",
    "        u1, u2 = i1, i2\n",
    "    \n",
    "    cds1_end = cds1_start + cds1_len\n",
    "    \n",
    "    cds2_start = cds1_end + cds_distance\n",
    "    cds2_end = cds2_start + cds2_len\n",
    "    \n",
    "    region = (cds1_start - 250, cds2_end + 250)\n",
    "    \n",
    "    cds1 = {\n",
    "        'start_pos': cds1_start,\n",
    "        'end_pos': cds1_end,\n",
    "        'strand': cds1_strand,\n",
    "        'product': '',\n",
    "    }\n",
    "    cds2 = {\n",
    "        'start_pos': cds2_start,\n",
    "        'end_pos': cds2_end,\n",
    "        'strand': -cds1_strand,\n",
    "        'product': '',\n",
    "    }\n",
    "    \n",
    "    cds_df = pd.DataFrame([cds1, cds2])\n",
    "\n",
    "    ir_pair1_left1 = cds1['start_pos'] + o1\n",
    "    ir_pair1_right2 = cds2['end_pos'] - o2\n",
    "    ir_pair1 = {\n",
    "        'left1': ir_pair1_left1,\n",
    "        'right1': ir_pair1_left1 + r1,\n",
    "        'left2': ir_pair1_right2 - r1,\n",
    "        'right2': ir_pair1_right2,\n",
    "        'color': ir_pair1_color,\n",
    "    }\n",
    "    ir_pair2_left2 = cds1['end_pos'] - i1\n",
    "    ir_pair2_right1 = cds2['start_pos'] + i2\n",
    "    ir_pair2 = {\n",
    "        'left1': ir_pair2_left2 - r2,\n",
    "        'right1': ir_pair2_left2,\n",
    "        'left2': ir_pair2_right1,\n",
    "        'right2': ir_pair2_right1 + r2,\n",
    "        'color': ir_pair2_color,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    pairs_df = pd.DataFrame([ir_pair1, ir_pair2])\n",
    "\n",
    "    region_bottom = region_top - cds_height\n",
    "    draw_cds_invertion_on_ax(\n",
    "        ax=ax,\n",
    "        region=region,\n",
    "        cds_df=cds_df,\n",
    "        ir_pairs_df=pairs_df,\n",
    "        cds_arrowhead_width=cds_arrowhead_width,\n",
    "        region_bottom_and_top=(region_bottom, region_top),\n",
    "    )\n",
    "    \n",
    "    upper_attr_line_y = region_top + dist_between_attr_line_and_cds\n",
    "    lower_attr_line_y = region_bottom - dist_between_attr_line_and_cds\n",
    "    upper_attr_text_y = upper_attr_line_y + dist_between_attr_line_and_text\n",
    "    lower_attr_text_y = lower_attr_line_y - dist_between_attr_line_and_text\n",
    "    \n",
    "    \n",
    "    draw_attr_arrow = get_draw_attr_arrow(ax, upper_attr_line_y, upper_attr_text_y, lower_attr_line_y, lower_attr_text_y, ATTR_ARROW_PROPS)\n",
    "        \n",
    "    if cds1_strand == 1:\n",
    "        line_xs = [cds1_start, ir_pair1['left1']]\n",
    "    else:\n",
    "        line_xs = [ir_pair2['right1'], cds1_end]\n",
    "    draw_attr_arrow(line_xs, r'$u_L$', is_upper=False)\n",
    "    \n",
    "    if cds1_strand == 1:\n",
    "        line_xs = [ir_pair1['right2'], cds2_end]\n",
    "    else:\n",
    "        line_xs = [cds2_start, ir_pair2['left2']]\n",
    "    draw_attr_arrow(line_xs, r'$u_R$', is_upper=False)\n",
    "    \n",
    "    \n",
    "    formulas_y = np.mean([region_top, region_bottom])\n",
    "    \n",
    "    region_len = region[1] - region[0]\n",
    "    left_x = region[0] - region_left_margin_len_as_fraction_of_region_len * region_len\n",
    "    right_x = region[1] + region_right_margin_len_as_fraction_of_region_len * region_len\n",
    "    formulas_left_x = region[1] + dist_between_region_and_formulas_as_fraction_of_region_len * region_len\n",
    "    \n",
    "    formula_vertical_dist_from_region_middle = 0.55\n",
    "    \n",
    "    asymmetry = 1 - min(u1, u2) / max(u1, u2)\n",
    "    ax.text(\n",
    "        formulas_left_x,\n",
    "        formulas_y, \n",
    "        r'$\\mathrm{asymmetry}=' + f'{asymmetry:.2f}$',\n",
    "        va='center', ha='left',\n",
    "    )\n",
    "    \n",
    "    # set_x_lim_according_to_region(ax, region, margin_relative_size=0.03)\n",
    "    ax.set_xlim(left_x, right_x)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "def plot_cum_hist_on_ax(ax, curr_df, column_name, bool_column_name, expected_critical_x, \n",
    "                        outermost_bin_edges=None, pvalue_on_top_left=False):\n",
    "    assert not curr_df[column_name].isna().any()\n",
    "    \n",
    "    column_min = curr_df[column_name].min()\n",
    "    column_max = curr_df[column_name].max()\n",
    "\n",
    "    \n",
    "    bins = list(np.sort(curr_df[column_name].unique()))\n",
    "    \n",
    "    if outermost_bin_edges:\n",
    "        # this allows hiding the annoying vertical lines ax.hist draws, and also showing the ticks on the edges (due to ax.set_xlim later)\n",
    "        assert outermost_bin_edges[0] <= column_min\n",
    "        assert outermost_bin_edges[1] >= column_max\n",
    "        if outermost_bin_edges[0] < bins[0]:\n",
    "            bins = [outermost_bin_edges[0]] + bins\n",
    "        if outermost_bin_edges[1] > bins[-1]:\n",
    "            bins.append(outermost_bin_edges[1])\n",
    "    \n",
    "    hist_ys = []\n",
    "    print(curr_df[bool_column_name].value_counts())\n",
    "    curr_df_grouped = curr_df.groupby(bool_column_name)\n",
    "    for high_confidence, group_df in curr_df_grouped:\n",
    "        hist_res = ax.hist(\n",
    "            group_df[column_name],\n",
    "            bins=bins,\n",
    "            density=True,\n",
    "            cumulative=True,\n",
    "            histtype='step', \n",
    "            # linewidth=8,\n",
    "            # alpha=0.9,\n",
    "            label=ATTR_NAME_TO_VAL_TO_LABEL[bool_column_name][high_confidence],\n",
    "            color=ATTR_NAME_TO_VAL_TO_COLOR[bool_column_name][high_confidence],\n",
    "        )\n",
    "        hist_ys.append(hist_res[0])\n",
    "        \n",
    "    hist1_ys, hist2_ys = hist_ys\n",
    "    x_with_highest_hist_dif = bins[np.argmax(np.abs(hist1_ys - hist2_ys))]\n",
    "    # assert np.isclose(x_with_highest_hist_dif, expected_critical_x)\n",
    "    assert x_with_highest_hist_dif == expected_critical_x\n",
    "    \n",
    "    _, pvalue = scipy.stats.ks_2samp(\n",
    "        *[x[column_name] for _, x in curr_df_grouped],\n",
    "        alternative='two-sided',\n",
    "        \n",
    "        mode='asymp', \n",
    "        # mode='exact', \n",
    "    )\n",
    "    print(f'KS pvalue: {pvalue}')\n",
    "    \n",
    "    u_statistic, pvalue = scipy.stats.mannwhitneyu(\n",
    "        *[x[column_name] for _, x in curr_df_grouped],\n",
    "        alternative='two-sided',\n",
    "    )\n",
    "    assert (~curr_df[bool_column_name]).sum() == len(curr_df) - curr_df[bool_column_name].sum()\n",
    "    u_statistic_for_identical_distributions = curr_df[bool_column_name].sum() * (~curr_df[bool_column_name]).sum() / 2\n",
    "    print(f'MW U statistic: {u_statistic} (u_statistic_for_identical_distributions: {u_statistic_for_identical_distributions})')\n",
    "    \n",
    "    \n",
    "    if pvalue == 0:\n",
    "        pvalue_repr = '0'\n",
    "    else:\n",
    "        log10_pval = np.log10(pvalue)\n",
    "        if log10_pval < -10:\n",
    "            assert int(log10_pval) != log10_pval\n",
    "            log10_pval_ceil = np.ceil(log10_pval)\n",
    "            pvalue_repr = get_pvalue_repr(10 ** log10_pval_ceil, verbose=False)\n",
    "            pvalue_repr = f'P<{pvalue_repr}'\n",
    "        else:\n",
    "            pvalue_repr = get_pvalue_repr(pvalue, verbose=False)\n",
    "            pvalue_repr = f'P={pvalue_repr}'\n",
    "    if pvalue_on_top_left:\n",
    "        ax.text(0.025, 0.97, f'{pvalue_repr}', va='top', ha='left', transform=ax.transAxes)\n",
    "    else:\n",
    "        ax.text(0.972, 0.01, f'{pvalue_repr}', va='bottom', ha='right', transform=ax.transAxes)\n",
    "    \n",
    "    ax.set_ylabel(\n",
    "        # 'CDS pair\\ncumulative fraction', \n",
    "        'PIC cumulative fraction', \n",
    "        # labelpad=5,\n",
    "    )\n",
    "\n",
    "    ylim = (0, 1.011)\n",
    "    ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "    ax.plot([x_with_highest_hist_dif] * 2, ylim,\n",
    "            linestyle='--',color='red',\n",
    "            alpha=THRESHOLD_DASHED_LINE_ALPHA)\n",
    "    # ax.text(x_with_highest_hist_dif, 1.01, critical_x_name_repr, va='bottom', ha='center', clip_on=False)\n",
    "    ax.set_ylim(*ylim)\n",
    "    ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%g'))\n",
    "    ax.yaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%g'))\n",
    "    \n",
    "def plot_cds_spacer_len_cum_histograms_on_ax(fig, ax, df, bool_column_name):\n",
    "    # curr_df = cds_pairs_df.copy()\n",
    "    curr_df = df.copy()\n",
    "\n",
    "    column_name = 'operon_spacer_len'\n",
    "    curr_df[column_name] = curr_df[column_name] / 1000.0\n",
    "    \n",
    "    plot_cum_hist_on_ax(ax, curr_df, column_name, bool_column_name, \n",
    "                        expected_critical_x=column_name_to_critical_val_and_passed_threshold_column_name[column_name][0] / 1000,\n",
    "                        outermost_bin_edges=(curr_df[column_name].min(),15.02))\n",
    "    \n",
    "    ax.set_xlabel(\n",
    "        'CDS distance (kbp)', \n",
    "        # labelpad=7, # ugh. seems like this makes things better only for 100 dpi???\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "def plot_max_repeat_len_cum_histograms_on_ax(fig, ax, df, bool_column_name):\n",
    "    curr_df = df.copy()\n",
    "    \n",
    "    column_name = 'max_repeat_len'\n",
    "    \n",
    "    \n",
    "    \n",
    "    plot_cum_hist_on_ax(ax, curr_df, column_name, bool_column_name,\n",
    "                        expected_critical_x=column_name_to_critical_val_and_passed_threshold_column_name[column_name][0])\n",
    "    \n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel(\n",
    "        'repeat length (bp)', \n",
    "        # labelpad=7,\n",
    "    )\n",
    "    \n",
    "    orig_xlim = ax.get_xlim()\n",
    "    ax.set_xlim(orig_xlim[0] - 1, orig_xlim[-1])\n",
    "    \n",
    "\n",
    "def plot_asymmetry_cum_histograms_on_ax(fig, ax, df, bool_column_name):\n",
    "    curr_df = df.copy()\n",
    "\n",
    "    column_name = 'operon_asymmetry'\n",
    "\n",
    "\n",
    "    plot_cum_hist_on_ax(ax, curr_df, column_name, bool_column_name, \n",
    "                        expected_critical_x=column_name_to_critical_val_and_passed_threshold_column_name[column_name][0],\n",
    "                       outermost_bin_edges=(0,1.009), pvalue_on_top_left=True)\n",
    "    \n",
    "    ax.set_xlabel(\n",
    "        'asymmetry', \n",
    "        # labelpad=7, # ugh. seems like this makes things better only for 100 dpi???\n",
    "    )\n",
    "\n",
    "    \n",
    "def plot_repeat_orientation_matching_cum_histograms_on_ax(fig, ax, df, bool_column_name):\n",
    "    curr_df = df.copy()\n",
    "\n",
    "    column_name = 'operon_closest_repeat_position_orientation_matching'\n",
    "\n",
    "\n",
    "    plot_cum_hist_on_ax(ax, curr_df, column_name, bool_column_name, \n",
    "                        expected_critical_x=column_name_to_critical_val_and_passed_threshold_column_name[column_name][0],\n",
    "                       outermost_bin_edges=(0,1.009), pvalue_on_top_left=True)\n",
    "    \n",
    "    ax.set_xlabel(\n",
    "        'orientation matching', \n",
    "        # labelpad=7, # ugh. seems like this makes things better only for 100 dpi???\n",
    "    )\n",
    "\n",
    "\n",
    "def draw_logistic_regression_coefficients(fig, ax, logistic_regression_fit_result_df):\n",
    "    curr_df = logistic_regression_fit_result_df.copy()\n",
    "    curr_df['adjusted_coef_minus_its_std_err'] = curr_df['adjusted_coef'] - curr_df['adjusted_std err']\n",
    "    \n",
    "    bars_alpha = 0.85\n",
    "    error_bar_alpha = 0.85\n",
    "    error_bar_capsize = 2.5\n",
    "    \n",
    "    curr_df.sort_values(['adjusted_coef_minus_its_std_err'], inplace=True, ascending=False)\n",
    "    \n",
    "    num_of_predictors = len(curr_df)\n",
    "\n",
    "    xs = np.arange(1, num_of_predictors + 1)\n",
    "    width = 0.3  # the width of the bars\n",
    "    adjusted_bars = ax.bar(xs - width/2, curr_df['adjusted_coef'], width, yerr=curr_df['adjusted_std err'], \n",
    "                           label='adjusted', \n",
    "                           color='grey',\n",
    "                           alpha=bars_alpha,\n",
    "                           capsize=error_bar_capsize, error_kw={'alpha': error_bar_alpha})\n",
    "    unadjusted_bars = ax.bar(xs + width/2, curr_df['coef'], width, yerr=curr_df['std err'], \n",
    "                             label='unadjusted', \n",
    "                             color='grey', \n",
    "                             # edgecolor='black', \n",
    "                             # hatch='..', \n",
    "                             alpha=(bars_alpha - 0.4),\n",
    "                             capsize=error_bar_capsize, error_kw={'alpha': error_bar_alpha})\n",
    "    \n",
    "    \n",
    "    ax.set_ylabel(\n",
    "        'logistic regression\\ncoefficient',\n",
    "        # fontsize='small',\n",
    "    )\n",
    "    ax.set_xticks(xs)\n",
    "    ax.set_xticklabels(\n",
    "        [LOGISTIC_REGRESSION_COEFFICIENT_BOOL_COLUMN_NAME_TO_LABEL[x] if x in LOGISTIC_REGRESSION_COEFFICIENT_BOOL_COLUMN_NAME_TO_LABEL else x for x in curr_df['bool_column_name']],\n",
    "        # rotation=45, ha='right', rotation_mode='anchor', \n",
    "        # fontsize='small',\n",
    "    )\n",
    "\n",
    "    legend = ax.legend(fontsize='small')\n",
    "    \n",
    "if logistic_regression_fit_result_df is not None:\n",
    "    draw_logistic_regression_coefficients(\n",
    "        fig2, fig2f_ax, \n",
    "        logistic_regression_fit_result_df, \n",
    "    )\n",
    "    \n",
    "ir_pair1_color = COLORBREWER_SET1_WITHOUT_GREY_AND_YELLOW[1]\n",
    "ir_pair2_color = COLORBREWER_SET1_WITHOUT_GREY_AND_YELLOW[2]\n",
    "bool_column_name = 'any_high_confidence_ir_pair_linked_to_cds_pair'\n",
    "df_for_cum_hists = extended1_cds_pairs_for_enrichment_analysis_and_logistic_regression_df\n",
    "draw_schematic_invertible_cds_attributes_on_fig_and_ax(fig2, fig2a_ax, cds_height=0.22, ir_pair1_color=ir_pair1_color, \n",
    "                                                       ir_pair2_color=ir_pair2_color)\n",
    "plot_max_repeat_len_cum_histograms_on_ax(fig2, fig2b_ax, df_for_cum_hists, bool_column_name)\n",
    "plot_cds_spacer_len_cum_histograms_on_ax(fig2, fig2c_ax, df_for_cum_hists, bool_column_name)\n",
    "plot_repeat_orientation_matching_cum_histograms_on_ax(fig2, fig2d_ax, df_for_cum_hists, bool_column_name)\n",
    "plot_asymmetry_cum_histograms_on_ax(fig2, fig2e_ax, df_for_cum_hists, bool_column_name)\n",
    "\n",
    "legend_elements = [\n",
    "    matplotlib.lines.Line2D([0], [0], color=ATTR_NAME_TO_VAL_TO_COLOR['any_high_confidence_ir_pair_linked_to_cds_pair'][high_confidence], \n",
    "           label=ATTR_NAME_TO_VAL_TO_LABEL['any_high_confidence_ir_pair_linked_to_cds_pair'][high_confidence]) \n",
    "    for high_confidence in (True, False)\n",
    "]\n",
    "fig2b_ax.legend(\n",
    "    handles=legend_elements,\n",
    "    fontsize='small', \n",
    "    bbox_transform=fig2b_ax.transAxes,\n",
    "    # bbox_transform=fig2e_ax.transAxes,\n",
    "    \n",
    "    # bbox_to_anchor=(0, 1.2),\n",
    "    # loc='lower left',\n",
    "    \n",
    "    bbox_to_anchor=(\n",
    "        1.04, \n",
    "        # 1.04,\n",
    "        1,\n",
    "    ),\n",
    "    loc='lower right',\n",
    ")\n",
    "\n",
    "for ax in (fig2c_ax, fig2e_ax):\n",
    "    ax.axes.get_yaxis().get_label().set_visible(False)\n",
    "    # ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    \n",
    "if 1:\n",
    "    fig2a_ax.text(0.03, 0.975, 'A', va='center', ha='center', transform=fig2.transFigure, fontweight='bold')\n",
    "    fig2b_ax.text(0.03, 1.15, 'B', va='center', ha='center', fontweight='bold',\n",
    "                      transform=matplotlib.transforms.blended_transform_factory(fig2.transFigure, fig2b_ax.transAxes))\n",
    "    fig2c_ax.text(-0.07, 1.15, 'C', va='center', ha='center', fontweight='bold', transform=fig2c_ax.transAxes)\n",
    "                      # transform=matplotlib.transforms.blended_transform_factory(fig2c_ax.transAxes, fig2.transFigure))\n",
    "    fig2d_ax.text(0.03, 1.15, 'D', va='center', ha='center', fontweight='bold',\n",
    "                      transform=matplotlib.transforms.blended_transform_factory(fig2.transFigure, fig2d_ax.transAxes))\n",
    "    fig2e_ax.text(-0.07, 1.15, 'E', va='center', ha='center', fontweight='bold', transform=fig2e_ax.transAxes)\n",
    "    fig2f_ax.text(0.03, 1.15, 'F', va='center', ha='center', fontweight='bold',\n",
    "                      transform=matplotlib.transforms.blended_transform_factory(fig2.transFigure, fig2f_ax.transAxes))\n",
    "\n",
    "        \n",
    "if 1:\n",
    "    if 'fig_s6' in locals():\n",
    "        plt.close(fig_s6)\n",
    "    fig_s6 = plt.figure(\n",
    "        figsize=(6,4.5), \n",
    "    )\n",
    "    fig_s6_gridspec = fig_s6.add_gridspec(\n",
    "        3, 1, \n",
    "        height_ratios=[1,0.4,1], \n",
    "        wspace=0, hspace=0,\n",
    "    )\n",
    "\n",
    "    fig_s6.subplots_adjust(\n",
    "        top=0.94,\n",
    "        bottom=0.03,\n",
    "        left=0.23,\n",
    "        right=0.91,\n",
    "        # hspace=0.2,\n",
    "        # wspace=0.2\n",
    "    )\n",
    "\n",
    "    fig_s6a_ax = fig_s6.add_subplot(fig_s6_gridspec[0])\n",
    "    fig_s6b_ax = fig_s6.add_subplot(fig_s6_gridspec[2])\n",
    "\n",
    "    cds_and_repeat_info = {\n",
    "        'cds_arrowhead_width': 100,\n",
    "        'r1': 200,\n",
    "        'r2': 200,\n",
    "        'cds1_len': 1700,\n",
    "        'cds2_len': 1500,\n",
    "        'cds1_strand': -1,\n",
    "        'cds_distance': 250,\n",
    "        'cds1_start': 0,\n",
    "        'o1': 900,\n",
    "        'o2': 750,\n",
    "        'i1': 285,\n",
    "        'i2': 270,\n",
    "    }\n",
    "    cds_height = 0.14\n",
    "    \n",
    "    draw_schematic_orientation_matching_examples_on_fig_and_ax(\n",
    "        fig_s6, fig_s6a_ax, \n",
    "        cds_height=cds_height, \n",
    "        ir_pair1_color=ir_pair1_color, ir_pair2_color=ir_pair2_color,\n",
    "        cds_and_repeat_info=cds_and_repeat_info,\n",
    "    )\n",
    "    \n",
    "    cds_and_repeat_info['cds1_strand'] = -cds_and_repeat_info['cds1_strand']\n",
    "    draw_schematic_orientation_matching_examples_on_fig_and_ax(\n",
    "        fig_s6, fig_s6b_ax, \n",
    "        cds_height=cds_height, \n",
    "        ir_pair1_color=ir_pair1_color, ir_pair2_color=ir_pair2_color,\n",
    "        cds_and_repeat_info=cds_and_repeat_info,\n",
    "    )\n",
    "    \n",
    "    fig_s6a_ax.text(-0.28, 1.07, 'A', va='center', ha='center', transform=fig_s6a_ax.transAxes, fontweight='bold')\n",
    "    fig_s6b_ax.text(-0.28, 1.07, 'B', va='center', ha='center', transform=fig_s6b_ax.transAxes, fontweight='bold')\n",
    "\n",
    "if 1:\n",
    "    if 'fig_s7' in locals():\n",
    "        plt.close(fig_s7)\n",
    "    fig_s7 = plt.figure(\n",
    "        figsize=(6,2.5), \n",
    "    )\n",
    "    fig_s7_gridspec = fig_s7.add_gridspec(\n",
    "        3, 1, \n",
    "        height_ratios=[1,0.25,1], \n",
    "        wspace=0, hspace=0,\n",
    "    )\n",
    "\n",
    "    fig_s7.subplots_adjust(\n",
    "        top=0.99,\n",
    "        bottom=0,\n",
    "        left=0.02,\n",
    "        right=0.97,\n",
    "        # hspace=0.2,\n",
    "        # wspace=0.2\n",
    "    )\n",
    "\n",
    "    fig_s7a_ax = fig_s7.add_subplot(fig_s7_gridspec[0])\n",
    "    fig_s7b_ax = fig_s7.add_subplot(fig_s7_gridspec[2])\n",
    "\n",
    "    cds_and_repeat_info = {\n",
    "        'cds_arrowhead_width': 100,\n",
    "        'r1': 200,\n",
    "        'r2': 200,\n",
    "        'cds1_len': 1700,\n",
    "        'cds2_len': 1050,\n",
    "        'cds1_strand': -1,\n",
    "        'cds_distance': 350,\n",
    "        'cds1_start': 0,\n",
    "        'o1': 290,\n",
    "        'o2': 330,\n",
    "        'i1': 900,\n",
    "        'i2': 200,\n",
    "    }\n",
    "    cds_height = 0.225\n",
    "    \n",
    "    draw_schematic_asymmetry_examples_on_fig_and_ax(\n",
    "        fig_s7, fig_s7a_ax, \n",
    "        cds_height=cds_height, \n",
    "        ir_pair1_color=ir_pair1_color, ir_pair2_color=ir_pair2_color,\n",
    "        cds_and_repeat_info=cds_and_repeat_info,\n",
    "    )\n",
    "    \n",
    "    cds_and_repeat_info['cds1_strand'] = -cds_and_repeat_info['cds1_strand']\n",
    "    draw_schematic_asymmetry_examples_on_fig_and_ax(\n",
    "        fig_s7, fig_s7b_ax, \n",
    "        cds_height=cds_height, \n",
    "        ir_pair1_color=ir_pair1_color, ir_pair2_color=ir_pair2_color,\n",
    "        cds_and_repeat_info=cds_and_repeat_info,\n",
    "    )\n",
    "    \n",
    "    fig_s7a_ax.text(-0.03, 1.03, 'A', va='center', ha='center', transform=fig_s7a_ax.transAxes, fontweight='bold')\n",
    "    fig_s7b_ax.text(-0.03, 1.03, 'B', va='center', ha='center', transform=fig_s7b_ax.transAxes, fontweight='bold')\n",
    "    \n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html says: \n",
    "# The two-sided exact computation computes the complementary probability and then subtracts from 1. \n",
    "# As such, the minimum probability it can return is about 1e-16. (in the end i only used asymp, to avoid pvalue=0)\n",
    "\n",
    "# according to https://stackoverflow.com/questions/37647396/statsmodels-logistic-regression-odds-ratio/47740828#47740828 (and also \n",
    "# according to https://en.wikipedia.org/wiki/Logistic_regression#Logistic_function,_odds,_odds_ratio,_and_logit, IIUC, it seems that \n",
    "# the odds ratio == exp(coefficient)\n",
    "\n",
    "if 1:\n",
    "    fig2.savefig(os.path.join(paper_figs_and_tables_dir_path, 'fig2.png'))\n",
    "    fig2.savefig(os.path.join(paper_figs_and_tables_dir_path, 'fig2.pdf'))\n",
    "    fig_s6.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s6.png'))\n",
    "    # fig_s6.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s6.pdf'))\n",
    "    fig_s7.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s7.png'))\n",
    "    # fig_s7.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s7.pdf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b417287-d323-45fb-802b-81a078dd3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'fig3' in locals():\n",
    "    plt.close(fig3)\n",
    "# raise    \n",
    "fig3 = plt.figure(\n",
    "    figsize=(6.5,9), \n",
    "    # constrained_layout=True,\n",
    ")\n",
    "fig3_gridspec = fig3.add_gridspec(\n",
    "    3, 1, \n",
    "    height_ratios=[1,0.3,1], \n",
    "    wspace=0, hspace=0,\n",
    ")\n",
    "\n",
    "fig3.subplots_adjust(\n",
    "    top=0.97,\n",
    "    bottom=0.08,\n",
    "    left=0.125,\n",
    "    # right=0.99,\n",
    "    right=0.805,\n",
    "    # hspace=0.2,\n",
    "    # wspace=0.2\n",
    ")\n",
    "\n",
    "fig3_uppper_sub_gridspec = fig3_gridspec[0].subgridspec(1, 3, width_ratios=[0.0002, 1,0.05], wspace=0, hspace=0)\n",
    "fig3_lower_sub_gridspec = fig3_gridspec[2].subgridspec(1, 3, width_ratios=[0.0002, 1,0.05], wspace=0, hspace=0)\n",
    "\n",
    "fig3a_ax = fig3.add_subplot(fig3_uppper_sub_gridspec[1])\n",
    "fig3b_ax = fig3.add_subplot(fig3_lower_sub_gridspec[1])\n",
    "\n",
    "def draw_volcano_plot_on_ax(fig, ax, test_results_df, enrichment_desc_for_xlabel, \n",
    "                            jitter_sd_as_fraction_of_ax_limit_range=0, corrected_pvalue_threshold=0):\n",
    "    curr_df = test_results_df.copy()\n",
    "    \n",
    "    any_corrected_pvalue_below_threshold = curr_df['corrected_pvalue'].min() <= corrected_pvalue_threshold\n",
    "    curr_df.loc[curr_df['corrected_pvalue'] <= corrected_pvalue_threshold, 'corrected_pvalue'] = corrected_pvalue_threshold\n",
    "    \n",
    "    \n",
    "    curr_df = add_product_class_column(curr_df, 'product', 'product_class')\n",
    "    \n",
    "    any_zero_pval = curr_df['corrected_pvalue'].min() == 0\n",
    "    if any_zero_pval:\n",
    "        min_non_zero_pval = generic_utils.get_min_strictly_positive(curr_df['corrected_pvalue'])\n",
    "        pval_fake_min = min_non_zero_pval / 100\n",
    "        curr_df.loc[curr_df['corrected_pvalue'] == 0, 'corrected_pvalue'] = pval_fake_min\n",
    "        threshold_for_pval_fake_min = (min_non_zero_pval + pval_fake_min) / 2\n",
    "\n",
    "    any_inf_odds_ratio = np.isposinf(curr_df['odds_ratio']).any()\n",
    "    if any_inf_odds_ratio:\n",
    "        max_finite_odds_ratio = curr_df[np.isfinite(curr_df['odds_ratio'])]['odds_ratio'].max()\n",
    "        fake_inf_odds_ratio = max_finite_odds_ratio * 1.2\n",
    "        curr_df.loc[np.isposinf(curr_df['odds_ratio']), 'odds_ratio'] = fake_inf_odds_ratio\n",
    "    curr_df = curr_df[curr_df['odds_ratio'] > 0]\n",
    "    \n",
    "    curr_df['log_odds_ratio'] = np.log2(curr_df['odds_ratio'])\n",
    "    assert not (curr_df['corrected_pvalue'] == 0).any()\n",
    "    curr_df['minus_log_corrected_pvalue'] = -np.log10(curr_df['corrected_pvalue'])\n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    x_jitter_sd = jitter_sd_as_fraction_of_ax_limit_range * (xlim[1] - xlim[0])\n",
    "    ylim = ax.get_ylim()\n",
    "    y_jitter_sd = jitter_sd_as_fraction_of_ax_limit_range * (ylim[1] - ylim[0])\n",
    "    \n",
    "    num_of_data_points = len(curr_df)\n",
    "    curr_df['log_odds_ratio'] += np.random.normal(scale=x_jitter_sd, size=num_of_data_points)\n",
    "    curr_df['minus_log_corrected_pvalue'] += np.random.normal(scale=y_jitter_sd, size=num_of_data_points)\n",
    "    \n",
    "    curr_df_grouped = curr_df.groupby('product_class')\n",
    "    product_classes = set(curr_df['product_class'])\n",
    "    product_class_to_scatter_result = {}\n",
    "    assert set(curr_df['product_class']) <= set(PRODUCT_CLASSES_ORDERED)\n",
    "    for product_class in PRODUCT_CLASSES_ORDERED:\n",
    "        if product_class in product_classes:\n",
    "            group_df = curr_df_grouped.get_group(product_class)\n",
    "            product_class_to_scatter_result[product_class] = ax.scatter(\n",
    "                group_df['log_odds_ratio'],\n",
    "                group_df['minus_log_corrected_pvalue'],\n",
    "                # alpha=HISTOGRAM_WITH_OVERLAP_ALPHA,\n",
    "                # alpha=0.5,\n",
    "                # edgecolor='black',\n",
    "                edgecolor=VOLCANO_PLOT_EDGE_COLOR,\n",
    "                linewidth=VOLCANO_PLOT_LINEWIDTH,\n",
    "                label=product_class,\n",
    "                # color=PRODUCT_CLASS_TO_COLOR[product_class],\n",
    "                color=('none' if (PRODUCT_CLASS_TO_COLOR[product_class] == 'white') else PRODUCT_CLASS_TO_COLOR[product_class]),\n",
    "            )\n",
    "    \n",
    "\n",
    "    # ax.set_xlabel('enrichment of annotation near rearranged CDSs ($\\log_2$(odds ratio))')\n",
    "    ax.set_xlabel(f'{ODDS_RATIO_DESCRIPTION_START} {enrichment_desc_for_xlabel}\\n($\\log_2$(odds ratio))')\n",
    "    ax.set_ylabel('$-\\log_{10}$(corrected p-value)')\n",
    "\n",
    "    if any_inf_odds_ratio:\n",
    "        log_fake_inf_odds_ratio = np.log2(fake_inf_odds_ratio)\n",
    "        non_inf_log_odds_ratios = curr_df[curr_df['log_odds_ratio'] < log_fake_inf_odds_ratio]['log_odds_ratio']\n",
    "        dotted_line_log_odds_ratio = log_fake_inf_odds_ratio - 0.11\n",
    "        assert (non_inf_log_odds_ratios < dotted_line_log_odds_ratio).all()\n",
    "        orig_ylim = ax.get_ylim()\n",
    "        ax.plot([dotted_line_log_odds_ratio] * 2, orig_ylim,\n",
    "                linestyle=FAKE_VAL_THRESHOLD_LINE_STYLE,\n",
    "                color=FAKE_VAL_THRESHOLD_LINE_COLOR,\n",
    "                # label=f'x=0',\n",
    "                alpha=FAKE_VAL_THRESHOLD_LINE_ALPHA)\n",
    "        ax.set_ylim(orig_ylim)\n",
    "        \n",
    "    \n",
    "    if any_corrected_pvalue_below_threshold:\n",
    "        minus_log10_corrected_pvalue_threshold = -np.log10(corrected_pvalue_threshold)\n",
    "        assert minus_log10_corrected_pvalue_threshold == int(minus_log10_corrected_pvalue_threshold)\n",
    "        minus_log10_corrected_pvalue_threshold = int(minus_log10_corrected_pvalue_threshold)\n",
    "        \n",
    "        below_thresh_minus_log10_corrected_pvalues = curr_df[curr_df['minus_log_corrected_pvalue'] < minus_log10_corrected_pvalue_threshold]['minus_log_corrected_pvalue']\n",
    "        dotted_line_minus_log10_corrected_pvalue = minus_log10_corrected_pvalue_threshold - 0.6\n",
    "        assert (below_thresh_minus_log10_corrected_pvalues < dotted_line_minus_log10_corrected_pvalue).all()\n",
    "        \n",
    "        orig_xlim = ax.get_xlim()\n",
    "        # ax.plot([0.075] * 2, orig_xlim,\n",
    "        ax.plot(orig_xlim, [dotted_line_minus_log10_corrected_pvalue] * 2,\n",
    "                linestyle=FAKE_VAL_THRESHOLD_LINE_STYLE,\n",
    "                color=FAKE_VAL_THRESHOLD_LINE_COLOR,\n",
    "                # label=f'x=0',\n",
    "                alpha=FAKE_VAL_THRESHOLD_LINE_ALPHA)\n",
    "        ax.set_xlim(orig_xlim)\n",
    "        \n",
    "        \n",
    "    if any_corrected_pvalue_below_threshold:\n",
    "        orig_y_tick_positions = ax.get_yticks()\n",
    "\n",
    "        minus_log10_corrected_pvalue_threshold_tick_label = f'$\\geqslant${minus_log10_corrected_pvalue_threshold}'\n",
    "        new_y_tick_positions = [y for y in orig_y_tick_positions if y < minus_log10_corrected_pvalue_threshold] + [minus_log10_corrected_pvalue_threshold]\n",
    "        ax.set_yticks(new_y_tick_positions)\n",
    "        \n",
    "        new_y_tick_labels = [remove_trailing_zeros(str(x)) for x in new_y_tick_positions[:-1]] + [minus_log10_corrected_pvalue_threshold_tick_label]\n",
    "        \n",
    "        ax.set_yticklabels(new_y_tick_labels)\n",
    "        \n",
    "    if any_inf_odds_ratio:\n",
    "        orig_x_tick_positions = ax.get_xticks()\n",
    "\n",
    "        inf_tick_label = r'$\\inf$' # r'$\\infty$' doesnt look good...\n",
    "        new_x_tick_positions = [x for x in orig_x_tick_positions if x < log_fake_inf_odds_ratio] + [log_fake_inf_odds_ratio]\n",
    "        ax.set_xticks(new_x_tick_positions)\n",
    "        \n",
    "        new_x_tick_labels = [remove_trailing_zeros(str(x)) for x in new_x_tick_positions[:-1]] + [inf_tick_label]\n",
    "        \n",
    "        ax.set_xticklabels(new_x_tick_labels)\n",
    "        \n",
    "    margin_size = 0.16\n",
    "    ax.set_xlim(curr_df['log_odds_ratio'].min() - margin_size, curr_df['log_odds_ratio'].max() + margin_size)\n",
    "            \n",
    "            \n",
    "    new_bottom_ylim = -5\n",
    "    assert curr_df['minus_log_corrected_pvalue'].min() >= new_bottom_ylim\n",
    "    ax.set_ylim(new_bottom_ylim, ax.get_ylim()[1])\n",
    "            \n",
    "    return product_class_to_scatter_result\n",
    "    \n",
    "\n",
    "corrected_pvalue_threshold = 1e-30\n",
    "target_product_class_to_scatter_result = draw_volcano_plot_on_ax(\n",
    "    fig3, fig3a_ax, \n",
    "    test_results_df=repeat_cds_product_fisher_or_g_result_df,\n",
    "    enrichment_desc_for_xlabel=TARGET_PRODUCT_ODDS_RATIO_DESCRIPTION_END,\n",
    "    # jitter_sd_as_fraction_of_ax_limit_range=0.08,\n",
    "    jitter_sd_as_fraction_of_ax_limit_range=0,\n",
    "    corrected_pvalue_threshold=corrected_pvalue_threshold,\n",
    ")\n",
    "\n",
    "neighbor_product_class_to_scatter_result = draw_volcano_plot_on_ax(\n",
    "    fig3, fig3b_ax, \n",
    "    test_results_df=adjacent_repeat_cds_product_fisher_or_g_result_df,\n",
    "    enrichment_desc_for_xlabel=NEIGHBOR_PRODUCT_ODDS_RATIO_DESCRIPTION_END,\n",
    "    # jitter_sd_as_fraction_of_ax_limit_range=0.08,\n",
    "    jitter_sd_as_fraction_of_ax_limit_range=0,\n",
    "    corrected_pvalue_threshold=corrected_pvalue_threshold,\n",
    ")\n",
    "\n",
    "relevant_product_classes_and_scatter_results_ordered = []\n",
    "for product_class in PRODUCT_CLASSES_ORDERED:\n",
    "    if product_class in target_product_class_to_scatter_result:\n",
    "        relevant_product_classes_and_scatter_results_ordered.append((product_class, target_product_class_to_scatter_result[product_class]))\n",
    "    elif product_class in neighbor_product_class_to_scatter_result:\n",
    "        relevant_product_classes_and_scatter_results_ordered.append((product_class, neighbor_product_class_to_scatter_result[product_class]))\n",
    "        \n",
    "fig3a_ax.legend(\n",
    "    [x[1] for x in relevant_product_classes_and_scatter_results_ordered], \n",
    "    [x[0] for x in relevant_product_classes_and_scatter_results_ordered], \n",
    "    # bbox_transform=matplotlib.transforms.blended_transform_factory(fig3.transFigure, fig3a_ax.transAxes),\n",
    "    bbox_transform=fig3a_ax.transAxes,\n",
    "    bbox_to_anchor=(0.995, 1.03),\n",
    "    # bbox_to_anchor=(1, 0.9),\n",
    "    loc='upper left',\n",
    "    # loc='upper right',\n",
    "    # fontsize='x-small', \n",
    "    fontsize='small', \n",
    ")\n",
    "\n",
    "if 1:\n",
    "    fig3a_ax.text(0.03, 1.02, 'A', va='center', ha='center', fontweight='bold',\n",
    "                  transform=matplotlib.transforms.blended_transform_factory(fig3.transFigure, fig3a_ax.transAxes))\n",
    "    fig3b_ax.text(0.03, 1.02, 'B', va='center', ha='center', fontweight='bold',\n",
    "                  transform=matplotlib.transforms.blended_transform_factory(fig3.transFigure, fig3b_ax.transAxes))\n",
    "\n",
    "if 1:\n",
    "    fig3.savefig(os.path.join(paper_figs_and_tables_dir_path, 'fig3.png'))\n",
    "    fig3.savefig(os.path.join(paper_figs_and_tables_dir_path, 'fig3.pdf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268b679-954d-4950-842f-8a68e6c1af7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'fig4' in locals():\n",
    "    plt.close(fig4)\n",
    "# raise\n",
    "\n",
    "\n",
    "\n",
    "MTASE_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED = [\n",
    "    ('MTase: two upstream helicases', 'NZ_CP068294.1', (3122130, 3125861)),\n",
    "    ('MTase: two upstream helicases', 'NZ_UGYW01000002.1', (1269931, 1273395)),\n",
    "    ('MTase: two upstream helicases', 'NZ_UFVQ01000003.1', (2778297, 2784881)),\n",
    "    ('MTase: Class 1 DISARM', 'NZ_CP010519.1', (3712964, 3717292)),\n",
    "    ('MTase: Class 1 DISARM-like', 'NZ_CP061344.1', (2385445, 2390031)),\n",
    "\n",
    "    ('MTase: upstream PLD&SNF2 helicase', 'CP083813.1', (5324991, 5328359)),\n",
    "    ('MTase: downstream PLD&SNF2 helicase', 'CP044495.1', (1257875, 1261063)),\n",
    "    ('MTase: upstream DUF1016', 'CP033760.1', (4019614, 4023351)), \n",
    "    ('MTase: solitary', 'NZ_CP082886.1', (2883305, 2887030)),\n",
    "    \n",
    "    ('MTase: BREX type 1', 'NC_013198.1', (2161973, 2165527)),\n",
    "    ('MTase: BREX type 1, downstream extra short PglX', 'NZ_CP068173.1', (2193315, 2195903)),\n",
    "]\n",
    "OTHER_PRESUMABLY_NOVEL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED = [\n",
    "    ('DUF4965', 'CP065872.1', (6177248, 6179761)),\n",
    "    ('PilV and phage tail collar', 'CP084655.1', (816997, 818571)),\n",
    "]\n",
    "\n",
    "\n",
    "NON_NOVEL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED = [\n",
    "    ('RM specificity: R-M-S', 'NZ_CP022464.2', (6453804, 6455096)),\n",
    "    ('RM specificity: R-M-DUF1016-S', 'CP046428.1', (3459162, 3460715)),\n",
    "    ('RM specificity: R-M-RhuM-S', 'CP081899.1', (2601241, 2602485)),\n",
    "    ('RM specificity: R-M-Fic/DOC-S', 'NZ_CP082886.1', (4369547, 4371175)),\n",
    "    ('RM specificity: M-invertibleSs-R', 'NZ_CP059830.1', (15345, 16556)),\n",
    "    \n",
    "    ('phage tail: upstream phage tail protein I and downstream tail fiber assembly', 'CP056267.1', (5092989, 5094518)),\n",
    "    ('phage tail: downstream transporter and endonuclease', 'CP076386.1', (93499, 94659)),\n",
    "    ('phage tail: upstream DUF2313 and downstream tail fiber assembly', 'CP066032.1', (4023024, 4024490)),\n",
    "\n",
    "    ('OM receptor: downstream SusD/RagB', 'NZ_CP012938.1', (2847275, 2850391)),\n",
    "]\n",
    "\n",
    "FIG4_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED = (\n",
    "    MTASE_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED\n",
    "    + \n",
    "    OTHER_PRESUMABLY_NOVEL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED\n",
    "    # + \n",
    "    # NON_NOVEL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED\n",
    ")\n",
    "\n",
    "ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED = (\n",
    "    FIG4_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED + \n",
    "    NON_NOVEL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED\n",
    ")\n",
    "    \n",
    "FIG4_CDS_POLYGON_FILL_ALPHA = 1\n",
    "FIG4_CDS_POLYGON_EDGE_ALPHA = 1\n",
    "\n",
    "FIG4_PRODUCT_CLASS_TO_FILL_COLOR_WITH_ALPHA = {\n",
    "    **{\n",
    "        # product_class: (*matplotlib.colors.to_rgb(color), FIG4_CDS_POLYGON_FILL_ALPHA)\n",
    "        product_class: color\n",
    "        for product_class, color in PRODUCT_CLASS_TO_COLOR.items()\n",
    "        if product_class not in ('other', 'hypothetical')\n",
    "    },\n",
    "    'other': 'white',\n",
    "}\n",
    "FIG4_CDS_POLYGON_EDGE_COLOR_WITH_ALPHA = (*matplotlib.colors.to_rgb('black'), FIG4_CDS_POLYGON_EDGE_ALPHA)\n",
    "    \n",
    "    \n",
    "num_of_cds_contexts = len(FIG4_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED)\n",
    "\n",
    "\n",
    "fig4 = plt.figure(\n",
    "    figsize=(\n",
    "        12, \n",
    "        10.5,\n",
    "        # 1.5 + 0.5 * num_of_cds_contexts,\n",
    "    ), \n",
    "    # constrained_layout=True,\n",
    ")\n",
    "num_of_rows = 2 + num_of_cds_contexts * 2 - 1\n",
    "fig4_gridspec = fig4.add_gridspec(\n",
    "    num_of_rows, 7, \n",
    "    width_ratios=[2.3,10,0.1,1.2,0.8,1.4,0.15], \n",
    "    wspace=0.0001,\n",
    "    hspace=0.0001,\n",
    "    \n",
    "    # height_ratios=[2.2, 0.5] + ([1, 0.26] * num_of_cds_contexts)[:-1], \n",
    "    height_ratios=([2.2, 0.5] + ([1, 0.26] * (num_of_cds_contexts - 1))[:-1] + [1.3, 1]), \n",
    ")\n",
    "\n",
    "fig4.subplots_adjust(\n",
    "    top=0.98,\n",
    "    bottom=0.08,\n",
    "    left=0.03,\n",
    "    right=0.99,\n",
    "    # hspace=0.2,\n",
    "    # wspace=0.2\n",
    ")\n",
    "\n",
    "long_read_alignment_sub_gridspec = fig4_gridspec[0,1].subgridspec(1, 9, width_ratios=[0,1,0.08,1,0.08,1,0.08,1,0.8], wspace=0.00001)\n",
    "\n",
    "fig4_long_read_alignment_axes = [fig4.add_subplot(long_read_alignment_sub_gridspec[i]) for i in range(1,8,2)]\n",
    "fig4_cds_context_axes = [fig4.add_subplot(fig4_gridspec[x,1]) for x in range(2, num_of_rows, 2)]\n",
    "fig4_pie_axes = [fig4.add_subplot(fig4_gridspec[x,3]) for x in range(2, num_of_rows, 2)]\n",
    "fig4_taxon_dist_axes = [fig4.add_subplot(fig4_gridspec[x,5]) for x in range(2, num_of_rows, 2)]\n",
    "sub_gridspec_for_colorbar = fig4_gridspec[0,-2].subgridspec(2, 1, height_ratios=[1, 0.3], hspace=0)\n",
    "fig4_taxon_dist_colorbar_ax = fig4.add_subplot(sub_gridspec_for_colorbar[0])\n",
    "\n",
    "remove_all_ax_spines_an_x_and_y_axes(fig4_taxon_dist_colorbar_ax)\n",
    "\n",
    "    \n",
    "    \n",
    "def draw_taxon_distribution_heatmap(fig, axes, colorbar_ax, cds_context_names_and_longer_linked_cds_loci_ordered, \n",
    "                                    taxon_column_name_for_counting, counted_obj_desc, heatmap_colormap):\n",
    "    curr_df = all_homologs_extended_df.copy()\n",
    "    \n",
    "    curr_df = curr_df.merge(pd.DataFrame([(x[:2] + x[2]) for x in cds_context_names_and_longer_linked_cds_loci_ordered], columns=[\n",
    "        'cds_context_name', 'longer_linked_repeat_cds_nuccore_accession', \n",
    "        'longer_linked_repeat_cds_start', 'longer_linked_repeat_cds_end',\n",
    "    ]))\n",
    "    curr_df = curr_df[['cds_context_name', 'taxon_phylum', taxon_column_name_for_counting, 'is_homolog_potentially_modified_by_pi']].drop_duplicates()\n",
    "    \n",
    "    count_df = curr_df.groupby(['cds_context_name', 'taxon_phylum'])[taxon_column_name_for_counting].nunique().reset_index(name='total_count').merge(\n",
    "        curr_df[curr_df['is_homolog_potentially_modified_by_pi']].groupby(\n",
    "            ['cds_context_name', 'taxon_phylum'])[taxon_column_name_for_counting].nunique().reset_index(\n",
    "            name='potentially_modified_by_pi_count'), how='left')\n",
    "    count_df['potentially_modified_by_pi_count'].fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    count_df['phylum_proportion'] = count_df['potentially_modified_by_pi_count'] / count_df['total_count']\n",
    "    assert (count_df['phylum_proportion'] >= 0).all()\n",
    "    assert (count_df['phylum_proportion'] <= 1).all()\n",
    "    assert len(count_df[['cds_context_name', 'taxon_phylum']].drop_duplicates()) == len(count_df)\n",
    "    cds_context_name_to_phylum_to_proportion_and_total_count = {\n",
    "        cds_context_name: {\n",
    "            df_row['taxon_phylum']: (df_row['phylum_proportion'], df_row['total_count'])\n",
    "            for _, df_row in context_df.iterrows()\n",
    "        }\n",
    "        for cds_context_name, context_df in count_df.groupby('cds_context_name')\n",
    "    }\n",
    "    \n",
    "    phylum_names_sorted = sorted(count_df['taxon_phylum'].unique())\n",
    "    num_of_phyla = len(phylum_names_sorted)\n",
    "\n",
    "    for i, (ax, cds_context_name_and_longer_linked_cds_locus) in enumerate(zip(axes, cds_context_names_and_longer_linked_cds_loci_ordered)):\n",
    "        cds_context_name = cds_context_name_and_longer_linked_cds_locus[0]\n",
    "        \n",
    "        cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args = search_for_pis_args['stage6'][\n",
    "            'cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args']\n",
    "        alignment_region_raw_read_alignment_args = get_alignment_region_raw_read_alignment_args_by_cds_context_name_and_locus(\n",
    "            cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args, \n",
    "            *cds_context_name_and_longer_linked_cds_locus,\n",
    "        )\n",
    "        longer_linked_cds_phylum = alignment_region_raw_read_alignment_args['phylum']\n",
    "        \n",
    "        no_homologs = cds_context_name not in cds_context_name_to_phylum_to_proportion_and_total_count\n",
    "        \n",
    "        # print(f'cds_context_name: {cds_context_name}')\n",
    "        if not no_homologs:\n",
    "            phylum_to_proportion_and_total_count = cds_context_name_to_phylum_to_proportion_and_total_count[cds_context_name]\n",
    "\n",
    "            # make sure the phylum of the locus with long read evidence does not contradict the heatmap.\n",
    "            assert phylum_to_proportion_and_total_count[longer_linked_cds_phylum][0] > 0\n",
    "        else:\n",
    "            phylum_to_proportion_and_total_count = {}\n",
    "            \n",
    "\n",
    "        ordered_proportion_and_total_count = [phylum_to_proportion_and_total_count[phylum] if phylum in phylum_to_proportion_and_total_count else (np.nan, np.nan) \n",
    "                                              for phylum in phylum_names_sorted]\n",
    "        \n",
    "        im = ax.imshow(\n",
    "            [[x[0] for x in ordered_proportion_and_total_count]],\n",
    "            cmap=heatmap_colormap,\n",
    "            vmin=0, vmax=1, \n",
    "            # norm=matplotlib.colors.LogNorm(vmin=min_non_zero_value_in_heatmap, vmax=1),\n",
    "        )\n",
    "        # ys = np.arange(len(cds_context_names_and_longer_linked_cds_loci_ordered))\n",
    "        # ax.set_yticks(ys)\n",
    "        # ax.set_yticklabels(heatmap_df.iloc[i:i+1].index)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        \n",
    "        xlim = ax.get_xlim()\n",
    "        assert (xlim[1] - xlim[0]) == num_of_phyla\n",
    "        \n",
    "        # ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=1) # doesn't work for some reason...\n",
    "        ylim = ax.get_ylim()\n",
    "        \n",
    "        ax.vlines(x=np.arange(xlim[0] + 1, xlim[1] - 0.5), ymin=ylim[0], ymax=ylim[1], color=\"black\", linestyle='-', linewidth=0.5, alpha=0.4)\n",
    "        for i, proportion_and_total_count in enumerate(ordered_proportion_and_total_count):\n",
    "            total_count = proportion_and_total_count[1]\n",
    "            if pd.isna(total_count):\n",
    "                for ys in (ylim, ylim[::-1]):\n",
    "                    ax.plot([xlim[0] + i, xlim[0] + i + 1], ys, color=\"black\", linestyle='-', linewidth=0.5, alpha=0.4)\n",
    "            else:\n",
    "                ax.text(xlim[0] + i + 0.5, 1, str(total_count), \n",
    "                    va='bottom', ha='center', \n",
    "                    transform=matplotlib.transforms.blended_transform_factory(ax.transData, ax.transAxes),\n",
    "                    clip_on=False,\n",
    "                    # fontsize=9,\n",
    "                    fontsize='small',\n",
    "                )\n",
    "                    \n",
    "        \n",
    "\n",
    "    last_ax = axes[-1]\n",
    "    xs = np.arange(num_of_phyla)\n",
    "    # last_ax.set_xticks([])\n",
    "    last_ax.set_xticks(xs)\n",
    "    last_ax.set_xticklabels(phylum_names_sorted, rotation=45, ha=\"right\", rotation_mode=\"anchor\", fontsize='x-small')\n",
    "\n",
    "    cbar = fig.colorbar(\n",
    "        im, \n",
    "        # ax=last_ax, \n",
    "        ax=colorbar_ax, \n",
    "        # pad=0.15,\n",
    "        # https://stackoverflow.com/questions/18195758/set-matplotlib-colorbar-size-to-match-graph\n",
    "        # fraction=fraction=0.06,\n",
    "        fraction=0.09,\n",
    "        orientation='horizontal',\n",
    "    )\n",
    "    cbar.ax.set_title(f'{counted_obj_desc} fraction', fontsize='small')\n",
    "    cbar.ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%g'))\n",
    "    \n",
    "def draw_variant_representation(fig, ax, y_pos, ir_pairs, y_height, color, variant_regions_and_types=None, transform=None):\n",
    "    if transform is None:\n",
    "        transform = ax.transData\n",
    "        \n",
    "    hatch = '\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\n",
    "    \n",
    "    repeat_arrow_props = dict(\n",
    "        shrinkA=0, shrinkB=0, \n",
    "        linewidth=0.5, \n",
    "        # arrowstyle=\"->\", \n",
    "        # arrowstyle='->,head_width=5',\n",
    "        # arrowstyle='->,head_length=0.22',\n",
    "        arrowstyle='->,head_length=0.22,head_width=0.1',\n",
    "        # mutation_scale=5,\n",
    "        # color='black',\n",
    "    )\n",
    "    repeat_arrow_annotation_kwargs = dict(\n",
    "        arrowprops=repeat_arrow_props, \n",
    "        # clip_on=False, \n",
    "        # zorder=100,\n",
    "        annotation_clip=False,\n",
    "        # transform=transform,\n",
    "        xycoords=transform,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if variant_regions_and_types is None:\n",
    "        if len(ir_pairs) == 1:\n",
    "            ir_pair = next(iter(ir_pairs))\n",
    "            variant_regions_and_types = ((ir_pair[1:3], {'inverted'}), )\n",
    "        else:\n",
    "            assert len(ir_pairs) == 2\n",
    "            ir_pair1, ir_pair2 = list(ir_pairs)\n",
    "            if ir_pair2[0] < ir_pair1[0]:\n",
    "                ir_pair1, ir_pair2 = ir_pair2, ir_pair1\n",
    "            \n",
    "            assert ir_pair1[1] < ir_pair2[0]\n",
    "            assert ir_pair2[3] < ir_pair1[2]\n",
    "            \n",
    "            variant_regions_and_types = (\n",
    "                ((ir_pair1[1], ir_pair2[1]), {'moved', 'inverted'}),\n",
    "                ((ir_pair2[1], ir_pair2[2]), set()),\n",
    "                ((ir_pair2[2], ir_pair1[2]), {'moved', 'inverted'}),\n",
    "            )\n",
    "    for region, region_type in variant_regions_and_types:\n",
    "        region_bottom_left = (region[0], y_pos - y_height / 2)\n",
    "        region_width = region[1] - region[0]\n",
    "        basic_rectangle_kwargs = dict(\n",
    "            xy=region_bottom_left, width=region_width, height=y_height,\n",
    "            facecolor='white', edgecolor=color, fill=False,\n",
    "            linewidth=0.3, \n",
    "            clip_on=False,\n",
    "            transform=transform,\n",
    "            # zorder=100,\n",
    "        )\n",
    "        \n",
    "        if region_type:\n",
    "            if region_type == {'inverted'}:\n",
    "                rectangle_kwargs = dict(basic_rectangle_kwargs)\n",
    "                rectangle_kwargs['fill'] = False\n",
    "                rectangle_kwargs['hatch'] = hatch\n",
    "                ax.add_patch(matplotlib.patches.Rectangle(**rectangle_kwargs))\n",
    "            elif region_type == {'moved'}:\n",
    "                rectangle_kwargs = dict(basic_rectangle_kwargs)\n",
    "                rectangle_kwargs['fill'] = True\n",
    "                rectangle_kwargs['facecolor'] = color\n",
    "                ax.add_patch(matplotlib.patches.Rectangle(**rectangle_kwargs))\n",
    "            elif region_type == {'moved', 'inverted'}:\n",
    "                rectangle_kwargs = dict(basic_rectangle_kwargs)\n",
    "                rectangle_kwargs['fill'] = True\n",
    "                rectangle_kwargs['facecolor'] = color\n",
    "                rectangle_kwargs['edgecolor'] = 'white'\n",
    "                rectangle_kwargs['hatch'] = hatch\n",
    "                ax.add_patch(matplotlib.patches.Rectangle(**rectangle_kwargs))\n",
    "            else:\n",
    "                # print(f'region_type: {region_type}')\n",
    "                assert False\n",
    "        \n",
    "        ax.add_patch(matplotlib.patches.Rectangle(**basic_rectangle_kwargs))\n",
    "    \n",
    "    \n",
    "    for ir_pair in ir_pairs:\n",
    "        ax.annotate(\"\", xy=(ir_pair[1], y_pos), xytext=(ir_pair[0], y_pos), **repeat_arrow_annotation_kwargs)\n",
    "        ax.annotate(\"\", xy=(ir_pair[2], y_pos), xytext=(ir_pair[3], y_pos), **repeat_arrow_annotation_kwargs)\n",
    "    \n",
    "\n",
    "def get_position_tuple_mirror_image(positions):\n",
    "    return tuple(-x for x in positions[::-1])\n",
    "    \n",
    "def get_cds_df_or_mirror_image_cds_df(cds_df, longer_linked_repeat_cds_strand):\n",
    "    if longer_linked_repeat_cds_strand == -1:\n",
    "        cds_df = cds_df.copy()\n",
    "        cds_df.rename(columns={'start_pos': 'end_pos', 'end_pos': 'start_pos'}, inplace=True)\n",
    "        cds_df['start_pos'] = -cds_df['start_pos']\n",
    "        cds_df['end_pos'] = -cds_df['end_pos']\n",
    "        assert (cds_df['strand'].abs() == 1).all()\n",
    "        cds_df['strand'] = -cds_df['strand']\n",
    "    return cds_df\n",
    "\n",
    "def get_ir_pair_region_with_margins_or_its_mirror_image(ir_pair_region_with_margins, longer_linked_repeat_cds_strand):\n",
    "    return (\n",
    "        ir_pair_region_with_margins if (longer_linked_repeat_cds_strand == 1)\n",
    "        else get_position_tuple_mirror_image(ir_pair_region_with_margins)\n",
    "    )\n",
    "\n",
    "def get_variant_ir_pairs_or_their_mirror_images(variant_ir_pairs, longer_linked_repeat_cds_strand):\n",
    "    return (\n",
    "        variant_ir_pairs if (longer_linked_repeat_cds_strand == 1)\n",
    "        else frozenset({get_position_tuple_mirror_image(ir_pair) for ir_pair in variant_ir_pairs})\n",
    "    )\n",
    "\n",
    "def get_variant_regions_and_types_or_their_mirror_images(variant_regions_and_types, longer_linked_repeat_cds_strand):\n",
    "    return (\n",
    "        variant_regions_and_types if (longer_linked_repeat_cds_strand == 1)\n",
    "        else tuple((get_position_tuple_mirror_image(x), y) for x, y in variant_regions_and_types)\n",
    "    )\n",
    "    \n",
    "def draw_cds_context_representatives(fig, context_axes, pie_axes, cds_context_names_and_longer_linked_cds_loci_ordered, non_ref_variant_colors,\n",
    "                                     product_class_to_fill_color_with_alpha, cds_polygon_edge_color_with_alpha, \n",
    "                                     legend_kwargs, draw_variant_repr=True, move_right_for_non_ref_pie=False):\n",
    "    cds_height = 0.4\n",
    "    dist_between_cds_regions = 0.03\n",
    "    cds_region_margin_size_as_fraction_of_cds_region = 0.0002\n",
    "    dist_between_cds_region_bottom_and_arrow_ends = 0.23\n",
    "    vertical_dist_between_different_variant_arrow_heads = 0.2\n",
    "    vertical_dist_between_same_variant_arrows = 0.08\n",
    "    # cds_arrowhead_width_as_fraction_of_max_presumably_relevant_cds_region_len = 0.011\n",
    "    # cds_arrowhead_width_as_fraction_of_max_presumably_relevant_cds_region_len = 0.008\n",
    "    cds_arrowhead_width_as_fraction_of_max_presumably_relevant_cds_region_len = 0.006\n",
    "    \n",
    "    max_num_of_non_ref_variants = len(non_ref_variant_colors)\n",
    "    cds_context_classes_ordered = [x[0].partition(':')[0] for x in cds_context_names_and_longer_linked_cds_loci_ordered]\n",
    "    cds_context_classes = set(cds_context_classes_ordered)\n",
    "    num_of_cds_contexts = len(cds_context_names_and_longer_linked_cds_loci_ordered)\n",
    "    \n",
    "    cds_context_class_to_context_interval = {\n",
    "        x: (cds_context_classes_ordered.index(x), num_of_cds_contexts - cds_context_classes_ordered[::-1].index(x) - 1)\n",
    "        for x in cds_context_classes\n",
    "    }\n",
    "    assert len(generic_utils.get_merged_intervals(cds_context_class_to_context_interval.values())) == len(cds_context_class_to_context_interval)\n",
    "    \n",
    "    \n",
    "    curr_df = all_raw_read_alignment_results_df.copy()\n",
    "    curr_df = curr_df[curr_df['describe_in_the_paper']]\n",
    "    \n",
    "    curr_df = curr_df.merge(pd.DataFrame([(x[:2] + x[2]) for x in cds_context_names_and_longer_linked_cds_loci_ordered],\n",
    "                                         columns=['cds_context_name', 'nuccore_accession', \n",
    "                                                  'longer_linked_repeat_cds_start', 'longer_linked_repeat_cds_end']))\n",
    "    max_presumably_relevant_cds_region_len = (curr_df['presumably_relevant_cds_region_end'] - \n",
    "                                              curr_df['presumably_relevant_cds_region_start'] + 1).max()\n",
    "    \n",
    "    cds_arrowhead_width = round(cds_arrowhead_width_as_fraction_of_max_presumably_relevant_cds_region_len * max_presumably_relevant_cds_region_len)\n",
    "    \n",
    "    curr_df_grouped = curr_df.groupby(['cds_context_name', 'nuccore_accession', 'longer_linked_repeat_cds_start', 'longer_linked_repeat_cds_end'])\n",
    "    cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args = search_for_pis_args['stage6'][\n",
    "        'cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args']\n",
    "    name_to_cds_context_info = search_for_pis_args['enrichment_analysis']['name_to_cds_context_info']\n",
    "    all_product_classes_in_fig = set()\n",
    "    for (cds_context_name, nuccore_accession, longer_linked_repeat_cds_region), context_ax, pie_ax in zip(cds_context_names_and_longer_linked_cds_loci_ordered, context_axes, pie_axes):\n",
    "        # print(cds_context_name)\n",
    "        \n",
    "        alignment_region_raw_read_alignment_args = get_alignment_region_raw_read_alignment_args_by_cds_context_name_and_locus(\n",
    "            cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args, \n",
    "            cds_context_name, nuccore_accession, longer_linked_repeat_cds_region,\n",
    "        )\n",
    "        locus_definition = alignment_region_raw_read_alignment_args['locus_definition']\n",
    "        phylum = alignment_region_raw_read_alignment_args['phylum']\n",
    "        longer_linked_repeat_cds_protein_id = alignment_region_raw_read_alignment_args['longest_linked_repeat_cds_region_and_protein_id'][1]\n",
    "        \n",
    "        longer_linked_repeat_cds_region_middle = np.mean(longer_linked_repeat_cds_region)\n",
    "        \n",
    "        \n",
    "        curr_context_df = curr_df_grouped.get_group((cds_context_name, nuccore_accession, *longer_linked_repeat_cds_region))\n",
    "        assert len(curr_context_df[[\n",
    "            'ir_pairs_df_csv_file_path',\n",
    "            'nuccore_gb_file_path',\n",
    "            'filtered_extended_cds_df_csv_file_path',\n",
    "            'num_of_ref_variant_evidence_reads',\n",
    "            'num_of_non_ref_variant_evidence_reads',\n",
    "            'presumably_relevant_cds_region_start',\n",
    "            'presumably_relevant_cds_region_end',\n",
    "            'nuccore_accession',\n",
    "            'longer_linked_repeat_cds_strand',\n",
    "        ]].drop_duplicates()) == 1\n",
    "        \n",
    "        \n",
    "        ir_pairs_df_csv_file_path = curr_context_df['ir_pairs_df_csv_file_path'].iloc[0]\n",
    "        nuccore_accession = curr_context_df['nuccore_accession'].iloc[0]\n",
    "        ir_pair_region_with_margins = alignment_region_raw_read_alignment_args['ir_pair_region_with_margins']\n",
    "        alignment_region = alignment_region_raw_read_alignment_args['alignment_region']\n",
    "        cds_region = alignment_region_raw_read_alignment_args['presumably_relevant_cds_region']\n",
    "        nuccore_gb_file_path = curr_context_df['nuccore_gb_file_path'].iloc[0]\n",
    "        filtered_extended_cds_df_csv_file_path = curr_context_df['filtered_extended_cds_df_csv_file_path'].iloc[0]\n",
    "        num_of_ref_variant_evidence_reads = curr_context_df['num_of_ref_variant_evidence_reads'].iloc[0]\n",
    "        num_of_non_ref_variant_evidence_reads = curr_context_df['num_of_non_ref_variant_evidence_reads'].iloc[0]\n",
    "        longer_linked_repeat_cds_strand = curr_context_df['longer_linked_repeat_cds_strand'].iloc[0]\n",
    "        total_num_of_evidence_reads = num_of_ref_variant_evidence_reads + num_of_non_ref_variant_evidence_reads\n",
    "        \n",
    "        sra_accession_to_variants_and_reads_info = alignment_region_raw_read_alignment_args[\n",
    "            'sra_accession_to_variants_and_reads_info']\n",
    "        assert len(sra_accession_to_variants_and_reads_info) == 1\n",
    "        reads_info = next(iter(\n",
    "            sra_accession_to_variants_and_reads_info.values()))\n",
    "        non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref = reads_info[\n",
    "            'non_ref_variant_read_name_to_possible_ir_pairs_used_to_reach_from_ref']\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        cds_df = get_cds_df_with_product_class(filtered_extended_cds_df_csv_file_path, alignment_region_raw_read_alignment_args)\n",
    "            \n",
    "        all_product_classes_in_fig |= set(cds_df['product_class'])\n",
    "        \n",
    "        cds_region_len = cds_region[1] - cds_region[0] + 1\n",
    "        # region_top = 0.97\n",
    "        region_top = 0.5 + cds_height / 2\n",
    "        \n",
    "        region_bottom = region_top - cds_height\n",
    "        \n",
    "        cds_region_margin_size = round(cds_region_margin_size_as_fraction_of_cds_region * cds_region_len)\n",
    "        \n",
    "        \n",
    "        draw_cds_invertion_on_ax(\n",
    "            ax=context_ax,\n",
    "            # region=region,\n",
    "            # region=get_ir_pair_region_with_margins_or_its_mirror_image(ir_pair_region_with_margins, longer_linked_repeat_cds_strand),\n",
    "            cds_df=get_cds_df_or_mirror_image_cds_df(cds_df, longer_linked_repeat_cds_strand),\n",
    "            cds_arrowhead_width=cds_arrowhead_width,\n",
    "            region_bottom_and_top=(region_bottom, region_top),\n",
    "            cds_polygon_edge_color_with_alpha=cds_polygon_edge_color_with_alpha,\n",
    "            product_class_to_fill_color_with_alpha=product_class_to_fill_color_with_alpha,\n",
    "            draw_region_rectangle=False,\n",
    "            clip_on=False,\n",
    "            draw_cds_polygon_again=True,\n",
    "            # cds_polygon_edge_color=None,\n",
    "            # cds_polygon_edge_color='white',\n",
    "        )\n",
    "        text_left_to_context = f'{locus_definition}\\n{nuccore_accession}\\n{cds_region[0]}-{cds_region[1]}'\n",
    "        \n",
    "        if longer_linked_repeat_cds_strand == -1:\n",
    "            text_left_to_context += ', complementary'\n",
    "        \n",
    "        context_ax.text(-0.01, 0.5, text_left_to_context, \n",
    "                        va='center', ha='right', transform=context_ax.transAxes, clip_on=False, fontsize='x-small')\n",
    "        \n",
    "        \n",
    "        non_ref_variant_ir_pairs_to_num_of_evidence_reads = get_variant_ir_pairs_to_num_of_evidence_reads(\n",
    "            non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Actually, this isn't guaranteed, because a read can be discarded due to a better alignment to somewhere else in the genome.\n",
    "        # For example, SRR11812841.1.24913.1 (though see detailed explanation in massive_screening_configuration.py).\n",
    "        # assert sum(non_ref_variant_ir_pairs_to_num_of_evidence_reads.values()) == num_of_non_ref_variant_evidence_reads\n",
    "        \n",
    "        \n",
    "        sorted_non_ref_variant_ir_pairs_and_num_of_evidence_reads = sorted(\n",
    "            non_ref_variant_ir_pairs_to_num_of_evidence_reads.items(), key=(lambda x: x[1]), reverse=True)\n",
    "        \n",
    "        if max_num_of_non_ref_variants < len(non_ref_variant_ir_pairs_to_num_of_evidence_reads):\n",
    "            raise RuntimeError(f'too many non ref variants ({len(non_ref_variant_ir_pairs_to_num_of_evidence_reads)} variants here)!')\n",
    "            \n",
    "        # assert len(non_ref_variant_colors) >= len(non_ref_variant_ir_pairs_to_num_of_evidence_reads)\n",
    "        \n",
    "        non_ref_variant_ir_pairs_to_color = {x: y for (x, _), y in zip(sorted_non_ref_variant_ir_pairs_and_num_of_evidence_reads, non_ref_variant_colors)}\n",
    "        \n",
    "        if draw_variant_repr:\n",
    "            variant_height = vertical_dist_between_different_variant_arrow_heads / 2\n",
    "            \n",
    "            for i, (variant_ir_pairs, color) in enumerate(non_ref_variant_ir_pairs_to_color.items()):\n",
    "                if i % 2 == 0:\n",
    "                    variant_repr_y = (region_bottom - dist_between_cds_region_bottom_and_arrow_ends - \n",
    "                                      (i // 2) * vertical_dist_between_different_variant_arrow_heads)\n",
    "                else:\n",
    "                    variant_repr_y = (region_top + dist_between_cds_region_bottom_and_arrow_ends + \n",
    "                                      (i // 2) * vertical_dist_between_different_variant_arrow_heads)\n",
    "                \n",
    "                try:\n",
    "                    variant_regions_and_types = reads_info[\n",
    "                        'complex_variant_ir_pairs_to_variant_regions_and_types'][variant_ir_pairs]\n",
    "                    variant_regions_and_types_or_their_mirror_images = get_variant_regions_and_types_or_their_mirror_images(\n",
    "                        variant_regions_and_types, longer_linked_repeat_cds_strand)\n",
    "                except KeyError:\n",
    "                    variant_regions_and_types_or_their_mirror_images = None\n",
    "                    \n",
    "                except KeyError:\n",
    "                    variant_regions_and_types = None\n",
    "                draw_variant_representation(\n",
    "                    fig=fig, \n",
    "                    ax=context_ax, \n",
    "                    y_pos=variant_repr_y, \n",
    "                    ir_pairs=get_variant_ir_pairs_or_their_mirror_images(variant_ir_pairs, longer_linked_repeat_cds_strand),\n",
    "                    y_height=variant_height, \n",
    "                    color=color, \n",
    "                    variant_regions_and_types=variant_regions_and_types_or_their_mirror_images,\n",
    "                )\n",
    "                \n",
    "        \n",
    "        if longer_linked_repeat_cds_strand == 1:\n",
    "            xlim_left = cds_region[0]\n",
    "            xlim_right = cds_region[0] + max_presumably_relevant_cds_region_len\n",
    "            assert xlim_right >= cds_region[1]\n",
    "        else:\n",
    "            xlim_left = -cds_region[1]\n",
    "            xlim_right = xlim_left + max_presumably_relevant_cds_region_len\n",
    "            assert xlim_right >= -cds_region[0]\n",
    "            \n",
    "        x_lims = (xlim_left, xlim_right)\n",
    "        context_ax.set_xlim(x_lims)\n",
    "        context_ax.set_ylim(-0.25, 1.25)\n",
    "        \n",
    "        num_of_evidence_reads_per_variant_sorted = [x for (_, x) in sorted_non_ref_variant_ir_pairs_and_num_of_evidence_reads]\n",
    "        variant_colors_sorted = [non_ref_variant_ir_pairs_to_color[x] for x, _ in sorted_non_ref_variant_ir_pairs_and_num_of_evidence_reads]\n",
    "        total_num_of_non_ref_reads = sum(num_of_evidence_reads_per_variant_sorted)\n",
    "        ref_start_angle = 180 + total_num_of_non_ref_reads / (num_of_ref_variant_evidence_reads + total_num_of_non_ref_reads) * 180\n",
    "        \n",
    "        num_of_non_ref_variants = len(num_of_evidence_reads_per_variant_sorted)\n",
    "        draw_non_ref_pie = (num_of_non_ref_variants >= 4) and (num_of_non_ref_variants <= 10)\n",
    "        pie_radius = 1\n",
    "        \n",
    "        if draw_non_ref_pie and move_right_for_non_ref_pie:\n",
    "            ref_pie_center = (2.7, 0)\n",
    "            non_ref_pie_center = (0, 0)\n",
    "        else:\n",
    "            ref_pie_center = (0, 0)\n",
    "            non_ref_pie_center = (-2.7, 0)\n",
    "        \n",
    "        pie_ax.pie([num_of_ref_variant_evidence_reads] + num_of_evidence_reads_per_variant_sorted, \n",
    "                   colors=(['black'] + variant_colors_sorted), \n",
    "                   radius=pie_radius, \n",
    "                   # center=(cds_region[1] + 400, 0),\n",
    "                   center=ref_pie_center,\n",
    "                   startangle=ref_start_angle,\n",
    "             # counterclock=True, startangle=0,\n",
    "             )\n",
    "        pie_frame_circle_kwargs = dict(\n",
    "            color='black', fill=False, clip_on=False,\n",
    "            linewidth=0.4,\n",
    "        )\n",
    "        # pie_ax.add_patch(plt.Circle(ref_pie_center, radius=pie_radius, **pie_frame_circle_kwargs))\n",
    "        \n",
    "        if draw_non_ref_pie:\n",
    "            ref_start_angle_in_radians = ref_start_angle * np.pi / 180\n",
    "            ref_start_x = ref_pie_center[0] + np.cos(ref_start_angle_in_radians)\n",
    "            ref_start_y = ref_pie_center[1] + np.sin(ref_start_angle_in_radians)\n",
    "            \n",
    "            non_ref_pie_center_x = non_ref_pie_center[0]\n",
    "            non_ref_pie_radius = 1\n",
    "            \n",
    "            \n",
    "            # I want each of the lines that connect the two pies to be tangent to the new pie. I didn't manage to find an analytic solution\n",
    "            # fast, so I just solve it numerically (quick and dirty, but it seems to work).\n",
    "\n",
    "            # use the law of cosines\n",
    "            c_squared = (ref_start_x - non_ref_pie_center_x) ** 2 + ref_start_y ** 2\n",
    "            a = non_ref_pie_radius\n",
    "            a_squared = a ** 2\n",
    "            \n",
    "            def get_wanted_point_x(radian_angle_in_non_ref_pie):\n",
    "                return non_ref_pie_center_x + np.cos(radian_angle_in_non_ref_pie) * non_ref_pie_radius\n",
    "            \n",
    "            def get_wanted_point_y(radian_angle_in_non_ref_pie):\n",
    "                return np.sin(radian_angle_in_non_ref_pie) * non_ref_pie_radius\n",
    "                \n",
    "            def get_abs_dif_between_angle_and_90_in_radians(radian_angle_in_non_ref_pie):\n",
    "                x = get_wanted_point_x(radian_angle_in_non_ref_pie)\n",
    "                y = get_wanted_point_y(radian_angle_in_non_ref_pie)\n",
    "                b_squared = (ref_start_x - x) ** 2 + (-ref_start_y - y) ** 2\n",
    "                b = np.sqrt(b_squared)\n",
    "                return abs(np.arccos((a_squared + b_squared - c_squared) / (2 * a * b)) - np.pi / 2)\n",
    "            \n",
    "            angle_for_wanted_point = min([angle for angle in np.linspace(0, np.pi / 2, 100)], key=get_abs_dif_between_angle_and_90_in_radians)\n",
    "            wanted_point_x = get_wanted_point_x(angle_for_wanted_point)\n",
    "            wanted_upper_point_y = get_wanted_point_y(angle_for_wanted_point)\n",
    "            \n",
    "            pie_ax.add_patch(matplotlib.patches.Polygon(\n",
    "                [\n",
    "                    (wanted_point_x, wanted_upper_point_y),\n",
    "                    (wanted_point_x, -wanted_upper_point_y),\n",
    "                    (ref_start_x, ref_start_y),\n",
    "                    (ref_start_x, -ref_start_y),\n",
    "                ], \n",
    "                closed=True, fill=True, facecolor=(*matplotlib.colors.to_rgb('black'), 0.1), edgecolor=(*matplotlib.colors.to_rgb('black'), 1),\n",
    "                linewidth=0.3, \n",
    "                clip_on=False,\n",
    "            )).set_zorder(0)\n",
    "            \n",
    "            non_ref_pie_center = (non_ref_pie_center_x, 0)\n",
    "            non_ref_pie_patches, _ = pie_ax.pie(\n",
    "                num_of_evidence_reads_per_variant_sorted, \n",
    "                colors=variant_colors_sorted,\n",
    "                radius=non_ref_pie_radius, \n",
    "                # center=(cds_region[1] + 400, 0),\n",
    "                center=non_ref_pie_center,\n",
    "                startangle=0,\n",
    "                # counterclock=True, startangle=0,\n",
    "            )\n",
    "            [x.set_zorder(100) for x in non_ref_pie_patches]\n",
    "            \n",
    "            # pie_ax.add_patch(plt.Circle(non_ref_pie_center, radius=non_ref_pie_radius, **pie_frame_circle_kwargs))\n",
    "            \n",
    "\n",
    "        pie_ax.set_ylim(-1.01, 1.01)\n",
    "        pie_ax.set_xlim(-1.01, 1.01)\n",
    "        pie_ax.text(\n",
    "            ref_pie_center[0] + 1.13, 0, str(total_num_of_evidence_reads), va='center', ha='left', \n",
    "            # ref_pie_center[0] + 1.03, 0.5, str(total_num_of_evidence_reads), va='center', ha='left', \n",
    "            # transform=(matplotlib.transforms.blended_transform_factory(fig4_pie_axes[0].transAxes, pie_ax.transAxes)),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # ax.set_xlim(cds_region[0] - 300, cds_region[1] + )\n",
    "        \n",
    "    legend_elements = [\n",
    "        # ax.add_patch(matplotlib.patches.Polygon([(0,0), (0,0)], closed=True, fill=True, facecolor=product_class_to_fill_color_with_alpha[product_class], \n",
    "        #                                         edgecolor=cds_polygon_edge_color_with_alpha, label=product_class)) \n",
    "        matplotlib.patches.Patch(facecolor=product_class_to_fill_color_with_alpha[product_class], edgecolor=cds_polygon_edge_color_with_alpha,\n",
    "                         label=product_class)\n",
    "        for product_class in PRODUCT_CLASSES_ORDERED\n",
    "        if (product_class not in ('other', 'hypothetical')) and (product_class in all_product_classes_in_fig)\n",
    "    ]\n",
    "\n",
    "    \n",
    "    \n",
    "    fig.legend(\n",
    "        handles=legend_elements,\n",
    "        **legend_kwargs,\n",
    "    )\n",
    "\n",
    "def draw_long_read_alignments(fig, axes, cds_context_name, nuccore_accession, longer_linked_repeat_cds_region, \n",
    "                              product_class_to_fill_color_with_alpha, cds_polygon_edge_color_with_alpha, non_ref_variant_colors,\n",
    "                             num_of_axes_in_line, cds_arrowhead_width_as_fraction_of_ir_pair_region_with_margins=0.05):\n",
    "    \n",
    "    region_top = 0.12\n",
    "    region_bottom = 0.04\n",
    "    dist_between_arbitrary_lowest_pos_in_read_of_different_variants_as_fraction_of_ir_pair_region_with_margins = 0.25\n",
    "    \n",
    "    cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args = search_for_pis_args['stage6'][\n",
    "        'cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args']\n",
    "    alignment_region_raw_read_alignment_args = (\n",
    "        get_alignment_region_raw_read_alignment_args_by_cds_context_name_and_locus(\n",
    "            cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args, \n",
    "            cds_context_name, \n",
    "            nuccore_accession, \n",
    "            longer_linked_repeat_cds_region,\n",
    "        )\n",
    "    )\n",
    "    ir_pair_region_with_margins = alignment_region_raw_read_alignment_args['ir_pair_region_with_margins']\n",
    "    \n",
    "    ir_pair_region_with_margins_len = ir_pair_region_with_margins[1] - ir_pair_region_with_margins[0] + 1\n",
    "    \n",
    "    curr_df = all_raw_read_alignment_results_df.copy()\n",
    "    curr_df = curr_df[curr_df['cds_context_name'] == cds_context_name]\n",
    "    curr_df = curr_df[curr_df['nuccore_accession'] == nuccore_accession]\n",
    "    curr_df = curr_df[curr_df['ir_pair_region_with_margins_start'] == ir_pair_region_with_margins[0]]\n",
    "    curr_df = curr_df[curr_df['ir_pair_region_with_margins_end'] == ir_pair_region_with_margins[1]]\n",
    "    curr_df = curr_df[curr_df['is_best_evidence_read_for_variant']]\n",
    "\n",
    "    longer_linked_repeat_cds_strand = set(curr_df['longer_linked_repeat_cds_strand'])\n",
    "    assert len(longer_linked_repeat_cds_strand) == 1\n",
    "    longer_linked_repeat_cds_strand = next(iter(longer_linked_repeat_cds_strand))\n",
    "    \n",
    "    ref_read_name = curr_df[curr_df['read_evidence_type'] == 'ref_variant']['sseqid'].iloc[0]\n",
    "    non_ref_read_names = set(curr_df[curr_df['read_evidence_type'] == 'non_ref_variant']['sseqid'])\n",
    "    num_of_variants = len(curr_df)\n",
    "    \n",
    "    \n",
    "    assert curr_df['filtered_extended_cds_df_csv_file_path'].nunique() == 1\n",
    "    filtered_extended_cds_df_csv_file_path = curr_df['filtered_extended_cds_df_csv_file_path'].iloc[0]\n",
    "    assert curr_df['best_evidence_read_name_to_info_pickle_file_path'].nunique() == 1\n",
    "    best_evidence_read_name_to_info_pickle_file_path = curr_df['best_evidence_read_name_to_info_pickle_file_path'].iloc[0]\n",
    "    \n",
    "    with open(best_evidence_read_name_to_info_pickle_file_path, 'rb') as f:\n",
    "        best_evidence_read_name_to_info = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    sra_accession_to_variants_and_reads_info = alignment_region_raw_read_alignment_args[\n",
    "        'sra_accession_to_variants_and_reads_info']\n",
    "    assert len(sra_accession_to_variants_and_reads_info) == 1\n",
    "    reads_info = next(iter(\n",
    "        sra_accession_to_variants_and_reads_info.values()))\n",
    "    non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref = reads_info['non_ref_variant_read_name_to_possible_ir_pairs_used_to_reach_from_ref']\n",
    "    non_ref_variant_ir_pairs_to_num_of_evidence_reads = get_variant_ir_pairs_to_num_of_evidence_reads(non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref)\n",
    "    \n",
    "    non_ref_variant_ir_pairs_ordered = sorted(non_ref_variant_ir_pairs_to_num_of_evidence_reads, reverse=True, \n",
    "                                              key=lambda x: non_ref_variant_ir_pairs_to_num_of_evidence_reads[x])\n",
    "    non_ref_read_names_sorted = [\n",
    "        [read_name for read_name in non_ref_read_names\n",
    "         if (non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref[read_name] == variant_ir_pairs)][0]\n",
    "        for variant_ir_pairs in non_ref_variant_ir_pairs_ordered\n",
    "    ]\n",
    "    ref_and_non_ref_read_names_sorted = [ref_read_name] + non_ref_read_names_sorted\n",
    "    \n",
    "    \n",
    "    cds_df = get_cds_df_with_product_class(filtered_extended_cds_df_csv_file_path, alignment_region_raw_read_alignment_args)\n",
    "\n",
    "    assert len(non_ref_read_names_sorted) <= len(non_ref_variant_colors)\n",
    "    \n",
    "    num_of_reads_to_plot = len(ref_and_non_ref_read_names_sorted)\n",
    "    for i in range(num_of_reads_to_plot, len(axes)):\n",
    "        remove_all_ax_spines_an_x_and_y_axes(axes[i])\n",
    "        \n",
    "    max_ys = []\n",
    "    for ax, read_name, variant_color, variant_ir_pairs in zip(axes, ref_and_non_ref_read_names_sorted, \n",
    "                                                              ['black'] + non_ref_variant_colors, [None] + non_ref_variant_ir_pairs_ordered):\n",
    "        # print(read_name)\n",
    "        transform_for_plotting_cds = matplotlib.transforms.blended_transform_factory(ax.transData, ax.transAxes)\n",
    "        cds_arrowhead_width = round(cds_arrowhead_width_as_fraction_of_ir_pair_region_with_margins * ir_pair_region_with_margins_len)\n",
    "        \n",
    "\n",
    "        draw_cds_invertion_on_ax(\n",
    "            ax=ax,\n",
    "            region=get_ir_pair_region_with_margins_or_its_mirror_image(ir_pair_region_with_margins, longer_linked_repeat_cds_strand), # could actually remove this line?\n",
    "            cds_df=get_cds_df_or_mirror_image_cds_df(cds_df, longer_linked_repeat_cds_strand),\n",
    "            # ir_pairs_df=ir_pairs_df,\n",
    "            cds_arrowhead_width=cds_arrowhead_width,\n",
    "            region_bottom_and_top=(region_bottom, region_top),\n",
    "            cds_polygon_edge_color_with_alpha=cds_polygon_edge_color_with_alpha,\n",
    "            product_class_to_fill_color_with_alpha=product_class_to_fill_color_with_alpha,\n",
    "            draw_region_rectangle=False,\n",
    "            transform=transform_for_plotting_cds,\n",
    "            # clip_on=False,\n",
    "        )\n",
    "        \n",
    "        if variant_ir_pairs:\n",
    "            try:\n",
    "                variant_regions_and_types = reads_info[\n",
    "                    'complex_variant_ir_pairs_to_variant_regions_and_types'][variant_ir_pairs]\n",
    "                variant_regions_and_types_or_their_mirror_images = get_variant_regions_and_types_or_their_mirror_images(\n",
    "                    variant_regions_and_types, longer_linked_repeat_cds_strand)\n",
    "            except KeyError:\n",
    "                variant_regions_and_types_or_their_mirror_images = None\n",
    "            draw_variant_representation(\n",
    "                fig=fig, \n",
    "                ax=ax, \n",
    "                y_pos=-0.04, \n",
    "                ir_pairs=get_variant_ir_pairs_or_their_mirror_images(variant_ir_pairs, longer_linked_repeat_cds_strand),\n",
    "                y_height=0.06, \n",
    "                color=variant_color, \n",
    "                variant_regions_and_types=variant_regions_and_types_or_their_mirror_images,\n",
    "                transform=transform_for_plotting_cds,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        mauve_xmfa_file_path = best_evidence_read_name_to_info[read_name]['mauve_alignment_results_xmfa_file_path']\n",
    "        # print(mauve_xmfa_file_path)\n",
    "        list_of_matching_positions_df_per_pairwise_alignment = mauve_interface_and_utils.get_list_of_matching_positions_df_for_each_pairwise_alignment_in_xmfa(\n",
    "            mauve_xmfa_file_path,\n",
    "        )\n",
    "\n",
    "\n",
    "        lowest_pos_in_read = min(x['seq0_position'].min() for x in list_of_matching_positions_df_per_pairwise_alignment)\n",
    "        max_pos_in_read = max(x['seq0_position'].max() for x in list_of_matching_positions_df_per_pairwise_alignment)\n",
    "        \n",
    "        for matching_positions_df in list_of_matching_positions_df_per_pairwise_alignment:\n",
    "            seq1_positions = matching_positions_df['seq1_position'] + ir_pair_region_with_margins[0] - 1\n",
    "            \n",
    "            ax.scatter(\n",
    "                seq1_positions if (longer_linked_repeat_cds_strand == 1) else -seq1_positions,\n",
    "                matching_positions_df['seq0_position'] - lowest_pos_in_read + 1,\n",
    "                s=0.1,\n",
    "                marker='o',\n",
    "                # marker='.',\n",
    "                # color=variant_color,\n",
    "                facecolors=variant_color,\n",
    "                edgecolors=variant_color,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "        \n",
    "        curr_max_y = max_pos_in_read - lowest_pos_in_read + 1\n",
    "        max_ys.append(curr_max_y)\n",
    "        \n",
    "    max_y = max(max_ys)\n",
    "    for ax in axes[:num_of_reads_to_plot]:\n",
    "        min_y = -100\n",
    "        assert region_top > 0\n",
    "        new_ylim_bottom = (min_y - region_top * max_y) / (1 - region_top)\n",
    "        ax.set_ylim(new_ylim_bottom, max_y)\n",
    "        \n",
    "        # set_x_lim_according_to_region(ax, ir_pair_region_with_margins, margin_relative_size=0.001)\n",
    "        # ax.set_xlim(ir_pair_region_with_margins)\n",
    "        ax.set_xlim(get_ir_pair_region_with_margins_or_its_mirror_image(ir_pair_region_with_margins, longer_linked_repeat_cds_strand))\n",
    "        # ax.spines['left'].set_visible(True)\n",
    "        ax.get_yaxis().set_visible(True)\n",
    "        ax.plot([(ir_pair_region_with_margins[0] if (longer_linked_repeat_cds_strand == 1) else -ir_pair_region_with_margins[1])] * 2, \n",
    "                [region_top, 1], color='black', clip_on=True, \n",
    "                transform=matplotlib.transforms.blended_transform_factory(ax.transData, ax.transAxes))\n",
    "        ax.set_aspect('equal')\n",
    "        yticks = ax.get_yticks().tolist()\n",
    "        new_yticks = [x for x in yticks if ((x % 1000 == 0) and x >= 0)]\n",
    "        ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator(new_yticks))\n",
    "        ax.set_yticklabels([remove_trailing_zeros(str(x / 1000)) for x in new_yticks])\n",
    "        # ax.get_xaxis().set_visible(True)\n",
    "    \n",
    "    for i in range(0, num_of_reads_to_plot, num_of_axes_in_line):\n",
    "        axes[i].set_ylabel(\n",
    "            # 'position in\\nread (kbp)', \n",
    "            'relative position\\nin read (kbp)', \n",
    "            fontsize='small',\n",
    "        )\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "taxon_heatmap_colormap = matplotlib.colors.LinearSegmentedColormap.from_list('', ['0.8', '0'])\n",
    "taxon_heatmap_colormap = matplotlib.colors.LinearSegmentedColormap.from_list('', ['0.85', '0'])\n",
    "taxon_heatmap_colormap = matplotlib.colors.LinearSegmentedColormap.from_list('', ['0.9', '0'])\n",
    "taxon_heatmap_colormap = matplotlib.colors.LinearSegmentedColormap.from_list('', ['1', '0'])\n",
    "taxon_heatmap_colormap.set_bad(color='white')\n",
    "\n",
    "legend_kwargs = dict(\n",
    "    bbox_transform=fig4_long_read_alignment_axes[-1].transAxes,\n",
    "    bbox_to_anchor=(1.3, 0.5),\n",
    "    loc='center left',\n",
    "    # fontsize='x-small', \n",
    "    fontsize='small', \n",
    ")\n",
    "    \n",
    "draw_cds_context_representatives(\n",
    "    fig4, context_axes=fig4_cds_context_axes, pie_axes=fig4_pie_axes,\n",
    "    cds_context_names_and_longer_linked_cds_loci_ordered=FIG4_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED, \n",
    "    non_ref_variant_colors=NON_REF_VARIANT_COLORS,\n",
    "    product_class_to_fill_color_with_alpha=FIG4_PRODUCT_CLASS_TO_FILL_COLOR_WITH_ALPHA,\n",
    "    cds_polygon_edge_color_with_alpha=FIG4_CDS_POLYGON_EDGE_COLOR_WITH_ALPHA,\n",
    "    legend_kwargs=legend_kwargs,\n",
    "    # draw_variant_repr=False,\n",
    ")\n",
    "\n",
    "draw_taxon_distribution_heatmap(\n",
    "    fig4, axes=fig4_taxon_dist_axes, colorbar_ax=fig4_taxon_dist_colorbar_ax,\n",
    "    cds_context_names_and_longer_linked_cds_loci_ordered=FIG4_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED, \n",
    "    # taxon_column_name_for_counting='taxon_uid', \n",
    "    taxon_column_name_for_counting='taxon_species', \n",
    "    counted_obj_desc='species',\n",
    "    heatmap_colormap=taxon_heatmap_colormap,\n",
    ")\n",
    "\n",
    "draw_long_read_alignments(\n",
    "    fig4, fig4_long_read_alignment_axes, \n",
    "    cds_context_name=FIG4_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED[0][0], \n",
    "    nuccore_accession=FIG4_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED[0][1], \n",
    "    longer_linked_repeat_cds_region=FIG4_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED[0][2], \n",
    "    cds_polygon_edge_color_with_alpha=FIG4_CDS_POLYGON_EDGE_COLOR_WITH_ALPHA,\n",
    "    product_class_to_fill_color_with_alpha=FIG4_PRODUCT_CLASS_TO_FILL_COLOR_WITH_ALPHA,\n",
    "    non_ref_variant_colors=NON_REF_VARIANT_COLORS,\n",
    "    num_of_axes_in_line=4,\n",
    "    cds_arrowhead_width_as_fraction_of_ir_pair_region_with_margins=0.05,\n",
    ")\n",
    "\n",
    "arbitrary_ax = fig4_long_read_alignment_axes[0]\n",
    "arbitrary_ax.text(0.015, 0.985, 'A', va='center', ha='center', transform=fig4.transFigure, fontweight='bold')\n",
    "arbitrary_ax.text(0.015, 1.08, 'B', va='center', ha='center', fontweight='bold',\n",
    "                  transform=matplotlib.transforms.blended_transform_factory(fig4.transFigure, fig4_cds_context_axes[0].transAxes))\n",
    "arbitrary_ax.text(-0.09, 1.08, 'C', va='center', ha='center', fontweight='bold',\n",
    "                  transform=matplotlib.transforms.blended_transform_factory(fig4_pie_axes[0].transAxes, fig4_cds_context_axes[0].transAxes))\n",
    "arbitrary_ax.text(-0.07, 1.08, 'D', va='center', ha='center',fontweight='bold',\n",
    "                  transform=matplotlib.transforms.blended_transform_factory(fig4_taxon_dist_axes[0].transAxes, fig4_cds_context_axes[0].transAxes))\n",
    "\n",
    "if 1:\n",
    "    fig4.savefig(os.path.join(paper_figs_and_tables_dir_path, 'fig4.png'))\n",
    "    fig4.savefig(os.path.join(paper_figs_and_tables_dir_path, 'fig4.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41cde6-48c4-4a5f-b689-e8f357d2b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    plt.close('all')\n",
    "# raise\n",
    "\n",
    "sup_fig_start_num = 9\n",
    "num_of_long_read_alignments_per_line = 4\n",
    "for i, cds_context_name_and_locus in enumerate(ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED[1:]):\n",
    "    print(f'cds_context_name_and_locus: {cds_context_name_and_locus}')\n",
    "    cds_context_name, nuccore_accession, longer_linked_repeat_cds_region = cds_context_name_and_locus\n",
    "    \n",
    "    cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args = search_for_pis_args['stage6'][\n",
    "        'cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args']\n",
    "    alignment_region_raw_read_alignment_args = get_alignment_region_raw_read_alignment_args_by_cds_context_name_and_locus(\n",
    "        cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args, \n",
    "        cds_context_name, nuccore_accession, longer_linked_repeat_cds_region,\n",
    "    )\n",
    "    sra_accession_to_variants_and_reads_info = alignment_region_raw_read_alignment_args[\n",
    "        'sra_accession_to_variants_and_reads_info']\n",
    "    assert len(sra_accession_to_variants_and_reads_info) == 1\n",
    "    reads_info = next(iter(\n",
    "        sra_accession_to_variants_and_reads_info.values()))\n",
    "    non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref = reads_info[\n",
    "        'non_ref_variant_read_name_to_possible_ir_pairs_used_to_reach_from_ref']\n",
    "    non_ref_variant_ir_pairs_to_num_of_evidence_reads = get_variant_ir_pairs_to_num_of_evidence_reads(\n",
    "        non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref)\n",
    "    num_of_non_ref_variants = len(non_ref_variant_ir_pairs_to_num_of_evidence_reads)\n",
    "    num_of_variants = 1 + num_of_non_ref_variants\n",
    "    # num_of_long_read_rows = math.ceil(num_of_variants / 3)\n",
    "    num_of_long_read_rows = 2 if (num_of_variants <= 8) else 7\n",
    "    num_of_long_read_axes = num_of_long_read_rows * num_of_long_read_alignments_per_line\n",
    "    \n",
    "    y_size = 1.6 + 1.8 * num_of_long_read_rows\n",
    "    height_ratios = [0.7, 0.4] + ([2.2, 0.4] * num_of_long_read_rows)[:-1]\n",
    "    if num_of_variants > 15:\n",
    "        height_ratios = [0.76, 1.2] + ([2.2, 0.4] * num_of_long_read_rows)[:-1]\n",
    "    \n",
    "    fig = plt.figure(\n",
    "        figsize=(\n",
    "            10, \n",
    "            y_size,\n",
    "        ), \n",
    "    )\n",
    "    fig_gridspec = fig.add_gridspec(\n",
    "        1 + num_of_long_read_rows * 2, 4, \n",
    "        wspace=0.0001,\n",
    "        hspace=0.0001,\n",
    "        width_ratios=[3,10,0.43,1.2], \n",
    "        height_ratios=height_ratios, \n",
    "    )\n",
    "\n",
    "    fig.subplots_adjust(\n",
    "        # top=0.96 if (num_of_long_read_rows == 2) else 0.945,\n",
    "        top=0.945,\n",
    "        bottom=0.06,\n",
    "        left=0.02,\n",
    "        right=0.85,\n",
    "        # hspace=0.2,\n",
    "        # wspace=0.2\n",
    "    )\n",
    "    # fig1.tight_layout()\n",
    "\n",
    "    long_read_alignment_sub_gridspecs = [\n",
    "        fig_gridspec[x,1].subgridspec(1, 9, width_ratios=[0,1,0.33,1,0.33,1,0.33,1,0], wspace=0.00001)\n",
    "        for x in range(2, 2 + 2 * num_of_long_read_rows, 2)\n",
    "    ]\n",
    "    \n",
    "    long_read_alignment_axes = []\n",
    "    for long_read_alignment_sub_gridspec in long_read_alignment_sub_gridspecs:\n",
    "        long_read_alignment_axes.extend([fig.add_subplot(long_read_alignment_sub_gridspec[i]) for i in (1,3,5,7)])\n",
    "    \n",
    "    cds_context_ax = fig.add_subplot(fig_gridspec[0,1])\n",
    "    pie_ax = fig.add_subplot(fig_gridspec[0,3])\n",
    "\n",
    "    \n",
    "    legend_kwargs = dict(\n",
    "        # bbox_transform=cds_context_ax.transAxes,\n",
    "        # bbox_to_anchor=(1.027, -0.13),\n",
    "        # loc='upper left',\n",
    "        # bbox_transform=long_read_alignment_axes[2].transAxes,\n",
    "        bbox_transform=matplotlib.transforms.blended_transform_factory(pie_ax.transAxes, long_read_alignment_axes[0].transAxes),\n",
    "        bbox_to_anchor=(-0.14, 1.06),\n",
    "        loc='upper left',\n",
    "        fontsize='small', \n",
    "    )\n",
    "\n",
    "    draw_cds_context_representatives(\n",
    "        fig, context_axes=[cds_context_ax], pie_axes=[pie_ax],\n",
    "        cds_context_names_and_longer_linked_cds_loci_ordered=[cds_context_name_and_locus], \n",
    "        non_ref_variant_colors=NON_REF_VARIANT_COLORS,\n",
    "        product_class_to_fill_color_with_alpha=FIG4_PRODUCT_CLASS_TO_FILL_COLOR_WITH_ALPHA,\n",
    "        cds_polygon_edge_color_with_alpha=FIG4_CDS_POLYGON_EDGE_COLOR_WITH_ALPHA,\n",
    "        legend_kwargs=legend_kwargs,\n",
    "        move_right_for_non_ref_pie=True,\n",
    "        # show_variant_ir_pairs=False,\n",
    "    )\n",
    "\n",
    "    draw_long_read_alignments(\n",
    "        fig, long_read_alignment_axes, \n",
    "        cds_context_name=cds_context_name, \n",
    "        nuccore_accession=nuccore_accession, longer_linked_repeat_cds_region=longer_linked_repeat_cds_region, \n",
    "        cds_polygon_edge_color_with_alpha=FIG4_CDS_POLYGON_EDGE_COLOR_WITH_ALPHA,\n",
    "        product_class_to_fill_color_with_alpha=FIG4_PRODUCT_CLASS_TO_FILL_COLOR_WITH_ALPHA,\n",
    "        non_ref_variant_colors=NON_REF_VARIANT_COLORS,\n",
    "        num_of_axes_in_line=num_of_long_read_alignments_per_line,\n",
    "        cds_arrowhead_width_as_fraction_of_ir_pair_region_with_margins=0.03,\n",
    "    )\n",
    "    if 1:\n",
    "        fig.savefig(os.path.join(paper_figs_and_tables_dir_path, f'sup_fig_s{sup_fig_start_num + i}.png'))\n",
    "        # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, f'sup_fig_s{sup_fig_start_num + i}.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5bb2b-c927-4b6e-b289-6a576efea2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_df, excel_file_name, ordered_column_name_and_final_name_and_width, sheet_name in (\n",
    "    (repeat_cds_product_fisher_or_g_result_df, 'table1.xlsx', TARGET_PRODUCT_ENRICHMENT_TABLE_ORDERED_COLUMN_NAME_AND_FINAL_NAME_AND_WIDTH, 'Table 1'),\n",
    "    (adjacent_repeat_cds_product_fisher_or_g_result_df, 'table2.xlsx', NEIGHBOR_PRODUCT_ENRICHMENT_TABLE_ORDERED_COLUMN_NAME_AND_FINAL_NAME_AND_WIDTH, 'Table 2'),\n",
    "):\n",
    "    curr_df = curr_df[curr_df['corrected_pvalue'] <= search_for_pis_args[\n",
    "        'enrichment_analysis']['max_corrected_pvalue_for_product_to_be_considered_significantly_enriched']].copy()\n",
    "    curr_df.sort_values('odds_ratio', ascending=False, inplace=True)\n",
    "\n",
    "    table_file_path = os.path.join(paper_figs_and_tables_dir_path, excel_file_name)\n",
    "\n",
    "    write_xlsx_for_df(table_file_path, sheet_name, curr_df, ordered_column_name_and_final_name_and_width, \n",
    "                      column_name_to_num_format_specifier=PRODUCT_ENRICHMENT_COLUMN_NAME_TO_NUM_FORMAT_SPECIFIER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa02bc-61de-477c-9564-046c1c9fd60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_location_str_repr_columns(df):\n",
    "    if 'longer_linked_repeat_cds_start_pos' in df:\n",
    "        df['longest target CDS location'] = (\n",
    "            df['longer_linked_repeat_cds_start_pos'].astype(str) + '-' + \n",
    "            df['longer_linked_repeat_cds_end_pos'].astype(str)\n",
    "        )\n",
    "    \n",
    "    if 'repeat1_cds_start_pos' in df:\n",
    "        df['left target CDS location'] = (\n",
    "            df['repeat1_cds_start_pos'].astype(str) + '-' + \n",
    "            df['repeat1_cds_end_pos'].astype(str)\n",
    "        )\n",
    "        df['right target CDS location'] = (\n",
    "            df['repeat2_cds_start_pos'].astype(str) + '-' + \n",
    "            df['repeat2_cds_end_pos'].astype(str)\n",
    "        )\n",
    "    \n",
    "    if 'repeat1_cds_operon_start' in df:\n",
    "        df['left target CDS operon'] = (\n",
    "            df['repeat1_cds_operon_start'].astype(int).astype(str) + '-' + \n",
    "            df['repeat1_cds_operon_end'].astype(int).astype(str)\n",
    "        )\n",
    "        df['right target CDS operon'] = (\n",
    "            df['repeat2_cds_operon_start'].astype(int).astype(str) + '-' + \n",
    "            df['repeat2_cds_operon_end'].astype(int).astype(str)\n",
    "        )\n",
    "        \n",
    "    if 'merged_cds_pair_region_start' in df:\n",
    "        curr_df['PIC locus location'] = (\n",
    "            curr_df['merged_cds_pair_region_start'].astype(str) + '-' + \n",
    "            curr_df['merged_cds_pair_region_end'].astype(str)\n",
    "        )\n",
    "        \n",
    "    if 'region_in_other_nuccore_start' in df:\n",
    "        curr_df['homologous locus location'] = (\n",
    "            curr_df['region_in_other_nuccore_start'].astype(str) + '-' + \n",
    "            curr_df['region_in_other_nuccore_end'].astype(str)\n",
    "        )\n",
    "        \n",
    "    if 'start_pos' in df:\n",
    "        curr_df['CDS location'] = (\n",
    "            curr_df['start_pos'].astype(str) + '-' + \n",
    "            curr_df['end_pos'].astype(str)\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF = pd.DataFrame(\n",
    "    [x[1:2] + x[2] for x in ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED],\n",
    "    columns=['nuccore_accession', 'longer_linked_repeat_cds_start_pos', 'longer_linked_repeat_cds_end_pos'],\n",
    ")\n",
    "\n",
    "\n",
    "def verify_sra_accession_and_short_read_name_matches_read_name(reads_df):\n",
    "    for _, row in reads_df.iterrows():\n",
    "        short_read_name = row['short_read_name']\n",
    "        read_name = row['read_name']\n",
    "        sra_accession = row['sra_accession']\n",
    "        \n",
    "        is_valid = (\n",
    "            (read_name == f'{sra_accession}.1.{short_read_name}.1') or\n",
    "            (read_name == f'{sra_accession}.1.{short_read_name}.3') or\n",
    "            (read_name == f'{sra_accession}.1.{short_read_name}.5') or\n",
    "            (read_name == f'{sra_accession}.1.{short_read_name}.7') or\n",
    "            (read_name == f'{sra_accession}.1.{short_read_name}.9') or\n",
    "            (read_name == f'{sra_accession}.3.{short_read_name}.1') or\n",
    "            (read_name == f'{sra_accession}.3.{short_read_name}.3') or\n",
    "            (read_name == f'{sra_accession}.3.{short_read_name}.5') or\n",
    "            (read_name == f'{sra_accession}.3.{short_read_name}.7') or\n",
    "            (read_name == f'{sra_accession}.3.{short_read_name}.9') or\n",
    "            (read_name == f'{sra_accession}.1.{short_read_name}') or\n",
    "            (read_name == f'{sra_accession}.man.{short_read_name}')\n",
    "        )\n",
    "        if not is_valid:\n",
    "            print(read_name, sra_accession, short_read_name)\n",
    "        assert is_valid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620e405-74f7-4254-900f-a315712ea421",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_df = putative_pi_locus_descriptions_df.copy()\n",
    "\n",
    "curr_df = ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF.merge(curr_df)\n",
    "curr_df = add_location_str_repr_columns(curr_df)\n",
    "\n",
    "table_file_path = os.path.join(paper_figs_and_tables_dir_path, 'table3.xlsx')\n",
    "\n",
    "    \n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "    ('nuccore_accession', 'NCBI Nucleotide accession', 22),\n",
    "    ('longest target CDS location', 'longest target CDS location', 22),\n",
    "    # ('longer_linked_repeat_cds_protein_id', 'longest target CDS NCBI Protein accession', 23),\n",
    "    # ('num_of_identified_variants_including_ref', 'number of identified variants', 11),\n",
    "    ('target_gene_product_description', 'longest target CDS product description', 60),\n",
    "    ('locus_description_for_table_3', 'locus description', 80),\n",
    ")\n",
    "\n",
    "write_xlsx_for_df(table_file_path, 'Table 3', curr_df, ordered_column_name_and_final_name_and_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247b127-4dfc-4c57-ac28-cbe6b5e1c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE_BIG_TABLES = False\n",
    "WRITE_BIG_TABLES = True\n",
    "table_file_path = os.path.join(paper_figs_and_tables_dir_path, 'supplementary_tables.xlsx')\n",
    "excel_writer = pd.ExcelWriter(table_file_path, engine='xlsxwriter')\n",
    "\n",
    "curr_df = pd.DataFrame([\n",
    "    ('Table S1', 'Systematically scanned species and their corresponding representative genome NCBI Assembly accessions'),\n",
    "    ('Table S2', 'Homologous loci - loci homologous to programmed inversion candidate (PIC) loci found in same-species genomes'),\n",
    "    ('Table S3', 'PICs for which intra-species variation was identified'),\n",
    "    ('Table S4', 'All PICs and their genomic sequence attribute measures and predicted probability'),\n",
    "    ('Table S5', 'Genbank coding sequence (CDS) product annotations tested for enrichment in predicted programmed '\n",
    "                 'inversion targets'),\n",
    "    ('Table S6', 'Genbank CDS product annotations tested for enrichment in predicted programmed '\n",
    "                 'inversion target neighbors'),\n",
    "    ('Table S7', 'Programmed inversion loci and their corresponding inverted repeats (presumably) promoting programmed inversions'),\n",
    "    ('Table S8', 'DNA-seq Long-reads discarded due to anomalies identified during manual examination'),\n",
    "    ('Table S9', 'DNA-seq Long-reads matching programmed inversion loci'),\n",
    "    ('Table S10', 'GenBank CDS product annotations and their manually assigned gene families'),\n",
    "    ('Table S11', 'Homologs of programmed inversion target CDSs'),\n",
    "    ('Table S12', 'Previously described shufflon proteins (which were aligned to the shufflon and phage tail fusion-protein we identified)'),\n",
    "    ('Table S13', 'Aggregate summary of RNA-seq reads uniquely matching the reference genome or the reference genome with an in-silico inverted region (flanked by inverted repeats)'),\n",
    "    ('Table S14', 'RNA-seq reads uniquely matching the reference genome or the reference genome with an in-silico inverted region (flanked by inverted repeats)'),\n",
    "], columns=['name', 'description'])\n",
    "\n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "        ('name', 'name', 20),\n",
    "        ('description', 'description', 80),\n",
    "    )\n",
    "\n",
    "write_xlsx_for_df(None, 'Legend', curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "\n",
    "if WRITE_BIG_TABLES:\n",
    "    ordered_column_name_and_final_name_and_width = (\n",
    "        ('taxon_species', 'species', 40),\n",
    "        ('primary_assembly_accession', 'NCBI Assembly accession', 18),\n",
    "    )\n",
    "\n",
    "    write_xlsx_for_df(None, 'Supplementary Table S1', taxa_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "\n",
    "if WRITE_BIG_TABLES:\n",
    "    assert not region_in_other_nuccore_df['satisfied_mauve_total_match_proportion_and_min_sub_alignment_min_match_proportion_thresholds'].isna().any()\n",
    "    curr_df = region_in_other_nuccore_df[region_in_other_nuccore_df['satisfied_mauve_total_match_proportion_and_min_sub_alignment_min_match_proportion_thresholds']].copy()\n",
    "    curr_df['minus_mauve_total_match_proportion'] = -curr_df['mauve_total_match_proportion']\n",
    "    curr_df.sort_values([\n",
    "        'nuccore_accession', \n",
    "        'merged_cds_pair_region_start',\n",
    "        'minus_mauve_total_match_proportion',\n",
    "        'other_nuccore_accession',\n",
    "        'region_in_other_nuccore_start',\n",
    "    ], ascending=True, inplace=True)\n",
    "    \n",
    "    curr_df = add_location_str_repr_columns(curr_df)\n",
    "    \n",
    "    ordered_column_name_and_final_name_and_width = (\n",
    "        ('nuccore_accession', 'PIC locus NCBI Nucleotide accession', 24),\n",
    "        ('PIC locus location', 'PIC locus location', 22),\n",
    "        ('other_nuccore_accession', 'homologous locus NCBI Nucleotide accession', 24),\n",
    "        ('homologous locus location', 'homologous locus location', 22),\n",
    "        ('mauve_total_match_proportion', 'match proportion of alignment between PIC locus and homologous locus', 15),\n",
    "    )\n",
    "\n",
    "    write_xlsx_for_df(None, 'Supplementary Table S2', curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer, column_name_to_num_format_specifier={\n",
    "        'mauve_total_match_proportion': '0.000',\n",
    "    })\n",
    "\n",
    "curr_df = filtered_minimal_ir_pairs_linked_to_breakpoint_pairs_df[[\n",
    "    'nuccore_accession', 'index_in_nuccore_ir_pairs_df_csv_file', 'other_nuccore_accession',\n",
    "    'region_in_other_nuccore_start', 'region_in_other_nuccore_end', \n",
    "]].merge(pairs_with_highest_confidence_bps_df[[\n",
    "    'nuccore_accession', 'index_in_nuccore_ir_pairs_df_csv_file', \n",
    "    'repeat1_cds_start_pos',\n",
    "    'repeat1_cds_end_pos',\n",
    "    'repeat1_cds_product',\n",
    "    'repeat2_cds_start_pos',\n",
    "    'repeat2_cds_end_pos',\n",
    "    'repeat2_cds_product',\n",
    "    'high_confidence_bp_for_both_repeats',\n",
    "]])\n",
    "assert curr_df['high_confidence_bp_for_both_repeats'].all()\n",
    "\n",
    "curr_df.sort_values([\n",
    "    'nuccore_accession', \n",
    "    'repeat1_cds_start_pos',\n",
    "    'other_nuccore_accession',\n",
    "    'region_in_other_nuccore_start',\n",
    "], ascending=True, inplace=True)\n",
    "\n",
    "curr_df = add_location_str_repr_columns(curr_df)\n",
    "\n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "    ('nuccore_accession', 'PIC NCBI Nucleotide accession', 24),\n",
    "    ('left target CDS location', 'PIC left target CDS location', 22),\n",
    "    ('right target CDS location', 'PIC right target CDS location', 22),\n",
    "    ('repeat1_cds_product', 'PIC left target CDS product annotation', 25),\n",
    "    ('repeat2_cds_product', 'PIC right target CDS product annotation', 25),\n",
    "    ('other_nuccore_accession', 'homologous locus NCBI Nucleotide accession', 24),\n",
    "    ('homologous locus location', 'homologous locus location', 22),\n",
    ")\n",
    "\n",
    "write_xlsx_for_df(None, 'Supplementary Table S3', curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "    \n",
    "    \n",
    "if WRITE_BIG_TABLES:\n",
    "    curr_df = extended3_all_cds_pairs_df.copy()\n",
    "    curr_df['minus_predicted_any_high_confidence_ir_pair_linked_to_cds_pair_probability'] = -curr_df[\n",
    "        'predicted_any_high_confidence_ir_pair_linked_to_cds_pair_probability']\n",
    "    \n",
    "    curr_df.sort_values([\n",
    "        'minus_predicted_any_high_confidence_ir_pair_linked_to_cds_pair_probability', \n",
    "        'repeat1_cds_product', \n",
    "        'repeat2_cds_product', \n",
    "        'nuccore_accession', \n",
    "        'repeat1_cds_start_pos',\n",
    "    ], ascending=True, inplace=True)\n",
    "    \n",
    "    curr_df = add_location_str_repr_columns(curr_df)\n",
    "    \n",
    "    \n",
    "    ordered_column_name_and_final_name_and_width = (\n",
    "        ('nuccore_accession', 'NCBI Nucleotide accession', 22),\n",
    "        ('left target CDS location', 'left target CDS location', 22),\n",
    "        ('right target CDS location', 'right target CDS location', 22),\n",
    "        ('predicted_any_high_confidence_ir_pair_linked_to_cds_pair_probability', 'predicted programmed inversion probability', 13),\n",
    "        ('predicted_any_high_confidence_ir_pair_linked_to_cds_pair', 'predicted programmed inversion', 12),\n",
    "        \n",
    "        ('repeat1_cds_product', 'left target CDS product annotation', 25),\n",
    "        ('repeat2_cds_product', 'right target CDS product annotation', 25),\n",
    "        \n",
    "        ('left target CDS operon', 'left target CDS operon', 22),\n",
    "        ('right target CDS operon', 'right target CDS operon', 22),\n",
    "        \n",
    "        ('operon_spacer_len', 'CDS distance (bp)', 10),\n",
    "        ('low_operon_spacer_len', 'short CDS distance', 10),\n",
    "        ('max_repeat_len', 'repeat length (bp)', 8),\n",
    "        ('high_max_repeat_len', 'long repeats', 8),\n",
    "        ('operon_asymmetry', 'asymmetry', 11),\n",
    "        ('high_operon_asymmetry', 'high asymmetry', 11),\n",
    "        ('operon_closest_repeat_position_orientation_matching', 'orientation matching', 12),\n",
    "        ('high_operon_closest_repeat_position_orientation_matching', 'high orientation matching', 12),\n",
    "    )\n",
    "\n",
    "    write_xlsx_for_df(None, 'Supplementary Table S4', curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer, column_name_to_num_format_specifier={\n",
    "        'predicted_any_high_confidence_ir_pair_linked_to_cds_pair_probability': '0.000',\n",
    "        'operon_asymmetry': '0.000',\n",
    "        'operon_closest_repeat_position_orientation_matching': '0.000',\n",
    "    })\n",
    "    \n",
    "\n",
    "for curr_df, ordered_column_name_and_final_name_and_width, sheet_name in (\n",
    "    (repeat_cds_product_fisher_or_g_result_df, TARGET_PRODUCT_ENRICHMENT_TABLE_ORDERED_COLUMN_NAME_AND_FINAL_NAME_AND_WIDTH, 'Supplementary Table S5'),\n",
    "    (adjacent_repeat_cds_product_fisher_or_g_result_df, NEIGHBOR_PRODUCT_ENRICHMENT_TABLE_ORDERED_COLUMN_NAME_AND_FINAL_NAME_AND_WIDTH, 'Supplementary Table S6'),\n",
    "):\n",
    "    curr_df = curr_df.copy()\n",
    "    curr_df['minus_odds_ratio'] = -curr_df['odds_ratio']\n",
    "    curr_df.sort_values(['corrected_pvalue', 'minus_odds_ratio'], ascending=True, inplace=True)\n",
    "\n",
    "    write_xlsx_for_df(None, sheet_name, curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer,\n",
    "                      column_name_to_num_format_specifier=PRODUCT_ENRICHMENT_COLUMN_NAME_TO_NUM_FORMAT_SPECIFIER)\n",
    "\n",
    "\n",
    "sra_accession_to_type_and_sra_file_name = search_for_pis_args['stage6']['sra_accession_to_type_and_sra_file_name']\n",
    "\n",
    "read_ends_with_expected_suffix = lambda x: x.rsplit('.')[-1] in (str(x) for x in (1,3,5,7,9))\n",
    "\n",
    "variant_ir_pair_flat_dicts = []\n",
    "anomalous_read_flat_dicts = []\n",
    "non_ref_read_flat_dicts = []\n",
    "for nuccore_accession_to_alignment_regions_raw_read_alignment_args in search_for_pis_args['stage6'][\n",
    "    'cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args'\n",
    "].values():\n",
    "    for nuccore_accession, alignment_regions_raw_read_alignment_args in nuccore_accession_to_alignment_regions_raw_read_alignment_args.items():\n",
    "        for alignment_region_raw_read_alignment_args in alignment_regions_raw_read_alignment_args:\n",
    "            sra_accession_to_variants_and_reads_info = alignment_region_raw_read_alignment_args[\n",
    "                'sra_accession_to_variants_and_reads_info']\n",
    "            assert len(sra_accession_to_variants_and_reads_info) == 1\n",
    "            sra_accession = next(iter(\n",
    "                sra_accession_to_variants_and_reads_info.keys()))\n",
    "            reads_info = next(iter(\n",
    "                sra_accession_to_variants_and_reads_info.values()))\n",
    "            sra_file_name = sra_accession_to_type_and_sra_file_name[sra_accession][1]\n",
    "            read_name_prefix_len = len(sra_file_name) + 1\n",
    "            # print(f'sra_file_name: {sra_file_name}')\n",
    "            # print(f'read_name_prefix_len: {read_name_prefix_len}')\n",
    "            \n",
    "            non_ref_variant_read_name_to_possible_ir_pairs_used_to_reach_from_ref = reads_info[\n",
    "                'non_ref_variant_read_name_to_possible_ir_pairs_used_to_reach_from_ref']\n",
    "            non_ref_variant_ir_pairs_to_num_of_evidence_reads = get_variant_ir_pairs_to_num_of_evidence_reads(\n",
    "                non_ref_variant_read_name_to_possible_ir_pairs_used_to_reach_from_ref)\n",
    "            longer_linked_repeat_cds_region = alignment_region_raw_read_alignment_args['longest_linked_repeat_cds_region_and_protein_id'][0]\n",
    "            sorted_non_ref_variant_ir_pairs_and_num_of_evidence_reads = sorted(non_ref_variant_ir_pairs_to_num_of_evidence_reads.items(), \n",
    "                                                                               reverse=True, key=lambda x: x[1])\n",
    "            for non_ref_variant_ir_pairs, num_of_evidence_reads in sorted_non_ref_variant_ir_pairs_and_num_of_evidence_reads:\n",
    "                variant_ir_pair_repr = massive_screening_configuration.get_variant_ir_pair_str_repr(non_ref_variant_ir_pairs)\n",
    "                variant_ir_pair_flat_dicts.append({\n",
    "                    'nuccore_accession': nuccore_accession,\n",
    "                    'longer_linked_repeat_cds_start_pos': longer_linked_repeat_cds_region[0],\n",
    "                    'longer_linked_repeat_cds_end_pos': longer_linked_repeat_cds_region[1],\n",
    "                    'variant_ir_pair_repr': variant_ir_pair_repr,\n",
    "                    'number of reads matching this variant': num_of_evidence_reads,\n",
    "                })\n",
    "                \n",
    "                sorted_evidence_read_names = sorted(x for x in non_ref_variant_read_name_to_possible_ir_pairs_used_to_reach_from_ref\n",
    "                                                    if non_ref_variant_read_name_to_possible_ir_pairs_used_to_reach_from_ref[x] == non_ref_variant_ir_pairs)\n",
    "                assert len(sorted_evidence_read_names) == num_of_evidence_reads\n",
    "                # print([x for x in sorted_evidence_read_names if not read_ends_with_expected_suffix(x)])\n",
    "                assert all(read_ends_with_expected_suffix(x) for x in sorted_evidence_read_names)\n",
    "                sorted_evidence_short_read_names = [x[read_name_prefix_len:-2] for x in sorted_evidence_read_names]\n",
    "                \n",
    "                non_ref_read_flat_dicts.extend([\n",
    "                    {\n",
    "                        'nuccore_accession': nuccore_accession,\n",
    "                        'longer_linked_repeat_cds_start_pos': longer_linked_repeat_cds_region[0],\n",
    "                        'longer_linked_repeat_cds_end_pos': longer_linked_repeat_cds_region[1],\n",
    "                        'variant_ir_pair_repr': variant_ir_pair_repr, \n",
    "                        'read_name': read_name, \n",
    "                        'sra_accession': sra_accession, \n",
    "                        'short_read_name': short_read_name, \n",
    "                    }\n",
    "                    for read_name, short_read_name in zip(sorted_evidence_read_names, sorted_evidence_short_read_names)\n",
    "                ])\n",
    "                \n",
    "            if 'not_non_ref_variant_read_name_to_anomaly_description' in reads_info:\n",
    "                not_non_ref_variant_read_name_to_anomaly_description = reads_info['not_non_ref_variant_read_name_to_anomaly_description']\n",
    "\n",
    "                sorted_not_non_ref_variant_read_names_and_anomaly_descs = sorted(not_non_ref_variant_read_name_to_anomaly_description.items())\n",
    "                assert all(x.endswith('.1') or x.endswith('.3') for x, _ in sorted_not_non_ref_variant_read_names_and_anomaly_descs)\n",
    "                # print(read_name_prefix_len)\n",
    "                sorted_not_non_ref_short_read_names = [x[read_name_prefix_len:-2] for x, _ in sorted_not_non_ref_variant_read_names_and_anomaly_descs]\n",
    "                \n",
    "                anomalous_read_flat_dicts.extend([\n",
    "                    {\n",
    "                        'nuccore_accession': nuccore_accession,\n",
    "                        'longer_linked_repeat_cds_start_pos': longer_linked_repeat_cds_region[0],\n",
    "                        'longer_linked_repeat_cds_end_pos': longer_linked_repeat_cds_region[1],\n",
    "                        'read_name': read_name, \n",
    "                        'sra_accession': sra_accession, \n",
    "                        'short_read_name': short_read_name, \n",
    "                        'anomaly_description': anomaly_description,\n",
    "                    }\n",
    "                    for (read_name, anomaly_description), short_read_name in zip(sorted_not_non_ref_variant_read_names_and_anomaly_descs, \n",
    "                                                                                 sorted_not_non_ref_short_read_names)\n",
    "                ])\n",
    "\n",
    "\n",
    "curr_df = ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF.merge(pd.DataFrame(variant_ir_pair_flat_dicts))\n",
    "assert (len(curr_df[list(ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF)].drop_duplicates()) == \n",
    "        len(ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF))\n",
    "curr_df = add_location_str_repr_columns(curr_df)\n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "    ('nuccore_accession', 'NCBI Nucleotide accession', 22),\n",
    "    ('longest target CDS location', 'longest target CDS location', 22),\n",
    "    ('variant_ir_pair_repr', 'inverted repeats (presumably) promoting programmed inversions allowing switching from the reference variant to this variant', 50),\n",
    "    ('number of reads matching this variant', 'number of reads matching this variant', 14),\n",
    ")\n",
    "num_of_identified_pilv_phage_tail_collar_variants = 26\n",
    "assert (curr_df[curr_df['nuccore_accession'] == 'CP084655.1']['variant_ir_pair_repr'].nunique() + 1) == num_of_identified_pilv_phage_tail_collar_variants\n",
    "print(f'made sure that including the reference variant, we identified {num_of_identified_pilv_phage_tail_collar_variants} '\n",
    "      'variants for the gene encoding PilV and phage tail collar fusion protein')\n",
    "\n",
    "write_xlsx_for_df(None, 'Supplementary Table S7', curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "\n",
    "\n",
    "curr_df = ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF.merge(pd.DataFrame(anomalous_read_flat_dicts))\n",
    "curr_df = add_location_str_repr_columns(curr_df)\n",
    "verify_sra_accession_and_short_read_name_matches_read_name(curr_df)\n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "    ('nuccore_accession', 'NCBI Nucleotide accession', 22),\n",
    "    ('longest target CDS location', 'longest target CDS location', 22),\n",
    "    ('sra_accession', 'NCBI SRA accession', 20),\n",
    "    ('short_read_name', 'read name', 20),\n",
    "    ('anomaly_description', 'anomaly description', 80),\n",
    ")\n",
    "write_xlsx_for_df(None, 'Supplementary Table S8', curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "    \n",
    "\n",
    "curr_df = ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF.merge(pd.DataFrame(non_ref_read_flat_dicts))\n",
    "assert (len(curr_df[list(ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF)].drop_duplicates()) == \n",
    "        len(ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF))\n",
    "curr_df = add_location_str_repr_columns(curr_df)\n",
    "verify_sra_accession_and_short_read_name_matches_read_name(curr_df)\n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "    ('nuccore_accession', 'NCBI Nucleotide accession', 22),\n",
    "    ('longest target CDS location', 'longest target CDS location', 22),\n",
    "    ('variant_ir_pair_repr', 'inverted repeats (presumably) promoting programmed inversions allowing switching from the reference variant to this variant', 50),\n",
    "    ('sra_accession', 'NCBI SRA accession', 20),\n",
    "    ('short_read_name', 'read name', 20),\n",
    ")\n",
    "write_xlsx_for_df(None, 'Supplementary Table S9', curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "    \n",
    "    \n",
    "products_series = pd.concat(\n",
    "    [\n",
    "        repeat_cds_product_fisher_or_g_result_df[repeat_cds_product_fisher_or_g_result_df['corrected_pvalue'] <= search_for_pis_args[\n",
    "            'enrichment_analysis']['max_corrected_pvalue_for_product_to_be_considered_significantly_enriched']]['product'],\n",
    "        adjacent_repeat_cds_product_fisher_or_g_result_df[adjacent_repeat_cds_product_fisher_or_g_result_df['corrected_pvalue'] <= search_for_pis_args[\n",
    "            'enrichment_analysis']['max_corrected_pvalue_for_product_to_be_considered_significantly_enriched']]['product'],\n",
    "    ] + [pd.read_csv(x, sep='\\t', low_memory=False)['product'] for x in all_raw_read_alignment_results_df['filtered_extended_cds_df_csv_file_path'].unique()],\n",
    "    ignore_index=True,\n",
    ").drop_duplicates().sort_values()\n",
    "product_classes_df = products_series.to_frame()\n",
    "product_classes_df = add_product_class_column(product_classes_df, 'product', 'product_class')\n",
    "\n",
    "product_classes_df = product_classes_df[(product_classes_df['product_class'] != 'other')].copy()\n",
    "product_classes_df.sort_values(['product_class', 'product'], inplace=True)\n",
    "\n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "    ('product', 'GenBank CDS product annotation', 60),\n",
    "    ('product_class', 'gene family', max(len(x) for x in product_classes_df['product_class'].unique())),\n",
    ")\n",
    "\n",
    "write_xlsx_for_df(None, 'Supplementary Table S10', product_classes_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "\n",
    "\n",
    "curr_df = all_homologs_extended_df.copy()\n",
    "\n",
    "def get_homolog_ir_pair_str_repr(homolog_df_row):\n",
    "    ir_pair = tuple(homolog_df_row[[\n",
    "        'inverted_repeat_in_homolog_left',\n",
    "        'inverted_repeat_in_homolog_right',\n",
    "        'inverted_repeat_in_margin_left',\n",
    "        'inverted_repeat_in_margin_right',\n",
    "    ]].astype(int))\n",
    "    if ir_pair[2] < ir_pair[1]:\n",
    "        ir_pair = ir_pair[2:] + ir_pair[:2]\n",
    "    assert ir_pair[0] < ir_pair[1] < ir_pair[2] < ir_pair[3]\n",
    "    return massive_screening_configuration.get_ir_pair_str_repr(ir_pair)[1:-1]\n",
    "\n",
    "curr_df['min_evalue_repr'] = curr_df['min_evalue_of_alignment_to_inverted_repeat_in_margin'].apply(\n",
    "    lambda x: '' if np.isposinf(x) else f'{x:.2E}')\n",
    "curr_df['inverted_repeat_repr'] = curr_df.apply(\n",
    "    lambda row: ('no inverted repeats found' if row['min_evalue_repr'] == '' \n",
    "                 else get_homolog_ir_pair_str_repr(row)), \n",
    "    axis=1,\n",
    ")\n",
    "curr_df = add_location_str_repr_columns(curr_df)\n",
    "\n",
    "curr_df.sort_values(['taxon_phylum', 'taxon_species', 'nuccore_accession', 'CDS location'], inplace=True)\n",
    "all_cds_context_names_and_longer_linked_cds_loci_ordered_df_with_curr_column_names = (\n",
    "    ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF.rename(columns={\n",
    "        'nuccore_accession': 'longer_linked_repeat_cds_nuccore_accession',\n",
    "    }))\n",
    "curr_df = all_cds_context_names_and_longer_linked_cds_loci_ordered_df_with_curr_column_names.merge(curr_df)\n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "    ('longer_linked_repeat_cds_nuccore_accession', 'programmed inversion NCBI Nucleotide accession', 22),\n",
    "    ('longest target CDS location', 'programmed inversion longest target CDS location', 22),\n",
    "    ('taxon_phylum', 'homolog CDS phylum', 22),\n",
    "    ('taxon_species', 'homolog CDS species', 22),\n",
    "    ('nuccore_accession', 'homolog CDS NCBI Nucleotide accession', 24),\n",
    "    ('CDS location', 'homolog CDS location', 22),\n",
    "    ('product', 'homolog GenBank CDS product annotation', 60),\n",
    "    ('is_homolog_potentially_modified_by_pi', 'is homolog considered as potentially targeted by programmed inversions', 20),\n",
    "    ('inverted_repeat_repr', 'best inverted repeats between homolog CDS and its margins', 50),\n",
    "    ('min_evalue_repr', 'alignment evalue of best inverted repeats between homolog CDS and its margins', 20),\n",
    ")\n",
    "\n",
    "write_xlsx_for_df(None, 'Supplementary Table S11', curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "\n",
    "shufflon_protein_df = pd.DataFrame(\n",
    "    [\n",
    "        ('WP_001389385.1', 'Salmonella enterica subsp. enterica serovar Typhimurium plasmid R64', 'AP005147.1', '104790-106214'),\n",
    "        ('WP_001389385.1', 'Shigella sonnei plasmid P9', 'NC_002122.1', '77407-78831'),\n",
    "        ('WP_010895887.1', 'Escherichia coli plasmid R721', 'NC_002525.1', '44733-45986'),\n",
    "        ('AAO71709.1', 'Salmonella enterica subsp. enterica serovar Typhi Ty2', 'AE014613.1', '4418334-4419641'),\n",
    "    ],\n",
    "    columns=['NCBI Protein accession', 'Genome description', 'NCBI Nucleotide accession', 'CDS location'], \n",
    ")\n",
    "\n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "    ('NCBI Protein accession', 'NCBI Protein accession', 16),\n",
    "    ('Genome description', 'Genome description', 60),\n",
    "    ('NCBI Nucleotide accession', 'NCBI Nucleotide accession', 16),\n",
    "    ('CDS location', 'CDS location', 19),\n",
    ")\n",
    "\n",
    "write_xlsx_for_df(None, 'Supplementary Table S12', shufflon_protein_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "\n",
    "\n",
    "curr_df = ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF.merge(semi_min_rna_seq_summary_df)\n",
    "curr_df = add_location_str_repr_columns(curr_df)\n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "    ('nuccore_accession', 'NCBI Nucleotide accession', 22),\n",
    "    ('longest target CDS location', 'longest target CDS location', 22),\n",
    "    ('sra_accession', 'NCBI SRA accession', 20),\n",
    "    ('in_silico_inverted_region', 'predicted invertible region', 22),\n",
    "    ('num_of_reads_matching_ref_better', 'number of read pairs matching the reference substantially better than the alternative reference (>=100 difference between bowtie2 alignment scores to reference and alternative reference)', 26),\n",
    "    ('num_of_reads_matching_non_ref_better', 'number of read pairs matching the alternative reference substantially better than the reference (>=100 difference between bowtie2 alignment scores to alternative reference and reference)', 26),\n",
    ")\n",
    "write_xlsx_for_df(None, 'Supplementary Table S13', curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "\n",
    "curr_df = ALL_CDS_CONTEXT_NAMES_AND_LONGER_LINKED_CDS_LOCI_ORDERED_DF.merge(semi_min_all_all_diff_score_alignments_df)\n",
    "curr_df = add_location_str_repr_columns(curr_df)\n",
    "for score_column_name in ('pair_total_align_score_ref', 'pair_total_align_score_non_ref'):\n",
    "    curr_df[score_column_name].replace(-np.inf, 'no concordant alignment identified', inplace=True)\n",
    "    curr_df[score_column_name].replace(0, '0 (sequences match perfectly)', inplace=True)\n",
    "verify_sra_accession_and_short_read_name_matches_read_name(curr_df)\n",
    "ordered_column_name_and_final_name_and_width = (\n",
    "    ('nuccore_accession', 'NCBI Nucleotide accession', 22),\n",
    "    ('longest target CDS location', 'longest target CDS location', 22),\n",
    "    ('sra_accession', 'NCBI SRA accession', 20),\n",
    "    ('in_silico_inverted_region', 'predicted invertible region', 22),\n",
    "    ('short_read_name', 'read name', 20),\n",
    "    ('pair_total_align_score_ref', 'score of concordant alignment to the reference', 35),\n",
    "    ('pair_total_align_score_non_ref', 'score of concordant alignment to the alternative reference', 35),\n",
    "    # pair_total_align_score_ref  pair_total_align_score_non_ref  score_diff  max_score  is_read_matching_ref_better in_silico_inverted_region\n",
    ")\n",
    "write_xlsx_for_df(None, 'Supplementary Table S14', curr_df, ordered_column_name_and_final_name_and_width, writer=excel_writer)\n",
    "\n",
    "\n",
    "excel_writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c693a1-cdd7-44a8-bcf4-d4663536f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_ref_rhamnosus_reads = all_raw_read_alignment_results_df[\n",
    "    (all_raw_read_alignment_results_df['nuccore_accession'] == 'NC_013198.1') & \n",
    "    (all_raw_read_alignment_results_df['cds_context_name'] == 'MTase: BREX type 1') & \n",
    "    (all_raw_read_alignment_results_df['read_evidence_type'] == 'ref_variant')\n",
    "]['sseqid'].nunique()\n",
    "total_num_of_rhamnosus_reads = 176\n",
    "print(f'num_of_ref_rhamnosus_reads: {num_of_ref_rhamnosus_reads}')\n",
    "\n",
    "rhamnosus_alignment_regions_raw_read_alignment_args = search_for_pis_args['stage6'][\n",
    "    'cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args']['MTase: BREX type 1']['NC_013198.1'][0]\n",
    "sra_accession_to_variants_and_reads_info = rhamnosus_alignment_regions_raw_read_alignment_args['sra_accession_to_variants_and_reads_info']\n",
    "assert len(sra_accession_to_variants_and_reads_info) == 1\n",
    "reads_info = next(iter(sra_accession_to_variants_and_reads_info.values()))\n",
    "non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref = reads_info[\n",
    "    'non_ref_variant_read_name_to_possible_ir_pairs_used_to_reach_from_ref']\n",
    "non_ref_variant_ir_pairs_to_num_of_evidence_reads = get_variant_ir_pairs_to_num_of_evidence_reads(\n",
    "    non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref)\n",
    "assert sum(non_ref_variant_ir_pairs_to_num_of_evidence_reads.values()) + num_of_ref_rhamnosus_reads == total_num_of_rhamnosus_reads\n",
    "\n",
    "all_rhamnosus_ir_pairs = set()\n",
    "for ir_pairs in non_ref_variant_ir_pairs_to_num_of_evidence_reads:\n",
    "    all_rhamnosus_ir_pairs |= set(ir_pairs)\n",
    "min_left2 = min(x[2] for x in all_rhamnosus_ir_pairs)\n",
    "max_right2 = max(x[3] for x in all_rhamnosus_ir_pairs)\n",
    "(rhamnosus_pglx_start, rhamnosus_pglx_end), rhamnosus_pglx_protein_id = rhamnosus_alignment_regions_raw_read_alignment_args[\n",
    "    'longest_linked_repeat_cds_region_and_protein_id']\n",
    "long_region_upstream_to_all_repeats_len = rhamnosus_pglx_end - max_right2\n",
    "long_region_downstream_to_all_repeats_len = min_left2 - rhamnosus_pglx_start\n",
    "rhamnosus_pglx_gene_len = rhamnosus_pglx_end - rhamnosus_pglx_start + 1\n",
    "assert (rhamnosus_pglx_gene_len % 3) == 0\n",
    "rhamnosus_pglx_len = rhamnosus_pglx_gene_len // 3 - 1 # don't count stop codon\n",
    "rhamnosus_n_terminus_len = long_region_upstream_to_all_repeats_len // 3\n",
    "rhamnosus_c_terminus_len = long_region_downstream_to_all_repeats_len // 3 - 1 # don't count stop codon\n",
    "rhamnosus_var_part_len = rhamnosus_pglx_len - rhamnosus_n_terminus_len - rhamnosus_c_terminus_len\n",
    "rhamnosus_var_part_start = rhamnosus_n_terminus_len + 1\n",
    "rhamnosus_var_part_end = rhamnosus_var_part_start + rhamnosus_var_part_len - 1\n",
    "rhamnosus_c_terminus_start = rhamnosus_var_part_end + 1\n",
    "\n",
    "print(f'1-{rhamnosus_n_terminus_len}')\n",
    "print(f'{rhamnosus_var_part_start}-{rhamnosus_var_part_end}')\n",
    "print(f'{rhamnosus_c_terminus_start}-{rhamnosus_pglx_len}')\n",
    "\n",
    "casei_alignment_regions_raw_read_alignment_args = search_for_pis_args['stage6'][\n",
    "        'cds_context_name_to_nuccore_accession_to_alignment_regions_raw_read_alignment_args'][\n",
    "    'MTase: BREX type 1, downstream extra short PglX']['NZ_CP068173.1'][0]\n",
    "casei_sra_accession_to_variants_and_reads_info = casei_alignment_regions_raw_read_alignment_args['sra_accession_to_variants_and_reads_info']\n",
    "assert len(casei_sra_accession_to_variants_and_reads_info) == 1\n",
    "casei_reads_info = next(iter(casei_sra_accession_to_variants_and_reads_info.values()))\n",
    "casei_non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref = casei_reads_info[\n",
    "    'non_ref_variant_read_name_to_possible_ir_pairs_used_to_reach_from_ref']\n",
    "casei_non_ref_variant_ir_pairs_to_num_of_evidence_reads = get_variant_ir_pairs_to_num_of_evidence_reads(\n",
    "    casei_non_ref_read_name_to_possible_ir_pairs_used_to_reach_from_ref)\n",
    "all_casei_ir_pairs = set()\n",
    "for ir_pairs in casei_non_ref_variant_ir_pairs_to_num_of_evidence_reads:\n",
    "    all_casei_ir_pairs |= set(ir_pairs)\n",
    "casei_max_right2 = max(x[3] for x in all_casei_ir_pairs)\n",
    "(casei_pglx_start, casei_pglx_end), casei_pglx_protein_id = casei_alignment_regions_raw_read_alignment_args[\n",
    "    'longest_linked_repeat_cds_region_and_protein_id']\n",
    "casei_region_upstream_to_all_repeats_len = casei_pglx_end - casei_max_right2\n",
    "casei_n_terminus_len = casei_region_upstream_to_all_repeats_len // 3\n",
    "casei_pglx_gene_len = casei_pglx_end - casei_pglx_start + 1\n",
    "assert (casei_pglx_gene_len % 3) == 0\n",
    "casei_pglx_len = casei_pglx_gene_len // 3 - 1 # don't count stop codon\n",
    "casei_var_part_len = casei_pglx_len - casei_n_terminus_len\n",
    "casei_var_part_start = casei_n_terminus_len + 1\n",
    "casei_downstream_pglx_gene_len = 948\n",
    "assert (casei_downstream_pglx_gene_len % 3) == 0\n",
    "casei_downstream_pglx_len = casei_downstream_pglx_gene_len // 3\n",
    "\n",
    "# online blastp rhamnosus long PglX (WP_015765014.1) to casei long PglX (WP_201670672.1)\n",
    "main_casei_alignment_start_in_rhamnosus = 1\n",
    "main_casei_alignment_start_in_casei = 1\n",
    "main_casei_alignment_end_in_rhamnosus = 878\n",
    "main_casei_alignment_end_in_casei = 859\n",
    "\n",
    "# online blastp rhamnosus long PglX (WP_015765014.1) to casei downstream PglX (WP_236586838.1)\n",
    "downstream_casei_alignment_start_in_rhamnosus = 885\n",
    "downstream_casei_alignment_start_in_casei = 3\n",
    "downstream_casei_alignment_end_in_rhamnosus = 1172\n",
    "downstream_casei_alignment_end_in_casei = 299\n",
    "downstream_casei_protein_id = 'WP_236586838.1'\n",
    "\n",
    "rhamnosus_pglx_start = 74\n",
    "rhamnosus_top = 0.8\n",
    "protein_height = 0.1\n",
    "dist_between_rhamnosus_and_casei = 0.3\n",
    "dist_between_casei_pglx_proteins = 150\n",
    "show_n_and_c_for_each_protein = True\n",
    "show_protein_ids = True\n",
    "y_dist_between_protein_and_id = 0.02\n",
    "species_text_start = -80\n",
    "\n",
    "if 1:\n",
    "    plt.close('all')\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    top=1,\n",
    "    bottom=0.025,\n",
    "    left=0.27,\n",
    "    right=0.99,\n",
    "    # hspace=0.2,\n",
    "    # wspace=0.2\n",
    ")\n",
    "\n",
    "remove_all_ax_spines_an_x_and_y_axes(ax)\n",
    "\n",
    "\n",
    "rhamnosus_bottom = rhamnosus_top - protein_height\n",
    "rhamnosus_middle_y = np.mean((rhamnosus_bottom, rhamnosus_top))\n",
    "ax.add_patch(matplotlib.patches.Rectangle(\n",
    "    (rhamnosus_pglx_start, rhamnosus_bottom), rhamnosus_pglx_len, protein_height,\n",
    "    fill=False, \n",
    "))\n",
    "if show_n_and_c_for_each_protein:\n",
    "    ax.text(rhamnosus_pglx_start, rhamnosus_middle_y, 'N-', va='center', ha='right')\n",
    "    ax.text(rhamnosus_pglx_start + rhamnosus_pglx_len, rhamnosus_middle_y, '-C', va='center', ha='left')\n",
    "if show_protein_ids:\n",
    "    ax.text(np.mean((rhamnosus_pglx_start, rhamnosus_pglx_start + rhamnosus_pglx_len)), \n",
    "            rhamnosus_top + y_dist_between_protein_and_id, rhamnosus_pglx_protein_id, va='bottom', ha='center')\n",
    "ax.add_patch(matplotlib.patches.Rectangle(\n",
    "    (rhamnosus_pglx_start + rhamnosus_var_part_start, rhamnosus_bottom), rhamnosus_var_part_len, protein_height,\n",
    "    fill=False, \n",
    "    hatch='.....',\n",
    "    linewidth=0, \n",
    "))\n",
    "\n",
    "casei_top = rhamnosus_bottom - dist_between_rhamnosus_and_casei\n",
    "casei_bottom = casei_top - protein_height\n",
    "casei_downstream_pglx_start = casei_pglx_len + dist_between_casei_pglx_proteins\n",
    "casei_downstream_pglx_end = casei_downstream_pglx_start + casei_downstream_pglx_len\n",
    "casei_middle_y = np.mean((casei_bottom, casei_top))\n",
    "\n",
    "ax.add_patch(matplotlib.patches.Rectangle(\n",
    "    (1, casei_bottom), casei_pglx_len, protein_height,\n",
    "    fill=False, \n",
    "))\n",
    "var_part_handle = ax.add_patch(matplotlib.patches.Rectangle(\n",
    "    (casei_var_part_start, casei_bottom), casei_var_part_len, protein_height,\n",
    "    fill=False, \n",
    "    hatch='.....',\n",
    "    linewidth=0, \n",
    "))\n",
    "if show_n_and_c_for_each_protein:\n",
    "    ax.text(1, casei_middle_y, 'N-', va='center', ha='right')\n",
    "    ax.text(1 + casei_pglx_len, casei_middle_y, '-C', va='center', ha='left')\n",
    "if show_protein_ids:\n",
    "    ax.text(np.mean((1, 1 + casei_pglx_len)), \n",
    "            casei_bottom - y_dist_between_protein_and_id, casei_pglx_protein_id, va='top', ha='center')\n",
    "ax.add_patch(matplotlib.patches.Rectangle(\n",
    "    (casei_downstream_pglx_start, casei_bottom), casei_downstream_pglx_len, protein_height,\n",
    "    fill=False, \n",
    "))\n",
    "if show_n_and_c_for_each_protein:\n",
    "    ax.text(casei_downstream_pglx_start, casei_middle_y, 'N-', va='center', ha='right')\n",
    "    ax.text(casei_downstream_pglx_start + casei_downstream_pglx_len, casei_middle_y, '-C', va='center', ha='left')\n",
    "if show_protein_ids:\n",
    "    ax.text(np.mean((casei_downstream_pglx_start, casei_downstream_pglx_start + casei_downstream_pglx_len)), \n",
    "            casei_bottom - y_dist_between_protein_and_id, downstream_casei_protein_id, va='top', ha='center')\n",
    "    \n",
    "\n",
    "\n",
    "ax.add_patch(matplotlib.patches.Polygon([\n",
    "    (rhamnosus_pglx_start + main_casei_alignment_start_in_rhamnosus, rhamnosus_bottom), \n",
    "    (rhamnosus_pglx_start + main_casei_alignment_end_in_rhamnosus, rhamnosus_bottom), \n",
    "    (main_casei_alignment_end_in_casei, casei_top), \n",
    "    (main_casei_alignment_start_in_casei, casei_top), \n",
    "], closed=True, fill=True, facecolor=ALIGNMENT_PROJECTION_COLOR, alpha=ALIGNMENT_PROJECTION_ALPHA, edgecolor=ALIGNMENT_PROJECTION_EDGE_COLOR,\n",
    "    linewidth=ALIGNMENT_PROJECTION_LINE_WIDTH))\n",
    "\n",
    "ax.add_patch(matplotlib.patches.Polygon([\n",
    "    (rhamnosus_pglx_start + downstream_casei_alignment_start_in_rhamnosus, rhamnosus_bottom), \n",
    "    (rhamnosus_pglx_start + downstream_casei_alignment_end_in_rhamnosus, rhamnosus_bottom), \n",
    "    (downstream_casei_alignment_end_in_casei + casei_downstream_pglx_start, casei_top), \n",
    "    (downstream_casei_alignment_start_in_casei + casei_downstream_pglx_start, casei_top), \n",
    "], closed=True, fill=True, facecolor=ALIGNMENT_PROJECTION_COLOR, alpha=ALIGNMENT_PROJECTION_ALPHA, edgecolor=ALIGNMENT_PROJECTION_EDGE_COLOR,\n",
    "    linewidth=ALIGNMENT_PROJECTION_LINE_WIDTH))\n",
    "\n",
    "\n",
    "ax.set_xlim(-100, casei_downstream_pglx_end + 100)\n",
    "ax.text(species_text_start, rhamnosus_middle_y, rhamnosus_alignment_regions_raw_read_alignment_args['locus_definition'], va='center', ha='right')\n",
    "ax.text(species_text_start, casei_middle_y, casei_alignment_regions_raw_read_alignment_args['locus_definition'], va='center', ha='right')\n",
    "\n",
    "ax.legend(\n",
    "    handles=[var_part_handle],\n",
    "    labels=['variable region'],\n",
    "    # fontsize='small', \n",
    "    bbox_to_anchor=(\n",
    "        0.977, \n",
    "        0.05,\n",
    "    ),\n",
    "    handleheight=2, handlelength=4,\n",
    "    loc='lower right',\n",
    ")\n",
    "if 1:\n",
    "    fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s33.png'))\n",
    "    # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s33.pdf'))\n",
    "\n",
    "\n",
    "total_num_of_even_non_ref_reads = sum(\n",
    "    num_of_evidence_reads for non_ref_variant_ir_pairs, num_of_evidence_reads in non_ref_variant_ir_pairs_to_num_of_evidence_reads.items()\n",
    "    if (len(non_ref_variant_ir_pairs) % 2) == 0\n",
    ")\n",
    "total_num_of_odd_non_ref_reads = sum(\n",
    "    num_of_evidence_reads for non_ref_variant_ir_pairs, num_of_evidence_reads in non_ref_variant_ir_pairs_to_num_of_evidence_reads.items()\n",
    "    if (len(non_ref_variant_ir_pairs) % 2) == 1\n",
    ")\n",
    "total_num_of_non_ref_reads = total_num_of_even_non_ref_reads + total_num_of_odd_non_ref_reads\n",
    "assert total_num_of_non_ref_reads + num_of_ref_rhamnosus_reads == total_num_of_rhamnosus_reads\n",
    "\n",
    "print(f'all_rhamnosus_ir_pairs: {all_rhamnosus_ir_pairs}')\n",
    "print(f'num_of_ref_rhamnosus_reads: {num_of_ref_rhamnosus_reads}')\n",
    "print(f'total_num_of_non_ref_reads: {total_num_of_non_ref_reads}')\n",
    "print(f'total_num_of_even_non_ref_reads: {total_num_of_even_non_ref_reads}')\n",
    "print(f'total_num_of_odd_non_ref_reads: {total_num_of_odd_non_ref_reads}')\n",
    "print(f'long_region_upstream_to_all_repeats_len: {long_region_upstream_to_all_repeats_len}')\n",
    "print(f'long_region_downstream_to_all_repeats_len: {long_region_downstream_to_all_repeats_len}')\n",
    "\n",
    "binom_test_result_for_all_reads = scipy.stats.binom_test(\n",
    "    x=total_num_of_odd_non_ref_reads, \n",
    "    n=total_num_of_rhamnosus_reads, \n",
    "    p=0.5, alternative='two-sided',\n",
    ")\n",
    "binom_test_result_for_non_ref_reads = scipy.stats.binom_test(\n",
    "    x=total_num_of_odd_non_ref_reads, \n",
    "    n=total_num_of_non_ref_reads, \n",
    "    p=0.5, alternative='two-sided',\n",
    ")\n",
    "print(f'binom_test_result_for_all_reads: {binom_test_result_for_all_reads}')\n",
    "print(f'binom_test_result_for_non_ref_reads: {binom_test_result_for_non_ref_reads}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e057c94-c4aa-4c9c-9c67-39c436cadab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    plt.close('all')\n",
    "\n",
    "curr_df = pairs_linked_with_evidence_for_repeat_overlapping_mobile_element_df.copy()\n",
    "column_name = 'repeat_estimated_copy_num'\n",
    "\n",
    "curr_df = curr_df[~(curr_df[column_name].isna())]\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "orig_sample_size = len(curr_df)\n",
    "\n",
    "\n",
    "min_val = curr_df[column_name].min()\n",
    "max_val = curr_df[column_name].max()\n",
    "print(f'max_val: {max_val}')\n",
    "\n",
    "\n",
    "real_bins = list(range(31))\n",
    "first_real_bin_start = real_bins[0]\n",
    "last_real_bin_end = real_bins[-1]\n",
    "bin_size = 1\n",
    "fake_last_bin_end = last_real_bin_end + bin_size\n",
    "fake_last_bin_middle = fake_last_bin_end - bin_size / 2\n",
    "\n",
    "# print(curr_df[column_name].describe())\n",
    "\n",
    "curr_df.loc[curr_df[column_name] > last_real_bin_end, column_name] = fake_last_bin_middle\n",
    "\n",
    "bins = real_bins + [fake_last_bin_end]\n",
    "assert curr_df[column_name].min() >= bins[0]\n",
    "assert curr_df[column_name].max() <= bins[-1]\n",
    "assert ((curr_df[column_name] <= bins[-1]) & (curr_df[column_name] >= bins[0])).sum() == orig_sample_size\n",
    "\n",
    "hist_color = (*matplotlib.colors.to_rgb('grey'), 1)\n",
    "hist_color = 'grey'\n",
    "hist_color = (0.7,0.7,0.7)\n",
    "hist_color = GREY_HIST_COLOR\n",
    "\n",
    "ax.hist(\n",
    "    curr_df[column_name],\n",
    "    bins=bins,\n",
    "    # density=True,\n",
    "    # histtype='step', \n",
    "    # linewidth=8,\n",
    "    color=hist_color,\n",
    "    edgecolor=hist_color,\n",
    ")\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xlabel('repeat estimated copy number outside CDS pair locus')\n",
    "ax.set_xlabel('repeat estimated copy number outside inverted repeat pair locus')\n",
    "# ax.set_ylabel('# CDS pairs')\n",
    "ax.set_ylabel('# coding sequence pairs')\n",
    "# orig_xlim = ax.get_xlim()\n",
    "ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "add_fake_bin_tick_labels_and_dashed_lines_to_histogram(\n",
    "    ax, df=curr_df, column_name=column_name, bin_size=bin_size, \n",
    "    last_real_bin_end=last_real_bin_end, \n",
    "    first_real_bin_start=first_real_bin_start,\n",
    ")\n",
    "\n",
    "ylim = ax.get_ylim()\n",
    "ax.plot([search_for_pis_args['stage3']['min_max_estimated_copy_num_to_classify_as_mobile_element']] * 2, ylim,\n",
    "        linestyle='--',color='red',\n",
    "        # label=f'x=0',\n",
    "        alpha=THRESHOLD_DASHED_LINE_ALPHA)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "if 1:\n",
    "    fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s1.png'))\n",
    "    # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s1.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edca485-3398-43a6-961e-d29ade543b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    plt.close('all')\n",
    "\n",
    "curr_df = lens_of_spanning_regions_ratios_df.copy()\n",
    "\n",
    "curr_df['lens_of_spanning_regions_ratio_and_1'] = (curr_df['lens_of_spanning_regions_ratio'] - 1).abs()\n",
    "column_name = 'lens_of_spanning_regions_ratio_and_1'\n",
    "\n",
    "assert not curr_df[column_name].isna().any()\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "orig_sample_size = len(curr_df)\n",
    "\n",
    "min_val = curr_df[column_name].min()\n",
    "max_val = curr_df[column_name].max()\n",
    "\n",
    "bin_size = 0.01\n",
    "first_real_bin_start = 0\n",
    "last_real_bin_end = 0.5\n",
    "fake_last_bin_end = last_real_bin_end + bin_size\n",
    "fake_last_bin_middle = fake_last_bin_end - bin_size / 2\n",
    "real_bins = list(np.arange(first_real_bin_start, last_real_bin_end + bin_size, bin_size))\n",
    "\n",
    "# print(curr_df[column_name].describe())\n",
    "\n",
    "curr_df.loc[curr_df[column_name] > last_real_bin_end, column_name] = fake_last_bin_middle\n",
    "\n",
    "bins = real_bins + [fake_last_bin_end]\n",
    "assert curr_df[column_name].min() >= bins[0]\n",
    "assert curr_df[column_name].max() <= bins[-1]\n",
    "assert ((curr_df[column_name] <= bins[-1]) & (curr_df[column_name] >= bins[0])).sum() == orig_sample_size\n",
    "\n",
    "hist_color = GREY_HIST_COLOR\n",
    "\n",
    "ax.hist(\n",
    "    curr_df[column_name],\n",
    "    bins=bins,\n",
    "    # density=True,\n",
    "    # histtype='step', \n",
    "    # linewidth=8,\n",
    "    color=hist_color,\n",
    "    edgecolor=hist_color,\n",
    ")\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('relative locus length difference')\n",
    "ax.set_ylabel(\n",
    "    # '# collinear left and right margin alignment pairs',\n",
    "    # '# collinear alignment pairs',\n",
    "    '# similar length locus candidates',\n",
    ")\n",
    "# orig_xlim = ax.get_xlim()\n",
    "ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "add_fake_bin_tick_labels_and_dashed_lines_to_histogram(\n",
    "    ax, df=curr_df, column_name=column_name, bin_size=bin_size, \n",
    "    last_real_bin_end=last_real_bin_end, \n",
    "    first_real_bin_start=first_real_bin_start,\n",
    ")\n",
    "\n",
    "ylim = ax.get_ylim()\n",
    "ax.plot([search_for_pis_args['stage5']['blast_margins_and_identify_regions_in_other_nuccores'][\n",
    "    'max_dist_between_lens_of_spanning_regions_ratio_and_1']] * 2, ylim,\n",
    "        linestyle='--',color='red',\n",
    "        # label=f'x=0',\n",
    "        alpha=THRESHOLD_DASHED_LINE_ALPHA)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "\n",
    "if 1:\n",
    "    fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s2.png'))\n",
    "    # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s2.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401405c-f951-422a-b7ea-f1f8de644ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    plt.close('all')\n",
    "\n",
    "curr_df = region_in_other_nuccore_df.copy()\n",
    "\n",
    "column_name = 'mauve_total_match_proportion'\n",
    "\n",
    "assert not curr_df[column_name].isna().any()\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "bin_size = 0.01\n",
    "bins = list(np.arange(0, 1 + bin_size, bin_size))\n",
    "assert bins[-1] == 1\n",
    "\n",
    "# print(curr_df[column_name].describe())\n",
    "\n",
    "hist_color = GREY_HIST_COLOR\n",
    "\n",
    "ax.hist(\n",
    "    curr_df[column_name],\n",
    "    bins=bins,\n",
    "    # density=True,\n",
    "    # histtype='step', \n",
    "    # linewidth=8,\n",
    "    color=hist_color,\n",
    "    edgecolor=hist_color,\n",
    ")\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('alignment match proportion')\n",
    "ax.set_ylabel('# homologous locus candidates')\n",
    "# orig_xlim = ax.get_xlim()\n",
    "ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "\n",
    "ylim = ax.get_ylim()\n",
    "ax.plot([search_for_pis_args['stage5']['min_mauve_total_match_proportion']] * 2, ylim,\n",
    "        linestyle='--',color='red',\n",
    "        # label=f'x=0',\n",
    "        alpha=THRESHOLD_DASHED_LINE_ALPHA)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%g'))\n",
    "\n",
    "if 1:\n",
    "    fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s3.png'))\n",
    "    # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s3.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe5c0dd-47fe-46e9-9f56-e8a77bf4ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    plt.close('all')\n",
    "\n",
    "curr_df = region_in_other_nuccore_df.copy()\n",
    "curr_df = curr_df[curr_df['mauve_total_match_proportion'] >= search_for_pis_args['stage5']['min_mauve_total_match_proportion']]\n",
    "\n",
    "column_name = 'min_sub_alignment_min_match_proportion'\n",
    "\n",
    "assert not curr_df[column_name].isna().any()\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "bin_size = 0.01\n",
    "bins = list(np.arange(0, 1 + bin_size, bin_size))\n",
    "assert bins[-1] == 1\n",
    "\n",
    "# print(curr_df[column_name].describe())\n",
    "\n",
    "hist_color = GREY_HIST_COLOR\n",
    "\n",
    "ax.hist(\n",
    "    curr_df[column_name],\n",
    "    bins=bins,\n",
    "    # density=True,\n",
    "    # histtype='step', \n",
    "    # linewidth=8,\n",
    "    color=hist_color,\n",
    "    edgecolor=hist_color,\n",
    ")\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('lowest sub-alignment match proportion')\n",
    "ax.set_ylabel('# homologous locus candidates')\n",
    "# orig_xlim = ax.get_xlim()\n",
    "ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "\n",
    "ylim = ax.get_ylim()\n",
    "ax.plot([search_for_pis_args['stage5']['min_min_sub_alignment_min_match_proportion']] * 2, ylim,\n",
    "        linestyle='--',color='red',\n",
    "        # label=f'x=0',\n",
    "        alpha=THRESHOLD_DASHED_LINE_ALPHA)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%g'))\n",
    "\n",
    "if 1:\n",
    "    fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s4.png'))\n",
    "    # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s4.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a96a3-32c1-4407-af42-a30c8eec5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_df = minimal_ir_pairs_linked_to_breakpoint_pairs_df.copy()\n",
    "\n",
    "curr_df = curr_df.sort_values(by='max_dist_between_potential_breakpoint_containing_interval_and_repeat', ascending=True).drop_duplicates(\n",
    "    [\n",
    "        'nuccore_accession',\n",
    "        'index_in_nuccore_ir_pairs_df_csv_file',\n",
    "    ],\n",
    "    keep='first'\n",
    ").rename(columns={\n",
    "    'max_dist_between_potential_breakpoint_containing_interval_and_repeat': 'min_max_dist_between_potential_breakpoint_containing_interval_and_repeat'})\n",
    "column_name = 'min_max_dist_between_potential_breakpoint_containing_interval_and_repeat'\n",
    "# print(curr_df[column_name].describe())\n",
    "\n",
    "curr_df.loc[curr_df['min_max_dist_between_potential_breakpoint_containing_interval_and_repeat'] < 0, \n",
    "            'min_max_dist_between_potential_breakpoint_containing_interval_and_repeat'] = 0\n",
    "\n",
    "assert not curr_df[column_name].isna().any()\n",
    "orig_sample_size = len(curr_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "\n",
    "bin_size = 2\n",
    "# bin_size = 5\n",
    "real_bins = list(range(\n",
    "    0, \n",
    "    # 401, \n",
    "    201, \n",
    "    # 141, \n",
    "    bin_size,\n",
    "))\n",
    "first_real_bin_start = real_bins[0]\n",
    "last_real_bin_end = real_bins[-1]\n",
    "fake_last_bin_end = last_real_bin_end + bin_size\n",
    "fake_last_bin_middle = fake_last_bin_end - bin_size / 2\n",
    "\n",
    "\n",
    "curr_df.loc[curr_df[column_name] > last_real_bin_end, column_name] = fake_last_bin_middle\n",
    "\n",
    "bins = real_bins + [fake_last_bin_end]\n",
    "assert curr_df[column_name].min() >= bins[0]\n",
    "assert curr_df[column_name].max() <= bins[-1]\n",
    "assert ((curr_df[column_name] <= bins[-1]) & (curr_df[column_name] >= bins[0])).sum() == orig_sample_size\n",
    "\n",
    "\n",
    "# print(curr_df[column_name].describe())\n",
    "\n",
    "# hist_color = (0.7,0.7,0.7)\n",
    "hist_color = GREY_HIST_COLOR\n",
    "\n",
    "ax.hist(\n",
    "    curr_df[column_name],\n",
    "    bins=bins,\n",
    "    # density=True,\n",
    "    # histtype='step', \n",
    "    # linewidth=8,\n",
    "    color=hist_color,\n",
    "    edgecolor=hist_color,\n",
    ")\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xlabel('longer distance between repeat and closest breakpoint-containing region')\n",
    "# ax.set_xlabel('longer distance between repeat and closest breakpoint')\n",
    "# ax.set_xlabel('distance between further repeat and closest breakpoint')\n",
    "ax.set_xlabel('higher distance between repeat and closest breakpoint')\n",
    "ax.set_ylabel('# inverted repeat pairs')\n",
    "# orig_xlim = ax.get_xlim()\n",
    "ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "add_fake_bin_tick_labels_and_dashed_lines_to_histogram(\n",
    "    ax, df=curr_df, column_name=column_name, bin_size=bin_size, \n",
    "    last_real_bin_end=last_real_bin_end, \n",
    "    first_real_bin_start=first_real_bin_start,\n",
    ")\n",
    "\n",
    "\n",
    "ylim = ax.get_ylim()\n",
    "ax.plot([search_for_pis_args['stage5']['max_max_dist_between_potential_breakpoint_containing_interval_and_repeat']] * 2, ylim,\n",
    "        linestyle='--',color='red',\n",
    "        # label=f'x=0',\n",
    "        alpha=THRESHOLD_DASHED_LINE_ALPHA)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "\n",
    "if 1:\n",
    "    fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s5.png'))\n",
    "    # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s5.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b70177-48da-43cc-8bff-ab6cb61e58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_curve_for_each_sim = True\n",
    "plot_roc_curve_for_each_sim = False\n",
    "\n",
    "plot_mean_roc_curve = True\n",
    "# plot_mean_roc_curve = False\n",
    "\n",
    "curr_df = unified_roc_curve_df.copy()\n",
    "\n",
    "curr_df['tpr'] = 1 - curr_df['fnr']\n",
    "\n",
    "\n",
    "all_fprs_sorted = sorted(set(curr_df['fpr']))\n",
    "\n",
    "sim_all_fprs_and_interp_tprs_dfs = []\n",
    "for sim_i, sim_roc_curve_df in curr_df.groupby('sim_i'):\n",
    "    interp_func = scipy.interpolate.interp1d(sim_roc_curve_df['fpr'], sim_roc_curve_df['tpr'], kind='linear')\n",
    "    sim_all_fprs_and_interp_tprs_dfs.append(pd.DataFrame({'fpr': all_fprs_sorted, 'interp_tpr': interp_func(all_fprs_sorted), 'sim_i': sim_i}))\n",
    "\n",
    "all_fprs_and_interp_tprs_df = pd.concat(sim_all_fprs_and_interp_tprs_dfs, ignore_index=True)\n",
    "\n",
    "# fpr_and_tpr_stat_df = all_fprs_and_interp_tprs_df.groupby('fpr', as_index=False, sort=False).agg({'interp_tpr': np.mean, 'interp_tpr': np.std})\n",
    "\n",
    "interp_tpr_grouped_by_fpr = all_fprs_and_interp_tprs_df.groupby('fpr')['interp_tpr']\n",
    "fpr_and_tpr_stat_df = interp_tpr_grouped_by_fpr.mean().reset_index(name='interp_tpr_mean').merge(\n",
    "    interp_tpr_grouped_by_fpr.std().reset_index(name='interp_tpr_std')).merge(\n",
    "    interp_tpr_grouped_by_fpr.size().reset_index(name='num_of_interp_tprs'))\n",
    "\n",
    "num_of_sims = curr_df['sim_i'].nunique()\n",
    "print(f'num_of_sims: {num_of_sims}')\n",
    "assert next(iter(set(fpr_and_tpr_stat_df['num_of_interp_tprs']))) == num_of_sims\n",
    "    \n",
    "chosen_threshold = search_for_pis_args['enrichment_analysis']['min_predicted_rearrangement_probability']\n",
    "\n",
    "if 1:\n",
    "    plt.close('all')\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "aucs = []\n",
    "for sim_i, sim_roc_curve_df in curr_df.groupby('sim_i'):\n",
    "    sim_curr_df = sim_roc_curve_df.sort_values(by='fpr')\n",
    "    xs = sim_curr_df['fpr']\n",
    "    ys = sim_curr_df['tpr']\n",
    "    if plot_roc_curve_for_each_sim:\n",
    "        ax.plot(xs, ys)\n",
    "    auc = scipy.integrate.trapz(ys, xs)\n",
    "    aucs.append(auc)\n",
    "    # print(f'auc: {auc}')\n",
    "\n",
    "if plot_mean_roc_curve:\n",
    "    ax.plot(\n",
    "        fpr_and_tpr_stat_df['fpr'], \n",
    "        fpr_and_tpr_stat_df['interp_tpr_mean'],\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        fpr_and_tpr_stat_df['fpr'], \n",
    "        fpr_and_tpr_stat_df['interp_tpr_mean'] - fpr_and_tpr_stat_df['interp_tpr_std'],\n",
    "        fpr_and_tpr_stat_df['interp_tpr_mean'] + fpr_and_tpr_stat_df['interp_tpr_std'],\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    \n",
    "    \n",
    "chosen_threshold_sim_results_df = curr_df[curr_df['new_threshold'] == chosen_threshold]\n",
    "chosen_threshold_fpr_mean = chosen_threshold_sim_results_df['fpr'].mean()\n",
    "chosen_threshold_tpr_mean = chosen_threshold_sim_results_df['tpr'].mean()\n",
    "chosen_threshold_fpr_std = chosen_threshold_sim_results_df['fpr'].std()\n",
    "chosen_threshold_tpr_std = chosen_threshold_sim_results_df['tpr'].std()\n",
    "ax.errorbar(\n",
    "    x=chosen_threshold_fpr_mean, \n",
    "    y=chosen_threshold_tpr_mean, \n",
    "    xerr=chosen_threshold_fpr_std, \n",
    "    yerr=chosen_threshold_tpr_std, \n",
    "    zorder=100, color='red',\n",
    "    capsize=2.5,\n",
    ")\n",
    "print(f'chosen_threshold_fpr_mean: {chosen_threshold_fpr_mean}')\n",
    "print(f'chosen_threshold_tpr_mean: {chosen_threshold_tpr_mean}')\n",
    "\n",
    "ax.plot([0,1], [0,1], color='grey', linestyle='--', alpha=0.4)\n",
    "ax.set_xlabel('false positive rate')\n",
    "ax.set_ylabel('true positive rate')\n",
    "ax.set_xlim(-0.005, 1.005)\n",
    "ax.set_ylim(-0.005, 1.005)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%g'))\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%g'))\n",
    "\n",
    "auc_mean = np.mean(aucs)\n",
    "auc_std = np.std(aucs)\n",
    "print(f'auc_mean: {auc_mean}')\n",
    "print(f'auc_std: {auc_std}')\n",
    "\n",
    "if 1:\n",
    "    fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s8.png'))\n",
    "    # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s8.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a398c56-075b-47b8-9089-9911a5982326",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    plt.close('all')\n",
    "\n",
    "proportions_as_series = pd.Series(all_all_repeat_cds_covered_bases_proportions)\n",
    "\n",
    "assert not proportions_as_series.isna().any()\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "bin_size = 0.02\n",
    "bins = list(np.arange(0, 1 + bin_size, bin_size))\n",
    "assert bins[-1] == 1\n",
    "\n",
    "# print(proportions_as_series.describe())\n",
    "\n",
    "hist_color = GREY_HIST_COLOR\n",
    "\n",
    "ax.hist(\n",
    "    proportions_as_series,\n",
    "    bins=bins,\n",
    "    # density=True,\n",
    "    # histtype='step', \n",
    "    # linewidth=8,\n",
    "    color=hist_color,\n",
    "    edgecolor=hist_color,\n",
    ")\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xlabel('proportion of target coding sequence covered by alignment')\n",
    "ax.set_xlabel('fraction of target coding sequence covered by alignment')\n",
    "ax.set_ylabel('# homologous sequences')\n",
    "# orig_xlim = ax.get_xlim()\n",
    "ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "\n",
    "ylim = ax.get_ylim()\n",
    "ax.plot([search_for_pis_args['stage6']['blast_longer_linked_repeat_cds_to_each_taxon_genome']['min_repeat_cds_covered_bases_proportion']] * 2, ylim,\n",
    "        linestyle='--',color='red',\n",
    "        # label=f'x=0',\n",
    "        alpha=THRESHOLD_DASHED_LINE_ALPHA)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "# fig.set_size_inches(12, 6, forward=True)\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%g'))\n",
    "\n",
    "if 1:\n",
    "    fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s30.png'))\n",
    "    # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s30.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4da5b7-6ff2-415f-bc4b-329c4ec5bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    plt.close('all')\n",
    "\n",
    "proportions_as_series = pd.Series(all_all_alignment_bases_covered_by_cds_proportions)\n",
    "\n",
    "assert not proportions_as_series.isna().any()\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "bin_size = 0.02\n",
    "bins = list(np.arange(0, 1 + bin_size, bin_size))\n",
    "assert bins[-1] == 1\n",
    "\n",
    "# print(proportions_as_series.describe())\n",
    "\n",
    "hist_color = GREY_HIST_COLOR\n",
    "\n",
    "ax.hist(\n",
    "    proportions_as_series,\n",
    "    bins=bins,\n",
    "    # density=True,\n",
    "    # histtype='step', \n",
    "    # linewidth=8,\n",
    "    color=hist_color,\n",
    "    edgecolor=hist_color,\n",
    ")\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xlabel('proportion of homologous sequence covered by coding sequence')\n",
    "ax.set_xlabel('fraction of homologous sequence covered by coding sequence')\n",
    "ax.set_ylabel('# homologous sequences')\n",
    "# orig_xlim = ax.get_xlim()\n",
    "ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "\n",
    "ylim = ax.get_ylim()\n",
    "ax.plot([search_for_pis_args['stage6']['blast_longer_linked_repeat_cds_to_each_taxon_genome']['min_alignment_bases_covered_by_cds_proportion']] * 2, ylim,\n",
    "        linestyle='--',color='red',\n",
    "        # label=f'x=0',\n",
    "        alpha=THRESHOLD_DASHED_LINE_ALPHA)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "# fig.set_size_inches(12, 6, forward=True)\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%g'))\n",
    "\n",
    "if 1:\n",
    "    fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s31.png'))\n",
    "    # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s31.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebacaa-4c60-42a7-9252-8055533e0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    plt.close('all')\n",
    "\n",
    "fig = plt.figure(\n",
    "    figsize=(6,9.8), \n",
    ")\n",
    "fig_gridspec = fig.add_gridspec(\n",
    "    # 3, 1, \n",
    "    5, 1, \n",
    "    # wspace=0.0001,\n",
    "    hspace=0.0001,\n",
    "    # width_ratios=[3,10,0.43,1.2], \n",
    "    height_ratios=[4,3,4,3,4], \n",
    ")\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    # top=0.96 if (num_of_long_read_rows == 2) else 0.945,\n",
    "    top=0.91,\n",
    "    bottom=0.06,\n",
    "    left=0.07,\n",
    "    right=0.97,\n",
    "    # hspace=0.2,\n",
    "    # wspace=0.2\n",
    ")\n",
    "\n",
    "upper_subgridspec = fig_gridspec[0].subgridspec(1, 5, width_ratios=[0.2,1,0.7,1,0.2], wspace=0)\n",
    "middle_subgridspec = fig_gridspec[2].subgridspec(1, 5, width_ratios=[0.2,1,0.7,1,0.2], wspace=0)\n",
    "lower_subgridspec = fig_gridspec[4].subgridspec(1, 5, width_ratios=[0.2,1,0.7,1,0.2], wspace=0)\n",
    "\n",
    "sra_accession_and_inv_region_to_ax_info = {\n",
    "    ('SRR10883644', '3712356-3713584'): {'ax': fig.add_subplot(upper_subgridspec[1]), 'show_y_label': True, 'panel_label': 'A'},\n",
    "    ('SRR8867158', '5327747-5328904'):  {'ax': fig.add_subplot(upper_subgridspec[3]), 'show_y_label': True, 'panel_label': 'B'},\n",
    "    ('SRR6322568', '2158761-2164817'):  {'ax': fig.add_subplot(middle_subgridspec[1]), 'show_y_label': True, 'panel_label': 'C'},\n",
    "    ('SRR6322568', '2160408-2163224'):  {'ax': fig.add_subplot(middle_subgridspec[3]), 'show_y_label': True, 'panel_label': 'D'},\n",
    "    ('SRR6322568', '2160750-2162885'):  {'ax': fig.add_subplot(lower_subgridspec[1]), 'show_y_label': True, 'panel_label': 'E'},\n",
    "}\n",
    "    \n",
    "    \n",
    "curr_df = min_all_all_diff_score_alignments_df.copy()\n",
    "\n",
    "jitter_sd = 1\n",
    "for sra_accession, curr_sra_df in curr_df.groupby('sra_accession'):\n",
    "    curr_sra_df = curr_sra_df.copy()\n",
    "    \n",
    "    for in_silico_inverted_region, curr_inv_region_df in curr_sra_df.groupby('in_silico_inverted_region'):\n",
    "        curr_inv_region_df = curr_inv_region_df.copy()\n",
    "        \n",
    "        assert curr_inv_region_df['longer_linked_repeat_cds_nuccore_accession'].nunique() == 1\n",
    "        longer_linked_repeat_cds_nuccore_accession = curr_inv_region_df['longer_linked_repeat_cds_nuccore_accession'].iloc[0]\n",
    "        \n",
    "        num_of_reads = len(curr_inv_region_df)\n",
    "        # print(sra_accession, in_silico_inverted_region, num_of_reads)\n",
    "        \n",
    "        \n",
    "        min_finite_score = min((set(curr_inv_region_df['pair_total_align_score_ref']) | set(curr_inv_region_df['pair_total_align_score_non_ref'])) - {-np.inf})\n",
    "        assert not pd.isna(min_finite_score)\n",
    "        fake_minus_inf_score = min_finite_score - abs(min_finite_score * 0.1)\n",
    "        fake_minus_inf_score_thresh = min_finite_score - abs(min_finite_score * 0.05)\n",
    "        curr_inv_region_df['pair_total_align_score_ref'].replace(-np.inf, fake_minus_inf_score, inplace=True)\n",
    "        curr_inv_region_df['pair_total_align_score_non_ref'].replace(-np.inf, fake_minus_inf_score, inplace=True)\n",
    "        \n",
    "        \n",
    "        predicted_invertible_region_label = f'predicted invertible region:\\n{in_silico_inverted_region}'\n",
    "        \n",
    "        ax_info = sra_accession_and_inv_region_to_ax_info[(sra_accession, in_silico_inverted_region)]\n",
    "        ax = ax_info['ax']\n",
    "        show_y_label = ax_info['show_y_label']\n",
    "        panel_label = ax_info['panel_label']\n",
    "        \n",
    "        big_diff_df = curr_inv_region_df[curr_inv_region_df['big_abs_score_diff']]\n",
    "        non_big_diff_df = curr_inv_region_df[~curr_inv_region_df['big_abs_score_diff']]\n",
    "        \n",
    "        num_of_big_diff_reads = len(big_diff_df)\n",
    "        num_of_non_big_diff_reads = len(non_big_diff_df)\n",
    "        \n",
    "        assert not big_diff_df['pair_total_align_score_ref'].isna().any()\n",
    "        assert not big_diff_df['pair_total_align_score_non_ref'].isna().any()\n",
    "        ax.scatter(\n",
    "            big_diff_df['pair_total_align_score_ref'] + np.random.normal(scale=jitter_sd, size=num_of_big_diff_reads),\n",
    "            big_diff_df['pair_total_align_score_non_ref'] + np.random.normal(scale=jitter_sd, size=num_of_big_diff_reads),\n",
    "            label=predicted_invertible_region_label,\n",
    "            alpha=0.3,\n",
    "            facecolor='none',\n",
    "            # edgecolor='tab:blue',\n",
    "            # edgecolor='tab:green',\n",
    "            edgecolor='magenta',\n",
    "            # edgecolor='red',\n",
    "        )\n",
    "        \n",
    "        assert not non_big_diff_df['pair_total_align_score_ref'].isna().any()\n",
    "        assert not non_big_diff_df['pair_total_align_score_non_ref'].isna().any()\n",
    "        ax.scatter(\n",
    "            non_big_diff_df['pair_total_align_score_ref'] + np.random.normal(scale=jitter_sd, size=num_of_non_big_diff_reads),\n",
    "            non_big_diff_df['pair_total_align_score_non_ref'] + np.random.normal(scale=jitter_sd, size=num_of_non_big_diff_reads),\n",
    "            label=predicted_invertible_region_label,\n",
    "            alpha=0.3,\n",
    "            s=14,\n",
    "            facecolor='none',\n",
    "            edgecolor='grey',\n",
    "        )\n",
    "        \n",
    "        min_lim = fake_minus_inf_score * 1.06\n",
    "        max_lim = abs(fake_minus_inf_score) * 0.06\n",
    "        \n",
    "        # print(min_lim, max_lim)\n",
    "        ax.plot((min_lim, max_lim), (min_lim, max_lim),\n",
    "            linestyle='--',color='grey',\n",
    "            alpha=0.3)\n",
    "        ax.plot((min_lim, max_lim), [fake_minus_inf_score_thresh] * 2,\n",
    "                linestyle=FAKE_VAL_THRESHOLD_LINE_STYLE,\n",
    "                color=FAKE_VAL_THRESHOLD_LINE_COLOR,\n",
    "                alpha=FAKE_VAL_THRESHOLD_LINE_ALPHA)\n",
    "        ax.plot([fake_minus_inf_score_thresh] * 2, (min_lim, max_lim),\n",
    "                linestyle=FAKE_VAL_THRESHOLD_LINE_STYLE,\n",
    "                color=FAKE_VAL_THRESHOLD_LINE_COLOR,\n",
    "                alpha=FAKE_VAL_THRESHOLD_LINE_ALPHA)\n",
    "\n",
    "        \n",
    "        orig_y_tick_positions = ax.get_yticks()\n",
    "        new_y_tick_positions = [fake_minus_inf_score] + [y for y in orig_y_tick_positions if y > fake_minus_inf_score + 30]\n",
    "        ax.set_yticks(new_y_tick_positions)\n",
    "        new_y_tick_labels = ['NA'] + [remove_trailing_zeros(str(x)) for x in new_y_tick_positions[1:]]\n",
    "        ax.set_yticklabels(\n",
    "            new_y_tick_labels,\n",
    "            fontsize='small',\n",
    "        )\n",
    "        \n",
    "        orig_x_tick_positions = ax.get_xticks()\n",
    "        new_x_tick_positions = [fake_minus_inf_score] + [x for x in orig_x_tick_positions if x > fake_minus_inf_score + 30]\n",
    "        ax.set_xticks(new_x_tick_positions)\n",
    "        new_x_tick_labels = ['NA'] + [remove_trailing_zeros(str(x)) for x in new_x_tick_positions[1:]]\n",
    "        ax.set_xticklabels(\n",
    "            new_x_tick_labels,\n",
    "            fontsize='small',\n",
    "        )\n",
    "        \n",
    "        ax.set_xlim((min_lim, max_lim))\n",
    "        ax.set_ylim((min_lim, max_lim))\n",
    "        \n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlabel(\n",
    "            'score of paired read\\nalignment to ref',\n",
    "            fontsize='small',\n",
    "        )\n",
    "        if show_y_label:\n",
    "            ax.set_ylabel(\n",
    "                'score of paired read\\nalignment to alternative ref',\n",
    "                fontsize='small',\n",
    "            )\n",
    "        ax.set_title(f'ref: {longer_linked_repeat_cds_nuccore_accession}\\nSRA: {sra_accession}\\n{predicted_invertible_region_label}')\n",
    "\n",
    "        ax.text(-0.35, 1.39, panel_label, va='center', ha='center', transform=ax.transAxes, fontweight='bold')\n",
    "if 1:\n",
    "    fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s32.png'))\n",
    "    # fig.savefig(os.path.join(paper_figs_and_tables_dir_path, 'sup_fig_s32.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f582a-4d13-4d7f-af26-88b23b7829c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0228f34-a3b2-4155-ad3e-5bc336e0f1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}